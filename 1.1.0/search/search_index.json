{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"MLPotion: Brew Your ML Magic! \ud83e\uddea\u2728","text":"<p> <p>Provided and maintained by \ud83e\udd84 UnicoLab</p> </p> <p>Welcome, fellow alchemist! \ud83e\uddd9\u200d\u2642\ufe0f Ready to brew some machine learning magic without getting locked in a cauldron?</p> <p>MLPotion is your chest of modular, mix-and-match ML building blocks that work across Keras, TensorFlow, and PyTorch. Think of it as LEGO\u00ae for ML pipelines, but with fewer foot injuries and more flexibility!</p>"},{"location":"index.html#why-mlpotion","title":"Why MLPotion? \ud83e\udd14","text":"<p>Ever felt trapped by a framework that forces you to do things \"their way\"? We've been there. That's why we created MLPotion:</p> <ul> <li>\ud83c\udfaf Framework Agnostic: Write once, run anywhere (well, on Keras, TensorFlow, or PyTorch)</li> <li>\ud83e\uddf1 Modular by Design: Pick the pieces you need, leave the rest in the box</li> <li>\ud83d\udd2c Type-Safe: Python 3.10+ typing that actually helps you (mypy approved!)</li> <li>\ud83d\ude80 Production Ready: Built for the real world, not just notebooks</li> <li>\ud83c\udfa8 Orchestration Flexible: Works standalone OR with ZenML, Prefect, Airflow - your choice!</li> <li>\ud83d\udce6 Install What You Need: Core package works without any ML frameworks (you only install what you need)!</li> <li>\ud83e\udd1d Community-Driven: Missing something? Contribute it back - we love community additions!</li> </ul>"},{"location":"index.html#whats-in-the-potion","title":"What's in the Potion? \ud83e\uddea","text":"\u2697\ufe0f Core Ingredients <ul> <li>Type-safe protocols for all components</li> <li>Framework-agnostic result types</li> <li>Consistent error handling</li> <li>Zero-dependency core package</li> </ul> \ud83d\udd27 Framework Support <ul> <li>Keras 3.0+ - The friendly one</li> <li>TensorFlow 2.15+ - The production workhorse</li> <li>PyTorch 2.0+ - The researcher's favorite</li> </ul> \ud83d\udcca Data Processing <ul> <li>CSV loaders for all frameworks</li> <li>Dataset optimization utilities</li> <li>Data transformers</li> <li>Preprocessing pipelines</li> </ul> \ud83c\udf93 Training &amp; Evaluation <ul> <li>Unified training interface</li> <li>Comprehensive evaluation tools</li> <li>Rich result objects</li> <li>Training history tracking</li> </ul> \ud83d\udcbe Model Management <ul> <li>Save/load model checkpoints</li> <li>Export to production formats</li> <li>Model inspection utilities</li> <li>Multiple export formats</li> </ul> \ud83d\udd04 Orchestration Integration <ul> <li>ZenML integration built-in</li> <li>Extensible to Prefect, Airflow, etc.</li> <li>Works standalone (no orchestration needed!)</li> <li>Community contributions welcome</li> </ul>"},{"location":"index.html#the-mlpotion-philosophy","title":"The MLPotion Philosophy \ud83c\udfad","text":"<p>\"A good potion doesn't force you to drink it a certain way. It just... works.\"</p> <p>\u2014 Ancient ML Alchemist Proverb (we just made that up)</p> <p>We believe in:</p> <ol> <li>Flexibility &gt; Convention: Your project, your rules</li> <li>Simplicity &gt; Complexity: If it's hard to use, we failed</li> <li>Type Safety &gt; Runtime Surprises: Catch errors before they bite</li> <li>Modularity &gt; Monoliths: Use what you need, ignore the rest</li> <li>Consistency &gt; Chaos: Same patterns across all frameworks</li> <li>Community &gt; Corporate: Built by the community, for the community</li> </ol>"},{"location":"index.html#extensibility-community-contributions","title":"Extensibility &amp; Community Contributions \ud83c\udf1f","text":"<p>MLPotion is designed to be extensible. While we provide ZenML integration out-of-the-box, you can easily integrate with:</p> <ul> <li>Prefect: Wrap components as Prefect tasks</li> <li>Airflow: Use as operators in DAGs</li> <li>Kubeflow: Deploy in Kubeflow pipelines</li> <li>Your Custom Orchestrator: The building blocks work anywhere!</li> </ul> <p>Missing a feature? We actively encourage community contributions! Whether it's:</p> <ul> <li>A new data loader (Parquet, Avro, databases)</li> <li>Integration with another orchestration framework</li> <li>Framework-specific optimizations</li> <li>New export formats</li> </ul> <p>Your contributions help everyone. Check out our Contributing Guide to get started!</p>"},{"location":"index.html#whos-this-for","title":"Who's This For? \ud83c\udfaf","text":"<p>You'll love MLPotion if you:</p> <ul> <li>Switch between frameworks and hate rewriting everything</li> <li>Value heavily tested code that you can reuse</li> <li>Value type safety and IDE autocomplete (who doesn't?)</li> <li>Want production-ready code without enterprise bloat</li> <li>Believe ML pipelines should be composable and testable</li> </ul> <p>You might want something else if you:</p> <ul> <li>Do not like modularity</li> <li>Do not like reusability</li> <li>Are too lazy to contribute something that you can't already find here</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started \ud83d\ude80","text":"<p>Ready to start brewing? Here's your path:</p> 1 \ud83d\udce5 Install MLPotion <p>Choose your framework flavor</p> Installation Guide \u2192 2 \u26a1 Quick Start <p>Get up and running in 5 minutes</p> Quick Start \u2192 3 \ud83e\udde0 Learn Concepts <p>Understand the architecture</p> Core Concepts \u2192 4 \ud83c\udfa8 Build Pipelines <p>Create your first pipeline</p> First Pipeline \u2192"},{"location":"index.html#show-me-the-code","title":"Show Me the Code! \ud83d\udcbb","text":""},{"location":"index.html#standalone-usage-framework-only","title":"Standalone Usage (Framework-Only)","text":"KerasTensorFlowPyTorch <pre><code>\"\"\"Basic Keras usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion Keras workflow:\n1. Load data from CSV\n2. Create a Keras model\n3. Train the model\n4. Evaluate the model\n5. Save and export the model\n\"\"\"\n\nimport tensorflow as tf\nfrom loguru import logger\n\nfrom mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\n# =================== METHODS ===========================\ndef create_model(input_dim: int = 10) -&gt; tf.keras.Model:\n    \"\"\"Create a simple feedforward neural network.\n\n    Args:\n        input_dim: Number of input features.\n\n    Returns:\n        Compiled Keras model.\n    \"\"\"\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n        ]\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n    return model\n\n\n# =================== MAIN ===========================\n\n# 1. Load data (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n1. Loading data from CSV...\")\nloader = CSVDataLoader(\n    file_pattern=\"docs/examples/data/sample.csv\",\n    label_name=\"target\",\n    batch_size=8,\n    shuffle=True,\n)\ndataset = loader.load()\nlogger.info(f\"Dataset created: {dataset}\")\n\n# 2. Create model (CUSTOM)\nlogger.info(\"\\n2. Creating Keras model...\")\nmodel = create_model(input_dim=10)\nlogger.info(model.summary())\n\n# 3. Train model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n3. Training model...\")\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=10,\n    batch_size=8,\n    learning_rate=0.001,\n    validation_split=0.2,\n    verbose=1,\n)\n\nresult = trainer.train(\n    model=model,\n    data=dataset,\n    config=config,\n)\n\nlogger.info(\"\\nTraining completed!\")\nlogger.info(f\"Final training results: {result}\")\n\n# 4. Evaluate model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n4. Evaluating model...\")\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(\n    model=model,\n    data=dataset,\n    config=config,\n)\n\nlogger.info(\"Evaluation completed!\")\nlogger.info(f\"Evaluation results: {eval_result}\")\n\n# 5. Save model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n5. Saving model...\")\nmodel_path = \"/tmp/keras_model.keras\"\n\npersistence = ModelPersistence(\n    path=model_path,\n    model=model,\n)\npersistence.save(\n    save_format=\"keras\",\n)\nlogger.info(f\"Model saved to: {model_path}\")\n\n# 6. Load model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n6. Loading model...\")\nloaded_model, metadata = persistence.load()\nlogger.info(f\"Model loaded successfully: {type(loaded_model)}\")\n\nlogger.info(\"\\n\" + \"=\" * 60)\nlogger.info(\"Complete!\")\nlogger.info(\"=\" * 60)\n</code></pre> <pre><code>\"\"\"Basic TensorFlow usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion TensorFlow workflow:\n1. Load data from CSV\n2. Optimize dataset for performance\n3. Create a TensorFlow model\n4. Train the model\n5. Evaluate the model\n6. Save and export the model\n\"\"\"\n\nimport tensorflow as tf\n\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\ndef main() -&gt; None:\n    \"\"\"Run basic TensorFlow training pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow Basic Usage\")\n    print(\"=\" * 60)\n\n    # 1. Load data\n    print(\"\\n1. Loading data...\")\n    loader = CSVDataLoader(\n        file_pattern=\"examples/data/sample.csv\",\n        label_name=\"target\",\n        batch_size=1,  # Load unbatched, let DatasetOptimizer handle batching\n    )\n    dataset = loader.load()\n    print(f\"Dataset: {dataset}\")\n\n    # Unbatch the dataset first (since CSVDataLoader batches by default)\n    dataset = dataset.unbatch()\n\n    # Transform OrderedDict to single tensor\n    def prepare_features(features, label):\n        \"\"\"Convert OrderedDict of features to single tensor.\"\"\"\n        feature_list = [features[key] for key in sorted(features.keys())]\n        stacked_features = tf.stack(feature_list, axis=-1)\n        return stacked_features, label\n\n    dataset = dataset.map(prepare_features)\n\n    # 2. Optimize dataset\n    print(\"\\n2. Optimizing dataset...\")\n    optimizer = DatasetOptimizer(batch_size=8, shuffle_buffer_size=100)\n    dataset = optimizer.optimize(dataset)\n\n    # 3. Create model\n    print(\"\\n3. Creating model...\")\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n        ]\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n    print(model.summary())\n\n    # 4. Train model\n    print(\"\\n4. Training model...\")\n    trainer = ModelTrainer()\n    config = ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n    result = trainer.train(\n        model=model,\n        data=dataset,\n        config=config,\n    )\n\n    print(\"\\nTraining completed!\")\n    print(f\"{result=}\")\n\n    # 5. Evaluate model\n    print(\"\\n5. Evaluating model...\")\n    evaluator = ModelEvaluator()\n    from mlpotion.frameworks.tensorflow import ModelEvaluationConfig\n\n    eval_config = ModelEvaluationConfig(batch_size=8, verbose=1)\n    eval_result = evaluator.evaluate(\n        model=model,\n        data=dataset,\n        config=eval_config,\n    )\n    print(f\"{eval_result=}\")\n\n    # 6. Save model\n    print(\"\\n6. Saving model...\")\n    model_path = \"/tmp/tensorflow_model.keras\"\n    persistence = ModelPersistence(\n        path=model_path,\n        model=model,\n    )\n    persistence.save(\n        save_format=\".keras\",\n    )\n    print(f\"Model saved to: {model_path}\")\n\n    # 7. Load model\n    print(\"\\n7. Loading model...\")\n    loaded_model, metadata = persistence.load()\n    print(f\"Model loaded successfully: {type(loaded_model)}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>\"\"\"Basic PyTorch usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion PyTorch workflow:\n1. Load data from CSV\n2. Create a PyTorch model\n3. Train the model\n4. Evaluate the model\n5. Save and export the model\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom mlpotion.frameworks.pytorch import (\n    CSVDataset,\n    CSVDataLoader,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\nclass SimpleModel(nn.Module):\n    \"\"\"Simple feedforward neural network.\n\n    Args:\n        input_dim: Number of input features.\n        hidden_dim: Size of hidden layer.\n    \"\"\"\n\n    def __init__(self, input_dim: int = 10, hidden_dim: int = 64) -&gt; None:\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim, 32)\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network.\"\"\"\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        return self.fc3(x)\n\n\ndef main() -&gt; None:\n    \"\"\"Run basic PyTorch training pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - PyTorch Basic Usage\")\n    print(\"=\" * 60)\n\n    # 1. Load data\n    print(\"\\n1. Loading data...\")\n    dataset = CSVDataset(\n        file_pattern=\"examples/data/sample.csv\",\n        label_name=\"target\",\n    )\n    print(f\"Dataset size: {len(dataset)}\")\n\n    # 2. Create DataLoader\n    print(\"\\n2. Creating DataLoader...\")\n    factory = CSVDataLoader(batch_size=8, shuffle=True)\n    dataloader = factory.load(dataset)\n\n    # 3. Create model\n    print(\"\\n3. Creating model...\")\n    model = SimpleModel(input_dim=10, hidden_dim=64)\n    print(model)\n\n    # 4. Train model\n    print(\"\\n4. Training model...\")\n    trainer = ModelTrainer()\n    config = ModelTrainingConfig(\n        epochs=10,\n        learning_rate=0.001,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        verbose=1,\n    )\n    result = trainer.train(\n        model=model,\n        dataloader=dataloader,\n        config=config,\n    )\n\n    print(\"\\nTraining completed!\")\n    print(f\"Training time: {result.training_time:.2f}s\")\n    print(f\"Final loss: {result.metrics['loss']:.4f}\")\n\n    # 5. Evaluate model\n    print(\"\\n5. Evaluating model...\")\n    evaluator = ModelEvaluator()\n    eval_result = evaluator.evaluate(model, dataloader, config)\n\n    print(f\"Evaluation completed in {eval_result.evaluation_time:.2f}s\")\n    print(\"Evaluation metrics:\")\n    for metric_name, metric_value in eval_result.metrics.items():\n        print(f\"  - {metric_name}: {metric_value:.4f}\")\n\n    # 6. Save model\n    print(\"\\n6. Saving model...\")\n    persistence = ModelPersistence(\n        path=\"/tmp/pytorch_model.pth\",\n        model=model,\n    )\n    model_path = \"/tmp/pytorch_model.pth\"\n    persistence.save()\n    print(f\"Model saved to: {model_path}\")\n\n    # 7. Load model\n    print(\"\\n7. Loading model...\")\n    loaded_model, metadata = persistence.load()\n    print(f\"Model loaded successfully: {type(loaded_model)}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"index.html#zenml-pipeline-examples-mlops-mode","title":"ZenML Pipeline Examples (MLOps Mode)","text":"KerasTensorFlowPyTorch <pre><code>\"\"\"Keras training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's Keras components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\nimport keras\nfrom zenml import pipeline, step\n\nfrom mlpotion.integrations.zenml.keras.steps import (\n    evaluate_model,\n    export_model,\n    load_data,\n    save_model,\n    train_model,\n)\n\n\n@step\ndef create_model() -&gt; keras.Model:\n    \"\"\"Create and compile a Keras model.\n\n    Returns:\n        Compiled Keras model ready for training.\n    \"\"\"\n    model = keras.Sequential(\n        [\n            keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n            keras.layers.Dropout(0.2),\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dropout(0.2),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n\n    return model\n\n\n@pipeline(enable_cache=False)\ndef keras_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/keras_model.keras\",\n    export_path: str = \"/tmp/keras_model_export\",\n):\n    \"\"\"Complete Keras training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Create and configure model\n    3. Train model\n    4. Evaluate model\n    5. Save model\n    6. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataset = load_data(\n        file_path=file_path,\n        label_name=label_name,\n        batch_size=8,\n        shuffle=True,\n    )\n\n    # Step 2: Create model and config\n    model = create_model()\n\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    # Step 3: Train model\n    trained_model, training_metrics = train_model(\n        model=model,\n        data=dataset,\n        **_config_train,\n    )\n    # Step 4: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=trained_model,\n        data=dataset,\n        verbose=1,\n    )\n\n    # # Step 5: Save model\n    save_model(\n        model=trained_model,\n        save_path=model_save_path,\n    )\n\n    # # Step 6: Export model for serving\n    export_model(\n        model=trained_model,\n        export_path=export_path,\n        export_format=\"tf\",\n    )\n    return trained_model, training_metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the Keras ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - Keras ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Initialize ZenML (if not already initialized)\n    try:\n        from zenml.client import Client\n\n        client = Client()\n        print(f\"\u2705 ZenML initialized. Active stack: {client.active_stack_model.name}\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  ZenML client error: {e}\")\n        print(\"Run 'zenml init' if you haven't already\")\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = keras_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre> <pre><code>\"\"\"TensorFlow training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's TensorFlow components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\n\nimport tensorflow as tf\nfrom zenml import pipeline, step\n\nfrom mlpotion.frameworks.tensorflow import ModelTrainingConfig\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    evaluate_model,\n    export_model,\n    load_data,\n    optimize_data,\n    save_model,\n    train_model,\n)\n\n\n@step(enable_cache=False)  # Disable caching to ensure fresh model\ndef create_model() -&gt; tf.keras.Model:\n    \"\"\"Create and compile a TensorFlow model that accepts dict inputs.\n\n    Returns:\n        Compiled TensorFlow/Keras model ready for training.\n    \"\"\"\n    # Create inputs for each feature (10 features: feature_0 to feature_9)\n    # After batching, make_csv_dataset produces tensors with shape (batch_size,) for each scalar feature\n    # The materializer now correctly preserves this shape as (None,) where None is the batch dimension\n    inputs = {}\n    feature_list = []\n\n    for i in range(10):\n        # Each input has shape (1,) per sample after batching and materializer roundtrip\n        # The materializer preserves the concrete shape (batch_size, 1)\n        inp = tf.keras.Input(shape=(1,), name=f\"feature_{i}\", dtype=tf.float32)\n        inputs[f\"feature_{i}\"] = inp\n        # Already shape (batch_size, 1), no need to reshape\n        feature_list.append(inp)\n\n    # Concatenate all features along the last axis\n    # This will create shape (batch_size, 10)\n    concatenated = tf.keras.layers.Concatenate(axis=-1)(feature_list)\n\n    # Build the model architecture\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(concatenated)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(1)(x)\n\n    # Create the functional model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n\n    return model\n\n\n@step\ndef create_training_config() -&gt; ModelTrainingConfig:\n    \"\"\"Create training configuration.\n\n    Returns:\n        Training configuration with hyperparameters.\n    \"\"\"\n    return ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n\n@pipeline(enable_cache=False)\ndef tensorflow_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/tensorflow_model.keras\",\n    export_path: str = \"/tmp/tensorflow_model_export\",\n):\n    \"\"\"Complete TensorFlow training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Optimize dataset for performance\n    3. Create and configure model\n    4. Train model\n    5. Evaluate model\n    6. Save model\n    7. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataset = load_data(\n        file_path=file_path,\n        batch_size=1,\n        label_name=label_name,\n    )\n\n    # Step 2: Optimize dataset\n    optimized_dataset = optimize_data(\n        dataset=dataset,\n        batch_size=8,\n        shuffle_buffer_size=100,\n    )\n\n    # Step 3: Create model and config\n    model = create_model()\n\n    # Step 4: Train model\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    trained_model, training_metrics = train_model(\n        model=model,\n        dataset=optimized_dataset,\n        **_config_train,\n    )\n\n    # Step 5: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=trained_model,\n        dataset=optimized_dataset,\n    )\n\n    # Step 6: Save model\n    save_model(\n        model=trained_model,\n        save_path=model_save_path,\n    )\n\n    # Step 7: Export model for serving\n    export_model(\n        model=trained_model,\n        export_path=export_path,\n        export_format=\"keras\",\n    )\n\n    return trained_model, training_metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the TensorFlow ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = tensorflow_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre> <pre><code>\"\"\"PyTorch training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's PyTorch components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom zenml import pipeline, step\n\nfrom mlpotion.integrations.zenml.pytorch.steps import (\n    evaluate_model,\n    export_model,\n    load_csv_data,\n    save_model,\n    train_model,\n)\n\n\nclass SimpleModel(nn.Module):\n    \"\"\"Simple feedforward neural network.\n\n    Args:\n        input_dim: Number of input features.\n        hidden_dim: Size of hidden layer.\n    \"\"\"\n\n    def __init__(self, input_dim: int = 10, hidden_dim: int = 64) -&gt; None:\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim, 32)\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network.\"\"\"\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        return self.fc3(x)\n\n\n@step\ndef create_model() -&gt; nn.Module:\n    \"\"\"Create a PyTorch model.\n\n    Returns:\n        PyTorch model ready for training.\n    \"\"\"\n    model = SimpleModel(input_dim=10, hidden_dim=64)\n    return model\n\n\n@pipeline(enable_cache=False)\ndef pytorch_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/pytorch_model.pth\",\n    export_path: str = \"/tmp/pytorch_model_export.pt\",\n):\n    \"\"\"Complete PyTorch training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Create and configure model\n    3. Train model\n    4. Evaluate model\n    5. Save model\n    6. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataloader = load_csv_data(\n        file_path=file_path,\n        label_name=label_name,\n        batch_size=8,\n        shuffle=True,\n    )\n\n    # Step 2: Create model and config\n    model = create_model()\n\n    # Step 3: Train model\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    model, metrics = train_model(\n        model=model,\n        dataloader=dataloader,\n        **_config_train,\n    )\n\n    # Step 4: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=model,\n        dataloader=dataloader,\n    )\n\n    # Step 5: Save model\n    save_model(\n        model=model,\n        save_path=model_save_path,\n    )\n\n    # # Step 6: Export model for serving (TorchScript)\n    export_model(\n        model=model,\n        export_path=export_path,\n        export_format=\"torchscript\",\n    )\n    return model, metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the PyTorch ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - PyTorch ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Initialize ZenML (if not already initialized)\n    try:\n        from zenml.client import Client\n\n        client = Client()\n        print(f\"\u2705 ZenML initialized. Active stack: {client.active_stack_model.name}\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  ZenML client error: {e}\")\n        print(\"Run 'zenml init' if you haven't already\")\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = pytorch_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre>"},{"location":"index.html#feature-comparison","title":"Feature Comparison \ud83d\udcca","text":"Feature MLPotion Framework-Only All-in-One Solutions Multi-framework \u2705 Yes \u274c No \u26a0\ufe0f Limited Type Safety \u2705 Full \u26a0\ufe0f Partial \u26a0\ufe0f Partial Modular Install \u2705 Yes \u274c No \u274c No ZenML Native \u2705 Yes \u274c Manual \u26a0\ufe0f Adapters Learning Curve \ud83d\udcc8 Gentle \ud83d\udcc8 Framework-specific \ud83d\udcc8 Steep Production Ready \u2705 Yes \u26a0\ufe0f DIY \u2705 Yes Flexibility \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f \ud83c\udf1f\ud83c\udf1f"},{"location":"index.html#community-support","title":"Community &amp; Support \ud83e\udd1d","text":"\ud83d\udc19 GitHub Star, fork, contribute! \ud83d\udc1b Issues Report bugs, request features \ud83e\udd84 UnicoLab Enterprise AI solutions \ud83d\udcdc MIT License Free and open source"},{"location":"index.html#whats-next","title":"What's Next? \ud83d\uddfa\ufe0f","text":"\ud83d\udcda Learn the Basics <p>New to MLPotion? Start here!</p> <ul> <li>Installation</li> <li>Quick Start</li> <li>Core Concepts</li> </ul> \ud83d\udd27 Framework Guides <p>Deep dive into your framework</p> <ul> <li>TensorFlow Guide</li> <li>PyTorch Guide</li> <li>Keras Guide</li> </ul> \ud83c\udf93 Tutorials <p>Learn by building</p> <ul> <li>Your First Pipeline</li> <li>ZenML Integration</li> <li>Multi-Framework Project</li> </ul> \ud83d\udcd6 API Reference <p>Detailed documentation</p> <ul> <li>Core APIs</li> <li>Framework APIs</li> <li>Integrations</li> </ul> <p> Ready to brew some ML magic? Let's get started! \ud83e\uddea\u2728 Built with \u2764\ufe0f for the ML community by \ud83e\udd84 UnicoLab </p>"},{"location":"api/core.html","title":"Core API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's framework-agnostic core components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p>"},{"location":"api/core.html#protocols","title":"Protocols","text":""},{"location":"api/core.html#mlpotion.core.protocols","title":"mlpotion.core.protocols","text":"<p>Framework-agnostic protocols using Python 3.10+ type hints.</p> <p>These protocols define interfaces that work across TensorFlow, PyTorch, and other frameworks.</p>"},{"location":"api/core.html#mlpotion.core.protocols-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.protocols.DataLoader","title":"DataLoader","text":"<p>         Bases: <code>Protocol[DatasetT]</code></p> <p>Protocol for data loading components.</p> <p>This protocol defines the interface for loading data into a framework-specific format. Any class that implements a <code>load()</code> method returning a dataset satisfies this protocol.</p> Type Parameters <p>DatasetT: The type of the dataset returned (e.g., <code>tf.data.Dataset</code>, <code>torch.utils.data.DataLoader</code>).</p> Example <pre><code>class MyCSVLoader:\n    def load(self) -&gt; tf.data.Dataset:\n        # Implementation details...\n        return dataset\n\nloader: DataLoader = MyCSVLoader()\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.DataLoader-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.DataLoader.load","title":"load","text":"<pre><code>load() -&gt; DatasetT\n</code></pre> <p>Load data and return a framework-specific dataset.</p> <p>Returns:</p> Name Type Description <code>DatasetT</code> <code>DatasetT</code> <p>The loaded dataset in a framework-specific format.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def load(self) -&gt; DatasetT:\n    \"\"\"Load data and return a framework-specific dataset.\n\n    Returns:\n        DatasetT: The loaded dataset in a framework-specific format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.DataTransformer","title":"DataTransformer","text":"<p>         Bases: <code>Protocol[DatasetT, ModelT]</code></p> <p>Protocol for data transformation components.</p> <p>This protocol defines the interface for transforming datasets using a model, commonly used for tasks like generating embeddings or pre-processing data.</p> Type Parameters <p>DatasetT: The type of the dataset. ModelT: The type of the model used for transformation.</p>"},{"location":"api/core.html#mlpotion.core.protocols.DataTransformer-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.DataTransformer.transform","title":"transform","text":"<pre><code>transform(\n    dataset: DatasetT, model: ModelT, **kwargs: Any\n) -&gt; DatasetT\n</code></pre> <p>Transform a dataset using a model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetT</code> <p>The input dataset to transform.</p> required <code>model</code> <code>ModelT</code> <p>The model to use for the transformation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional framework-specific arguments (e.g., batch_size, device).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetT</code> <code>DatasetT</code> <p>The transformed dataset.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def transform(\n    self,\n    dataset: DatasetT,\n    model: ModelT,\n    **kwargs: Any,\n) -&gt; DatasetT:\n    \"\"\"Transform a dataset using a model.\n\n    Args:\n        dataset: The input dataset to transform.\n        model: The model to use for the transformation.\n        **kwargs: Additional framework-specific arguments (e.g., batch_size, device).\n\n    Returns:\n        DatasetT: The transformed dataset.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.DatasetOptimizer","title":"DatasetOptimizer","text":"<p>         Bases: <code>Protocol[DatasetT]</code></p> <p>Protocol for dataset optimization components.</p> <p>This protocol defines the interface for optimizing datasets, such as applying batching, caching, prefetching, or shuffling.</p> Type Parameters <p>DatasetT: The type of the dataset to be optimized.</p>"},{"location":"api/core.html#mlpotion.core.protocols.DatasetOptimizer-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.DatasetOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: DatasetT) -&gt; DatasetT\n</code></pre> <p>Optimize a dataset for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetT</code> <p>The input dataset to optimize.</p> required <p>Returns:</p> Name Type Description <code>DatasetT</code> <code>DatasetT</code> <p>The optimized dataset, ready for consumption by a model.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def optimize(self, dataset: DatasetT) -&gt; DatasetT:\n    \"\"\"Optimize a dataset for training or inference.\n\n    Args:\n        dataset: The input dataset to optimize.\n\n    Returns:\n        DatasetT: The optimized dataset, ready for consumption by a model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelEvaluator","title":"ModelEvaluator","text":"<p>         Bases: <code>Protocol[ModelT, DatasetT]</code></p> <p>Protocol for model evaluation components.</p> <p>This protocol defines the interface for evaluating models.</p> Type Parameters <p>ModelT: The type of the model to be evaluated. DatasetT: The type of the dataset used for evaluation.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: ModelT,\n    dataset: DatasetT,\n    config: EvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a model on a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model to evaluate.</p> required <code>dataset</code> <code>DatasetT</code> <p>The dataset to evaluate on.</p> required <code>config</code> <code>EvaluationConfig</code> <p>Configuration object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>An object containing the evaluation metrics.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def evaluate(\n    self,\n    model: ModelT,\n    dataset: DatasetT,\n    config: EvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a model on a given dataset.\n\n    Args:\n        model: The model to evaluate.\n        dataset: The dataset to evaluate on.\n        config: Configuration object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: An object containing the evaluation metrics.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelExporter","title":"ModelExporter","text":"<p>         Bases: <code>Protocol[ModelT]</code></p> <p>Protocol for model export components.</p> <p>This protocol defines the interface for exporting models to various formats for deployment.</p> Type Parameters <p>ModelT: The type of the model to be exported.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelExporter-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelExporter.export","title":"export","text":"<pre><code>export(model: ModelT, config: ExportConfig) -&gt; ExportResult\n</code></pre> <p>Export a model for serving or deployment.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model to export.</p> required <code>config</code> <code>ExportConfig</code> <p>Configuration object containing export parameters (path, format, etc.).</p> required <p>Returns:</p> Name Type Description <code>ExportResult</code> <code>ExportResult</code> <p>An object containing details about the exported model.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def export(\n    self,\n    model: ModelT,\n    config: ExportConfig,\n) -&gt; ExportResult:\n    \"\"\"Export a model for serving or deployment.\n\n    Args:\n        model: The model to export.\n        config: Configuration object containing export parameters (path, format, etc.).\n\n    Returns:\n        ExportResult: An object containing details about the exported model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelInspector","title":"ModelInspector","text":"<p>         Bases: <code>Protocol[ModelT]</code></p> <p>Protocol for model inspection components.</p> <p>This protocol defines the interface for inspecting models to extract metadata such as layer configuration, input/output shapes, and parameter counts.</p> Type Parameters <p>ModelT: The type of the model to be inspected.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelInspector-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelInspector.inspect","title":"inspect","text":"<pre><code>inspect(model: ModelT) -&gt; dict[str, Any]\n</code></pre> <p>Inspect a model and return structured metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model to inspect.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing model metadata (inputs, outputs, parameters, layers, etc.).</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def inspect(self, model: ModelT) -&gt; dict[str, Any]:\n    \"\"\"Inspect a model and return structured metadata.\n\n    Args:\n        model: The model to inspect.\n\n    Returns:\n        dict[str, Any]: A dictionary containing model metadata (inputs, outputs, parameters, layers, etc.).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence","title":"ModelPersistence","text":"<p>         Bases: <code>Protocol[ModelT]</code></p> <p>Protocol for model persistence operations (save/load).</p> <p>This protocol defines the interface for saving and loading models to/from disk.</p> Type Parameters <p>ModelT: The type of the model to be persisted.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence.load","title":"load","text":"<pre><code>load(**kwargs: Any) -&gt; tuple[ModelT, dict[str, Any] | None]\n</code></pre> <p>Load a model from the configured path.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional framework-specific loading options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ModelT, dict[str, Any] | None]</code> <p>tuple[ModelT, dict[str, Any] | None]: A tuple containing the loaded model and optional metadata.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def load(self, **kwargs: Any) -&gt; tuple[ModelT, dict[str, Any] | None]:\n    \"\"\"Load a model from the configured path.\n\n    Args:\n        **kwargs: Additional framework-specific loading options.\n\n    Returns:\n        tuple[ModelT, dict[str, Any] | None]: A tuple containing the loaded model and optional metadata.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence.save","title":"save","text":"<pre><code>save(**kwargs: Any) -&gt; None\n</code></pre> <p>Save a model to the configured path.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional framework-specific saving options.</p> <code>{}</code> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def save(self, **kwargs: Any) -&gt; None:\n    \"\"\"Save a model to the configured path.\n\n    Args:\n        **kwargs: Additional framework-specific saving options.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelTrainer","title":"ModelTrainer","text":"<p>         Bases: <code>Protocol[ModelT, DatasetT]</code></p> <p>Protocol for model training components.</p> <p>This protocol defines the standard interface for training models across different frameworks.</p> Type Parameters <p>ModelT: The type of the model to be trained. DatasetT: The type of the dataset used for training.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: ModelT,\n    dataset: DatasetT,\n    config: TrainingConfig,\n    validation_dataset: DatasetT | None = None,\n) -&gt; TrainingResult[ModelT]\n</code></pre> <p>Train a model using the provided dataset and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model instance to train.</p> required <code>dataset</code> <code>DatasetT</code> <p>The training dataset.</p> required <code>config</code> <code>TrainingConfig</code> <p>Configuration object containing training parameters (epochs, learning rate, etc.).</p> required <code>validation_dataset</code> <code>DatasetT | None</code> <p>Optional dataset for validation during training.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[ModelT]</code> <p>TrainingResult[ModelT]: An object containing the trained model, training history, and metrics.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def train(\n    self,\n    model: ModelT,\n    dataset: DatasetT,\n    config: TrainingConfig,\n    validation_dataset: DatasetT | None = None,\n) -&gt; TrainingResult[ModelT]:\n    \"\"\"Train a model using the provided dataset and configuration.\n\n    Args:\n        model: The model instance to train.\n        dataset: The training dataset.\n        config: Configuration object containing training parameters (epochs, learning rate, etc.).\n        validation_dataset: Optional dataset for validation during training.\n\n    Returns:\n        TrainingResult[ModelT]: An object containing the trained model, training history, and metrics.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#result-types","title":"Result Types","text":""},{"location":"api/core.html#mlpotion.core.results","title":"mlpotion.core.results","text":"<p>Result types using dataclasses with Python 3.10+ features.</p>"},{"location":"api/core.html#mlpotion.core.results-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.results.EvaluationResult","title":"EvaluationResult  <code>dataclass</code>","text":"<p>Result container for model evaluation operations.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>dict[str, float]</code> <p>A dictionary of evaluation metric values.</p> <code>config</code> <code>Any</code> <p>The configuration object used for this evaluation.</p> <code>evaluation_time</code> <code>float | None</code> <p>The total time taken for evaluation in seconds.</p>"},{"location":"api/core.html#mlpotion.core.results.EvaluationResult-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.results.EvaluationResult.get_metric","title":"get_metric","text":"<pre><code>get_metric(name: str) -&gt; float | None\n</code></pre> <p>Retrieve a specific metric value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric to retrieve.</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>float | None: The value of the metric, or None if not found.</p> Source code in <code>mlpotion/core/results.py</code> <pre><code>def get_metric(self, name: str) -&gt; float | None:\n    \"\"\"Retrieve a specific metric value.\n\n    Args:\n        name: The name of the metric to retrieve.\n\n    Returns:\n        float | None: The value of the metric, or None if not found.\n    \"\"\"\n    return self.metrics.get(name)\n</code></pre>"},{"location":"api/core.html#mlpotion.core.results.ExportResult","title":"ExportResult  <code>dataclass</code>","text":"<p>Result container for model export operations.</p> <p>Attributes:</p> Name Type Description <code>export_path</code> <code>str</code> <p>The absolute path where the model was exported.</p> <code>format</code> <code>str</code> <p>The format of the exported model (e.g., 'saved_model', 'onnx').</p> <code>config</code> <code>Any</code> <p>The configuration object used for this export.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata generated during export.</p>"},{"location":"api/core.html#mlpotion.core.results.InspectionResult","title":"InspectionResult  <code>dataclass</code>","text":"<p>Result container for model inspection operations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the model.</p> <code>backend</code> <code>str</code> <p>The framework backend used (e.g., 'tensorflow', 'pytorch').</p> <code>trainable</code> <code>bool</code> <p>Whether the model is trainable.</p> <code>inputs</code> <code>list[dict[str, Any]]</code> <p>List of input specifications (shape, dtype, etc.).</p> <code>input_names</code> <code>list[str]</code> <p>List of input names.</p> <code>outputs</code> <code>list[dict[str, Any]]</code> <p>List of output specifications.</p> <code>output_names</code> <code>list[str]</code> <p>List of output names.</p> <code>parameters</code> <code>dict[str, int]</code> <p>Dictionary of parameter counts (total, trainable, non_trainable).</p> <code>signatures</code> <code>dict[str, Any]</code> <p>Model signatures (specific to TensorFlow SavedModel).</p> <code>layers</code> <code>list[dict[str, Any]]</code> <p>List of layer details (name, class, args).</p>"},{"location":"api/core.html#mlpotion.core.results.LoadingResult","title":"LoadingResult  <code>dataclass</code>","text":"<p>         Bases: <code>Generic[ModelT]</code></p> <p>Result from model loading.</p>"},{"location":"api/core.html#mlpotion.core.results.TrainingResult","title":"TrainingResult  <code>dataclass</code>","text":"<p>         Bases: <code>Generic[ModelT]</code></p> <p>Result container for model training operations.</p> <p>This dataclass encapsulates all relevant information produced during a model training session.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelT</code> <p>The trained model instance.</p> <code>history</code> <code>dict[str, list[float]]</code> <p>A dictionary mapping metric names to lists of values per epoch.</p> <code>metrics</code> <code>dict[str, float]</code> <p>A dictionary of the final metric values after training.</p> <code>config</code> <code>Any</code> <p>The configuration object used for this training session.</p> <code>training_time</code> <code>float | None</code> <p>The total time taken for training in seconds.</p> <code>best_epoch</code> <code>int | None</code> <p>The epoch number where the best performance was achieved (if applicable).</p>"},{"location":"api/core.html#mlpotion.core.results.TrainingResult-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.results.TrainingResult.get_history","title":"get_history","text":"<pre><code>get_history(metric: str) -&gt; list[float] | None\n</code></pre> <p>Retrieve the history of a specific metric over epochs.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>The name of the metric to retrieve history for.</p> required <p>Returns:</p> Type Description <code>list[float] | None</code> <p>list[float] | None: The list of metric values per epoch, or None if not found.</p> Source code in <code>mlpotion/core/results.py</code> <pre><code>def get_history(self, metric: str) -&gt; list[float] | None:\n    \"\"\"Retrieve the history of a specific metric over epochs.\n\n    Args:\n        metric: The name of the metric to retrieve history for.\n\n    Returns:\n        list[float] | None: The list of metric values per epoch, or None if not found.\n    \"\"\"\n    return self.history.get(metric)\n</code></pre>"},{"location":"api/core.html#mlpotion.core.results.TrainingResult.get_metric","title":"get_metric","text":"<pre><code>get_metric(name: str) -&gt; float | None\n</code></pre> <p>Retrieve a specific final metric value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric to retrieve.</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>float | None: The value of the metric, or None if not found.</p> Source code in <code>mlpotion/core/results.py</code> <pre><code>def get_metric(self, name: str) -&gt; float | None:\n    \"\"\"Retrieve a specific final metric value.\n\n    Args:\n        name: The name of the metric to retrieve.\n\n    Returns:\n        float | None: The value of the metric, or None if not found.\n    \"\"\"\n    return self.metrics.get(name)\n</code></pre>"},{"location":"api/core.html#mlpotion.core.results.TransformationResult","title":"TransformationResult  <code>dataclass</code>","text":"<p>Result from data transformation.</p>"},{"location":"api/core.html#configurations","title":"Configurations","text":""},{"location":"api/core.html#mlpotion.core.config","title":"mlpotion.core.config","text":"<p>Framework-agnostic base configuration models using Pydantic 2.x.</p> <p>This module contains only truly framework-agnostic configuration classes. Framework-specific configurations should be defined in their respective framework modules (keras, tensorflow, pytorch).</p>"},{"location":"api/core.html#mlpotion.core.config-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.config.EvaluationConfig","title":"EvaluationConfig","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Base configuration for model evaluation.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Batch size for evaluation (must be &gt;= 1).</p> <code>verbose</code> <code>int</code> <p>Verbosity level (0=silent, 1=progress bar, 2=one line per epoch).</p> <code>framework_options</code> <code>dict[str, Any]</code> <p>Dictionary for framework-specific options.</p>"},{"location":"api/core.html#mlpotion.core.config.ExportConfig","title":"ExportConfig","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Base configuration for model export.</p> <p>Attributes:</p> Name Type Description <code>export_path</code> <code>str</code> <p>Destination path for the exported model.</p> <code>format</code> <code>str</code> <p>Format identifier for the export (e.g., 'saved_model', 'onnx').</p> <code>include_optimizer</code> <code>bool</code> <p>Whether to include the optimizer state in the export.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata to include with the export.</p>"},{"location":"api/core.html#mlpotion.core.config.TrainingConfig","title":"TrainingConfig","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Base configuration for model training.</p> <p>This class defines the standard configuration parameters for training models. Framework-specific configurations should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>epochs</code> <code>int</code> <p>Number of training epochs (must be &gt;= 1).</p> <code>batch_size</code> <code>int</code> <p>Batch size for training (must be &gt;= 1).</p> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer (must be &gt; 0.0).</p> <code>validation_split</code> <code>float</code> <p>Fraction of data to use for validation (0.0 to 1.0).</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the training data.</p> <code>verbose</code> <code>int</code> <p>Verbosity level (0=silent, 1=progress bar, 2=one line per epoch).</p> <code>framework_options</code> <code>dict[str, Any]</code> <p>Dictionary for framework-specific options that don't fit standard fields.</p>"},{"location":"api/core.html#exceptions","title":"Exceptions","text":""},{"location":"api/core.html#mlpotion.core.exceptions","title":"mlpotion.core.exceptions","text":"<p>Exception hierarchy for MLPotion.</p>"},{"location":"api/core.html#mlpotion.core.exceptions-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.exceptions.ConfigurationError","title":"ConfigurationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when there is an issue with the provided configuration.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.DataLoadingError","title":"DataLoadingError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during data loading or preprocessing.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.DataTransformationError","title":"DataTransformationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during data transformation operations.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.EvaluationError","title":"EvaluationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model evaluation.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ExportError","title":"ExportError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model export operations.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.FrameworkNotInstalledError","title":"FrameworkNotInstalledError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when a required framework (e.g., TensorFlow, PyTorch) is not installed or cannot be imported.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.MLPotionError","title":"MLPotionError","text":"<p>         Bases: <code>Exception</code></p> <p>Base exception for all MLPotion errors.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelEvaluatorError","title":"ModelEvaluatorError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs specifically within a ModelEvaluator component.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelExporterError","title":"ModelExporterError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs specifically within a ModelExporter component.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelInspectorError","title":"ModelInspectorError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model inspection.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelPersistenceError","title":"ModelPersistenceError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model saving or loading.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelTrainerError","title":"ModelTrainerError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs specifically within a ModelTrainer component.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.TrainingError","title":"TrainingError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during the model training process.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ValidationError","title":"ValidationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when data or model validation fails.</p>"},{"location":"api/core.html#utilities","title":"Utilities","text":""},{"location":"api/core.html#framework-detection","title":"Framework Detection","text":""},{"location":"api/core.html#mlpotion.utils.framework","title":"mlpotion.utils.framework","text":"<p>Framework detection and validation utilities.</p>"},{"location":"api/core.html#mlpotion.utils.framework-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.utils.framework.FrameworkChecker","title":"FrameworkChecker","text":"<p>Utility class to check availability of ML frameworks.</p>"},{"location":"api/core.html#mlpotion.utils.framework.FrameworkChecker-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.utils.framework.FrameworkChecker.is_available","title":"is_available  <code>classmethod</code>","text":"<pre><code>is_available(framework: FrameworkName) -&gt; bool\n</code></pre> <p>Check whether a framework is installed and importable.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>FrameworkName</code> <p>Framework identifier supported by this checker.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the framework can be imported, otherwise False.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided framework is not known.</p> Example <pre><code>if FrameworkChecker.is_available(\"torch\"):\n    print(\"PyTorch is installed!\")\nelse:\n    print(\"PyTorch is missing.\")\n</code></pre> Source code in <code>mlpotion/utils/framework.py</code> <pre><code>@classmethod\ndef is_available(cls, framework: FrameworkName) -&gt; bool:\n    \"\"\"Check whether a framework is installed and importable.\n\n    Args:\n        framework: Framework identifier supported by this checker.\n\n    Returns:\n        True if the framework can be imported, otherwise False.\n\n    Raises:\n        ValueError: If the provided framework is not known.\n\n    Example:\n        ```python\n        if FrameworkChecker.is_available(\"torch\"):\n            print(\"PyTorch is installed!\")\n        else:\n            print(\"PyTorch is missing.\")\n        ```\n    \"\"\"\n    if framework not in cls._FRAMEWORK_IMPORTS:\n        msg = f\"Unsupported framework: {framework}\"\n        logger.error(msg)\n        raise ValueError(msg)\n\n    module_name = cls._FRAMEWORK_IMPORTS[framework]\n\n    try:\n        import_module(module_name)\n        return True\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/core.html#mlpotion.utils.framework-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.utils.framework.get_available_frameworks","title":"get_available_frameworks","text":"<pre><code>get_available_frameworks() -&gt; list[FrameworkName]\n</code></pre> <p>Get list of available frameworks.</p> <p>Returns:</p> Type Description <code>list[FrameworkName]</code> <p>List of framework names that are installed</p> Source code in <code>mlpotion/utils/framework.py</code> <pre><code>def get_available_frameworks() -&gt; list[FrameworkName]:\n    \"\"\"Get list of available frameworks.\n\n    Returns:\n        List of framework names that are installed\n    \"\"\"\n    frameworks: list[FrameworkName] = list(FrameworkChecker._FRAMEWORK_IMPORTS.keys())\n    return [f for f in frameworks if is_framework_available(f)]\n</code></pre>"},{"location":"api/core.html#mlpotion.utils.framework.require_framework","title":"require_framework","text":"<pre><code>require_framework(\n    framework: FrameworkName, install_command: str\n) -&gt; None\n</code></pre> <p>Require a framework to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>FrameworkName</code> <p>Framework name</p> required <code>install_command</code> <code>str</code> <p>Installation command to show in error</p> required <p>Raises:</p> Type Description <code>FrameworkNotInstalledError</code> <p>If framework is not installed</p> Source code in <code>mlpotion/utils/framework.py</code> <pre><code>def require_framework(framework: FrameworkName, install_command: str) -&gt; None:\n    \"\"\"Require a framework to be installed.\n\n    Args:\n        framework: Framework name\n        install_command: Installation command to show in error\n\n    Raises:\n        FrameworkNotInstalledError: If framework is not installed\n    \"\"\"\n    if not is_framework_available(framework):\n        raise FrameworkNotInstalledError(\n            f\"{framework} is not installed. \"\n            f\"Install it with: poetry add {install_command}\"\n        )\n</code></pre>"},{"location":"api/core.html#decorators","title":"Decorators","text":"<p> For framework-specific APIs, see the respective framework documentation </p>"},{"location":"api/core.html#mlpotion.utils.decorators","title":"mlpotion.utils.decorators","text":""},{"location":"api/core.html#mlpotion.utils.decorators-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.utils.decorators.trycatch","title":"trycatch","text":"<pre><code>trycatch(\n    error: Type[Exception], success_msg: str | None = None\n) -&gt; None\n</code></pre> <p>Decorator for wrapping methods with unified exception handling and logging.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Type[Exception]</code> <p>Exception type to raise when unexpected errors occur.</p> required <code>success_msg</code> <code>str | None</code> <p>Optional success message to log on method completion.</p> <code>None</code> Example <pre><code>@trycatch(error=DataLoadingError, success_msg=\"Dataset loaded\")\ndef load(self):\n    ...\n</code></pre> Source code in <code>mlpotion/utils/decorators.py</code> <pre><code>def __init__(\n    self,\n    error: Type[Exception],\n    success_msg: str | None = None,\n) -&gt; None:\n    self.error = error\n    self.success_msg = success_msg\n</code></pre>"},{"location":"api/frameworks/keras.html","title":"Keras API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's Keras components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p> <p>Extensibility</p> <p>These components are built using protocol-based design, making MLPotion easy to extend. Want to add new data sources, training methods, or integrations? See Contributing Guide.</p>"},{"location":"api/frameworks/keras.html#data-loading","title":"Data Loading","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders","title":"mlpotion.frameworks.keras.data.loaders","text":"<p>Keras data loaders.</p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVDataLoader","title":"CSVDataLoader  <code>dataclass</code>","text":"<p>         Bases: <code>DataLoader[CSVSequence]</code></p> <p>Loader for CSV files into a Keras-ready Sequence.</p> <p>This class provides a high-level interface for loading data from CSV files into a <code>CSVSequence</code> compatible with Keras models. It handles file globbing, reading, column selection, and label separation.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load (e.g., \"data/*.csv\").</p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch in the resulting sequence.</p> <code>column_names</code> <code>list[str] | None</code> <p>List of column names to use as features. If None, all columns except the label are used.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as labels. If None, no labels are returned (inference mode).</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data between epochs.</p> <code>dtype</code> <code>np.dtype | str</code> <p>Data type to use for the loaded data (default: \"float32\").</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras.data.loaders import CSVDataLoader\n\n# Define a simple model\nmodel = keras.Sequential([\n    keras.layers.Dense(10, input_shape=(5,), activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Create loader\nloader = CSVDataLoader(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    batch_size=64,\n    shuffle=True\n)\n\n# Load data\ntrain_sequence = loader.load()\n\n# Train model\nmodel.fit(train_sequence, epochs=10)\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVDataLoader.load","title":"load","text":"<pre><code>load() -&gt; CSVSequence\n</code></pre> <p>Load CSV files and return a CSVSequence.</p> <p>Returns:</p> Type Description <code>CSVSequence</code> <p>CSVSequence that can be passed directly to <code>model.fit(...)</code>.</p> <p>Raises:</p> Type Description <code>DataLoadingError</code> <p>If files cannot be found or read.</p> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully created Keras CSV Sequence\",\n)\ndef load(self) -&gt; CSVSequence:\n    \"\"\"Load CSV files and return a CSVSequence.\n\n    Returns:\n        CSVSequence that can be passed directly to `model.fit(...)`.\n\n    Raises:\n        DataLoadingError: If files cannot be found or read.\n    \"\"\"\n    files = self._get_files()\n    df = self._load_dataframe(files)\n    df = self._select_columns(df)\n\n    features_np, labels_np = self._split_features_labels(df)\n\n    logger.info(\n        \"Creating CSVSequence: n_samples={n}, n_features={d}, labels={labels}\",\n        n=features_np.shape[0],\n        d=features_np.shape[1],\n        labels=\"yes\" if labels_np is not None else \"no\",\n    )\n\n    sequence = CSVSequence(\n        features=features_np,\n        labels=labels_np,\n        batch_size=self.batch_size,\n        shuffle=self.shuffle,\n        dtype=self.dtype,\n    )\n    return sequence\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence","title":"CSVSequence","text":"<pre><code>CSVSequence(\n    features: np.ndarray,\n    labels: np.ndarray | None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n    dtype: np.dtype | str = \"float32\",\n) -&gt; None\n</code></pre> <p>         Bases: <code>Sequence</code></p> <p>Keras Sequence for CSV data backed by NumPy arrays.</p> <p>This class implements the <code>keras.utils.Sequence</code> interface, allowing it to be used directly with <code>model.fit()</code>, <code>model.evaluate()</code>, and <code>model.predict()</code>. It handles data batching and shuffling efficiently in memory.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>np.ndarray</code> <p>The feature data as a 2D NumPy array.</p> <code>labels</code> <code>np.ndarray | None</code> <p>The label data as a NumPy array, or None if not available.</p> <code>batch_size</code> <code>int</code> <p>The size of each data batch.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at the end of each epoch.</p> <code>dtype</code> <code>np.dtype | str</code> <p>The data type of the features and labels.</p> Example <pre><code>from mlpotion.frameworks.keras.data.loaders import CSVSequence\nimport numpy as np\n\n# Create dummy data\nX = np.random.rand(100, 10)\ny = np.random.randint(0, 2, 100)\n\n# Create sequence\nsequence = CSVSequence(\n    features=X,\n    labels=y,\n    batch_size=32,\n    shuffle=True\n)\n\n# Use in training\nmodel.fit(sequence, epochs=5)\n</code></pre> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def __init__(\n    self,\n    features: np.ndarray,\n    labels: np.ndarray | None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n    dtype: np.dtype | str = \"float32\",\n) -&gt; None:\n    if features.ndim != 2:\n        raise ValueError(\n            f\"features must be 2D (n_samples, n_features), got shape {features.shape}\"\n        )\n\n    if labels is not None and len(labels) != len(features):\n        raise ValueError(\n            f\"features and labels must have same length, \"\n            f\"got {len(features)} != {len(labels)}\"\n        )\n\n    self._features = features.astype(dtype, copy=False)\n    self._labels = labels.astype(dtype, copy=False) if labels is not None else None\n    self._batch_size = int(batch_size)\n    self._shuffle = bool(shuffle)\n    self._indices = np.arange(len(self._features))\n\n    if self._shuffle:\n        np.random.shuffle(self._indices)\n\n    logger.info(\n        \"Initialized CSVSequence: \"\n        f\"n_samples={len(self._features)}, \"\n        f\"batch_size={self._batch_size}, \"\n        f\"shuffle={self._shuffle}, \"\n        f\"labels={'yes' if self._labels is not None else 'no'}\"\n    )\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; Any\n</code></pre> <p>Get batch by index.</p> <p>Returns:</p> Type Description <code>Any</code> <ul> <li>(x_batch, y_batch) if labels are available</li> </ul> <code>Any</code> <ul> <li>x_batch otherwise</li> </ul> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Any:\n    \"\"\"Get batch by index.\n\n    Returns:\n        - (x_batch, y_batch) if labels are available\n        - x_batch otherwise\n    \"\"\"\n    if idx &lt; 0 or idx &gt;= len(self):\n        raise IndexError(f\"Batch index out of range: {idx}\")\n\n    start = idx * self._batch_size\n    end = min(start + self._batch_size, len(self._features))\n    batch_idx = self._indices[start:end]\n\n    x_batch = self._features[batch_idx]\n    if self._labels is not None:\n        y_batch = self._labels[batch_idx]\n        return x_batch, y_batch\n\n    return x_batch\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Number of batches per epoch.</p> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of batches per epoch.\"\"\"\n    n_samples = len(self._features)\n    return int(np.ceil(n_samples / self._batch_size))\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end() -&gt; None\n</code></pre> <p>Shuffle indices between epochs if enabled.</p> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Shuffle indices between epochs if enabled.\"\"\"\n    if self._shuffle:\n        np.random.shuffle(self._indices)\n</code></pre>"},{"location":"api/frameworks/keras.html#training","title":"Training","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers","title":"mlpotion.frameworks.keras.training.trainers","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer","title":"ModelTrainer  <code>dataclass</code>","text":"<p>         Bases: <code>ModelTrainerProtocol[Model, Sequence]</code></p> <p>Generic trainer for Keras 3 models.</p> <p>This class implements the <code>ModelTrainerProtocol</code> for Keras models, providing a standardized interface for training. It wraps the standard <code>model.fit()</code> method but adds flexibility and consistency checks.</p> <p>It supports: - Automatic model compilation if <code>compile_params</code> are provided. - Handling of various data formats (tuples, dicts, generators). - Standardized return format (dictionary of history metrics).</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelTrainer\n\n# Prepare data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize trainer\ntrainer = ModelTrainer()\n\n# Train\nhistory = trainer.train(\n    model=model,\n    data=(X_train, y_train),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    fit_params={\n        \"epochs\": 5,\n        \"batch_size\": 32,\n        \"verbose\": 1\n    }\n)\n\nprint(history['loss'])\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]\n</code></pre> <p>Train a Keras model using the provided dataset and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to train.</p> required <code>dataset</code> <code>Any</code> <p>The training data. Can be a tuple <code>(x, y)</code>, a dictionary, a <code>Sequence</code>, or a generator.</p> required <code>config</code> <code>ModelTrainingConfig</code> <p>Configuration object containing training parameters.</p> required <code>validation_dataset</code> <code>Any | None</code> <p>Optional validation data.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[Model]</code> <p>TrainingResult[Model]: An object containing the trained model, training history, and metrics.</p> Source code in <code>mlpotion/frameworks/keras/training/trainers.py</code> <pre><code>@trycatch(\n    error=ModelTrainerError,\n    success_msg=\"\u2705 Successfully trained Keras model\",\n)\ndef train(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]:\n    \"\"\"Train a Keras model using the provided dataset and configuration.\n\n    Args:\n        model: The Keras model to train.\n        dataset: The training data. Can be a tuple `(x, y)`, a dictionary, a `Sequence`, or a generator.\n        config: Configuration object containing training parameters.\n        validation_dataset: Optional validation data.\n\n    Returns:\n        TrainingResult[Model]: An object containing the trained model, training history, and metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare compile parameters from config\n    compile_params = {\n        \"optimizer\": self._get_optimizer(config),\n        \"loss\": config.loss,\n        \"metrics\": config.metrics,\n    }\n\n    # Compile if needed or if forced by config (though we usually respect existing compilation)\n    # Here we'll ensure it's compiled. If the user wants to use their own compilation,\n    # they should probably compile it before passing it, but our config implies we control it.\n    # However, to be safe and flexible:\n    if not self._is_compiled(model):\n        if not config.optimizer or not config.loss:\n            raise RuntimeError(\n                \"Model is not compiled and config does not provide optimizer and loss. \"\n                \"Either compile the model beforehand or provide optimizer and loss in config.\"\n            )\n        logger.info(\"Compiling model with config parameters.\")\n        model.compile(**compile_params)\n    else:\n        logger.info(\"Model already compiled. Using existing compilation settings.\")\n\n    # Prepare fit parameters\n    fit_kwargs = {\n        \"epochs\": config.epochs,\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"shuffle\": config.shuffle,\n        \"validation_split\": config.validation_split,\n        \"callbacks\": self._prepare_callbacks(config),\n    }\n\n    if validation_dataset is not None:\n        fit_kwargs[\"validation_data\"] = validation_dataset\n\n    # Add any framework-specific options\n    fit_kwargs.update(config.framework_options)\n\n    logger.info(\"Starting Keras model training...\")\n    logger.debug(f\"Training data type: {type(dataset)!r}\")\n    logger.debug(f\"Fit parameters: {fit_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    history_obj = self._call_fit(model=model, data=dataset, fit_kwargs=fit_kwargs)\n\n    training_time = time.time() - start_time\n\n    # Convert History object to dict[str, list[float]]\n    history_dict = self._history_to_dict(history_obj)\n\n    # Extract final metrics\n    final_metrics = {}\n    for k, v in history_dict.items():\n        if v:\n            final_metrics[k] = v[-1]\n\n    logger.info(\"Training completed.\")\n    logger.debug(f\"Training history: {history_dict}\")\n\n    return TrainingResult(\n        model=model,\n        history=history_dict,\n        metrics=final_metrics,\n        config=config,\n        training_time=training_time,\n        best_epoch=None,  # Keras history doesn't explicitly track \"best\" unless using callbacks\n    )\n</code></pre>"},{"location":"api/frameworks/keras.html#evaluation","title":"Evaluation","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators","title":"mlpotion.frameworks.keras.evaluation.evaluators","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator","title":"ModelEvaluator  <code>dataclass</code>","text":"<p>         Bases: <code>ModelEvaluatorProtocol[Model, Sequence]</code></p> <p>Generic evaluator for Keras 3 models.</p> <p>This class implements the <code>ModelEvaluatorProtocol</code> for Keras models. It wraps the <code>model.evaluate()</code> method to provide a consistent evaluation interface.</p> <p>It ensures that the evaluation result is always returned as a dictionary of metric names to values, regardless of how the model was compiled or what arguments were passed.</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelEvaluator\n\n# Prepare data\nX_test = np.random.rand(20, 10)\ny_test = np.random.randint(0, 2, 20)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize evaluator\nevaluator = ModelEvaluator()\n\n# Evaluate\nmetrics = evaluator.evaluate(\n    model=model,\n    data=(X_test, y_test),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    eval_params={\"batch_size\": 32}\n)\n\nprint(metrics)  # {'loss': 0.693..., 'accuracy': 0.5...}\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a Keras model on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to evaluate.</p> required <code>dataset</code> <code>Any</code> <p>The evaluation data. Can be a tuple <code>(x, y)</code>, a dictionary, or a <code>Sequence</code>.</p> required <code>config</code> <code>ModelEvaluationConfig</code> <p>Configuration object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>An object containing the evaluation metrics.</p> Source code in <code>mlpotion/frameworks/keras/evaluation/evaluators.py</code> <pre><code>@trycatch(\n    error=ModelEvaluatorError,\n    success_msg=\"\u2705 Successfully evaluated Keras model\",\n)\ndef evaluate(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a Keras model on the given data.\n\n    Args:\n        model: The Keras model to evaluate.\n        dataset: The evaluation data. Can be a tuple `(x, y)`, a dictionary, or a `Sequence`.\n        config: Configuration object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: An object containing the evaluation metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare eval parameters\n    eval_kwargs = {\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"return_dict\": True,\n    }\n\n    # Add any framework-specific options\n    eval_kwargs.update(config.framework_options)\n\n    # We assume the model is already compiled. If not, Keras will raise an error\n    # unless we provide compile params, but EvaluationConfig doesn't typically carry them.\n    # The user should ensure the model is compiled (e.g. after loading or training).\n    if not self._is_compiled(model):\n        logger.warning(\n            \"Model is not compiled. Evaluation might fail if loss/metrics are not defined.\"\n        )\n\n    logger.info(\"Evaluating Keras model...\")\n    logger.debug(f\"Evaluation data type: {type(dataset)!r}\")\n    logger.debug(f\"Evaluation parameters: {eval_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    result = self._call_evaluate(model=model, data=dataset, eval_kwargs=eval_kwargs)\n\n    evaluation_time = time.time() - start_time\n\n    # At this point, result should be a dict[str, float]\n    if not isinstance(result, dict):\n        # Defensive fallback if user or Keras changed behavior\n        logger.warning(\n            f\"`model.evaluate` did not return a dict (got {type(result)!r}). \"\n            \"Wrapping into a dict under key 'metric_0'.\"\n        )\n        result = {\"metric_0\": float(result)}\n\n    metrics = {str(k): float(v) for k, v in result.items()}\n    logger.info(f\"Evaluation result: {metrics}\")\n\n    return EvaluationResult(\n        metrics=metrics,\n        config=config,\n        evaluation_time=evaluation_time,\n    )\n</code></pre>"},{"location":"api/frameworks/keras.html#persistence","title":"Persistence","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence","title":"mlpotion.frameworks.keras.deployment.persistence","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence","title":"ModelPersistence","text":"<pre><code>ModelPersistence(\n    path: str | Path, model: Model | None = None\n) -&gt; None\n</code></pre> <p>         Bases: <code>ModelPersistenceProtocol[Model]</code></p> <p>Persistence helper for Keras models.</p> <p>This class manages saving and loading of Keras models. It supports standard Keras formats (<code>.keras</code>, <code>.h5</code>) and SavedModel directories. It also integrates with <code>ModelInspector</code> to provide model metadata upon loading.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Path</code> <p>The file path for the model artifact.</p> <code>model</code> <code>Model | None</code> <p>The Keras model instance (optional).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelPersistence\n\n# Define model\nmodel = keras.Sequential([keras.layers.Dense(1)])\n\n# Save\nsaver = ModelPersistence(path=\"models/my_model.keras\", model=model)\nsaver.save()\n\n# Load\nloader = ModelPersistence(path=\"models/my_model.keras\")\nloaded_model, metadata = loader.load(inspect=True)\nprint(metadata['parameters'])\n</code></pre> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>def __init__(self, path: str | Path, model: Model | None = None) -&gt; None:\n    self._path = Path(path)\n    self._model = model\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence-attributes","title":"Attributes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.model","title":"model  <code>writable</code> <code>property</code>","text":"<pre><code>model: Model | None\n</code></pre> <p>Currently attached Keras model (may be None before loading).</p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.path","title":"path  <code>writable</code> <code>property</code>","text":"<pre><code>path: Path\n</code></pre> <p>Filesystem path where the model is saved/loaded.</p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.load","title":"load","text":"<pre><code>load(\n    *, inspect: bool = True, **kwargs: Any\n) -&gt; tuple[Model, dict[str, Any] | None]\n</code></pre> <p>Load a Keras model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inspect</code> <code>bool</code> <p>Whether to inspect the loaded model and return metadata.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>keras.models.load_model()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Model</code> <p>tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and</p> <code>dict[str, Any] | None</code> <p>optional inspection metadata.</p> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If the model file cannot be found or loaded.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully loaded Keras model\",\n)\ndef load(\n    self,\n    *,\n    inspect: bool = True,\n    **kwargs: Any,\n) -&gt; tuple[Model, dict[str, Any] | None]:\n    \"\"\"Load a Keras model from disk.\n\n    Args:\n        inspect: Whether to inspect the loaded model and return metadata.\n        **kwargs: Additional arguments passed to `keras.models.load_model()`.\n\n    Returns:\n        tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and\n        optional inspection metadata.\n\n    Raises:\n        ModelPersistenceError: If the model file cannot be found or loaded.\n    \"\"\"\n    path = self._ensure_path_exists()\n\n    logger.info(f\"Loading Keras model from: {path!s}\")\n    model = keras.models.load_model(path.as_posix(), **kwargs)\n\n    self._model = model  # keep instance in sync\n\n    inspection_result: dict[str, Any] | None = None\n    if inspect:\n        logger.info(\"Inspecting loaded Keras model with ModelInspector.\")\n        inspector = ModelInspector()\n        inspection_result = inspector.inspect(model)\n\n    return model, inspection_result\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.save","title":"save","text":"<pre><code>save(overwrite: bool = True, **kwargs: Any) -&gt; None\n</code></pre> <p>Save the attached model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>model.save()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If no model is attached or if the file exists and <code>overwrite</code> is False.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully saved Keras model\",\n)\ndef save(\n    self,\n    overwrite: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the attached model to disk.\n\n    Args:\n        overwrite: Whether to overwrite the file if it already exists.\n        **kwargs: Additional arguments passed to `model.save()`.\n\n    Raises:\n        ModelPersistenceError: If no model is attached or if the file exists and `overwrite` is False.\n    \"\"\"\n    model = self._ensure_model()\n    target = self._path\n\n    if target.exists() and not overwrite:\n        raise ModelPersistenceError(\n            f\"Target path already exists and overwrite=False: {target!s}\"\n        )\n\n    logger.info(f\"Saving Keras model to: {target!s}\")\n    target.parent.mkdir(parents=True, exist_ok=True)\n\n    # Keras 3 generally infers format from the path; `save_format` is\n    # deprecated / discouraged in newer APIs, so we do NOT pass it.\n    model.save(target.as_posix(), **kwargs)\n    logger.info(\"Keras model saved successfully.\")\n</code></pre>"},{"location":"api/frameworks/keras.html#export","title":"Export","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters","title":"mlpotion.frameworks.keras.deployment.exporters","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter","title":"ModelExporter","text":"<p>         Bases: <code>ModelExporterProtocol[Model]</code></p> <p>Generic exporter for Keras 3 models.</p> <p>This class implements <code>ModelExporterProtocol</code> and supports exporting Keras models to various formats, including native Keras formats (<code>.keras</code>, <code>.h5</code>) and inference formats like TensorFlow SavedModel or ONNX (via <code>model.export</code>).</p> <p>It also supports creating export archives with custom endpoints using <code>keras.export.ExportArchive</code>.</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelExporter\n\nmodel = keras.Sequential([keras.layers.Dense(1)])\nexporter = ModelExporter()\n\n# Export as standard Keras file\nexporter.export(model, \"models/model.keras\")\n\n# Export for serving (TF SavedModel)\nexporter.export(model, \"models/serving\", export_format=\"tf_saved_model\")\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter.export","title":"export","text":"<pre><code>export(model: Model, path: str, **kwargs: Any) -&gt; None\n</code></pre> <p>Export a Keras model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to export.</p> required <code>path</code> <code>str</code> <p>The destination path or directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional export options: - <code>export_format</code> (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc. - <code>dataset</code> (Iterable): Optional data for model warmup. - <code>endpoint_name</code> (str): Name for custom endpoint (uses ExportArchive). - <code>input_specs</code> (list[InputSpec]): Input signatures for custom endpoint. - <code>config</code> (dict): Extra arguments for the underlying save/export method.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelExporterError</code> <p>If export fails.</p> Source code in <code>mlpotion/frameworks/keras/deployment/exporters.py</code> <pre><code>@trycatch(\n    error=ModelExporterError,\n    success_msg=\"\u2705 Successfully Exported model\",\n)\ndef export(self, model: Model, path: str, **kwargs: Any) -&gt; None:\n    \"\"\"Export a Keras model to disk.\n\n    Args:\n        model: The Keras model to export.\n        path: The destination path or directory.\n        **kwargs: Additional export options:\n            - `export_format` (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc.\n            - `dataset` (Iterable): Optional data for model warmup.\n            - `endpoint_name` (str): Name for custom endpoint (uses ExportArchive).\n            - `input_specs` (list[InputSpec]): Input signatures for custom endpoint.\n            - `config` (dict): Extra arguments for the underlying save/export method.\n\n    Raises:\n        ModelExporterError: If export fails.\n    \"\"\"\n    export_path = Path(path)\n\n    export_format: str | None = kwargs.pop(\"export_format\", None)\n    dataset: Iterable[Any] | None = kwargs.pop(\"dataset\", None)\n    endpoint_name: str | None = kwargs.pop(\"endpoint_name\", None)\n    input_specs: Sequence[InputSpec] | None = kwargs.pop(\"input_specs\", None)\n    config: Mapping[str, Any] | None = kwargs.pop(\"config\", None)\n\n    if kwargs:\n        logger.warning(\n            \"Unused export kwargs passed to ModelExporter: \"\n            f\"{list(kwargs.keys())}\"\n        )\n\n    self._validate_model(model)\n    self._validate_config(config)\n\n    # Determine mode if export_format isn't explicitly set\n    if export_format is None:\n        export_format = self._infer_export_format_from_path(export_path)\n\n    logger.info(\n        f\"Exporting Keras model '{model.name}' to {export_path!s} \"\n        f\"with format '{export_format}'\"\n    )\n\n    # Optional warm-up pass\n    self._warmup_if_needed(model=model, dataset=dataset)\n\n    # Choose strategy\n    try:\n        if self._is_native_keras_format(export_format):\n            self._save_native_keras(model=model, path=export_path, config=config)\n        elif endpoint_name is not None or input_specs is not None:\n            self._export_with_export_archive(\n                model=model,\n                path=export_path,\n                endpoint_name=endpoint_name or self.default_endpoint_name,\n                input_specs=input_specs,\n                export_format=export_format,\n            )\n        else:\n            self._export_with_model_export(\n                model=model,\n                path=export_path,\n                export_format=export_format,\n                config=config,\n            )\n    except ValueError as err:\n        logger.warning(\n            f\"Export error: {err} \"\n            \"(you may need to build the model by calling it on example data \"\n            \"before exporting)\"\n        )\n\n    logger.info(f\"Model export completed: {export_path!s}\")\n</code></pre>"},{"location":"api/frameworks/keras.html#model-inspection","title":"Model Inspection","text":"<p> See the Keras Guide for usage examples </p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection","title":"mlpotion.frameworks.keras.models.inspection","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection.ModelInspector","title":"ModelInspector  <code>dataclass</code>","text":"<p>         Bases: <code>ModelInspectorProtocol[ModelLike]</code></p> <p>Inspector for Keras models.</p> <p>This class analyzes Keras models to extract metadata such as input/output shapes, parameter counts, layer details, and signatures. It is useful for validating models before training or deployment, and for generating model reports.</p> <p>Attributes:</p> Name Type Description <code>include_layers</code> <code>bool</code> <p>Whether to include detailed information about each layer.</p> <code>include_signatures</code> <code>bool</code> <p>Whether to include model signatures (if available).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelInspector\n\nmodel = keras.Sequential([keras.layers.Dense(1, input_shape=(10,))])\ninspector = ModelInspector()\n\ninfo = inspector.inspect(model)\nprint(f\"Total params: {info['parameters']['total']}\")\nprint(f\"Inputs: {info['inputs']}\")\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection.ModelInspector-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection.ModelInspector.inspect","title":"inspect","text":"<pre><code>inspect(model: ModelLike) -&gt; dict[str, Any]\n</code></pre> <p>Inspect a Keras model and return structured metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelLike</code> <p>The Keras model to inspect.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing model metadata: - <code>name</code>: Model name. - <code>backend</code>: Keras backend used. - <code>trainable</code>: Whether the model is trainable. - <code>inputs</code>: List of input specifications. - <code>outputs</code>: List of output specifications. - <code>parameters</code>: Dictionary of parameter counts. - <code>layers</code>: List of layer details (if <code>include_layers=True</code>). - <code>signatures</code>: Model signatures (if <code>include_signatures=True</code>).</p> Source code in <code>mlpotion/frameworks/keras/models/inspection.py</code> <pre><code>@trycatch(\n    error=ModelInspectorError,\n    success_msg=\"\u2705 Successfully inspected Keras model\",\n)\ndef inspect(self, model: ModelLike) -&gt; dict[str, Any]:\n    \"\"\"Inspect a Keras model and return structured metadata.\n\n    Args:\n        model: The Keras model to inspect.\n\n    Returns:\n        dict[str, Any]: A dictionary containing model metadata:\n            - `name`: Model name.\n            - `backend`: Keras backend used.\n            - `trainable`: Whether the model is trainable.\n            - `inputs`: List of input specifications.\n            - `outputs`: List of output specifications.\n            - `parameters`: Dictionary of parameter counts.\n            - `layers`: List of layer details (if `include_layers=True`).\n            - `signatures`: Model signatures (if `include_signatures=True`).\n    \"\"\"\n    if not isinstance(model, keras.Model):\n        raise TypeError(\n            f\"ModelInspector expects a keras.Model, got {type(model)!r}\"\n        )\n\n    logger.info(\"Inspecting Keras model...\")\n\n    backend_name = self._get_backend_name()\n\n    info: dict[str, Any] = {\n        \"name\": model.name,\n        \"backend\": backend_name,\n        \"trainable\": model.trainable,\n    }\n\n    info[\"inputs\"] = self._get_inputs(model)\n    info[\"input_names\"] = [input[\"name\"] for input in info[\"inputs\"]]\n    info[\"outputs\"] = self._get_outputs(model)\n    info[\"output_names\"] = [output[\"name\"] for output in info[\"outputs\"]]\n    info[\"parameters\"] = self._get_param_counts(model)\n\n    if self.include_signatures:\n        info[\"signatures\"] = self._get_signatures(model)\n\n    if self.include_layers:\n        info[\"layers\"] = self._get_layers_summary(model)\n\n    logger.debug(f\"Keras model inspection result: {info}\")\n    return info\n</code></pre>"},{"location":"api/frameworks/pytorch.html","title":"PyTorch API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's PyTorch components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p> <p>Extensibility</p> <p>These components are built using protocol-based design, making MLPotion easy to extend. Want to add new data sources, training methods, or integrations? See Contributing Guide.</p>"},{"location":"api/frameworks/pytorch.html#data-loading","title":"Data Loading","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets","title":"mlpotion.frameworks.pytorch.data.datasets","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset","title":"CSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>Dataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>PyTorch Dataset for CSV files with on-demand tensor conversion.</p> <p>This class loads CSV data into memory (using Pandas) and provides a map-style PyTorch Dataset. It supports filtering columns, separating labels, and efficient on-demand tensor conversion to minimize memory usage.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load. If None, all columns are loaded.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label. If None, no labels are returned.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features (default: <code>torch.float32</code>).</p> Example <pre><code>from mlpotion.frameworks.pytorch import CSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create dataset\ndataset = CSVDataset(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    column_names=[\"feature1\", \"feature2\", \"target_class\"]\n)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Iterate\nfor features, labels in dataloader:\n    print(features.shape, labels.shape)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n</code></pre> <p>Get item at index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global row index.</p> required <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor] | torch.Tensor</code> <p>(features, label) tuple if labels exist, else just features.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __getitem__(\n    self,\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n    \"\"\"Get item at index.\n\n    Args:\n        idx: Global row index.\n\n    Returns:\n        (features, label) tuple if labels exist, else just features.\n    \"\"\"\n    if self._features_df is None:\n        raise IndexError(\"Dataset is empty or not properly initialized.\")\n\n    row = self._features_df.iloc[idx]\n\n    # Convert to numpy array and then to tensor\n    features_np: np.ndarray = row.to_numpy(dtype=\"float32\", copy=False)\n    features = torch.as_tensor(features_np, dtype=self._dtype)\n\n    if self._labels is not None:\n        label_val = self._labels[idx]\n        label = torch.as_tensor(label_val, dtype=self._dtype)\n        return features, label\n\n    return features\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return dataset length.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return dataset length.\"\"\"\n    if self._features_df is None:\n        return 0\n    return len(self._features_df)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Eagerly load CSV files into a DataFrame and validate configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Eagerly load CSV files into a DataFrame and validate configuration.\"\"\"\n    try:\n        files = self._resolve_files()\n        df = self._load_dataframe(files)\n        df = self._select_columns(df)\n        self._split_features_labels(df)\n\n        logger.info(\n            \"Initialized CSVDataset with \"\n            \"n_rows={rows}, n_features={features}, labels={labels}\",\n            rows=len(self._features_df) if self._features_df is not None else 0,\n            features=len(self._feature_cols),\n            labels=\"yes\" if self._labels is not None else \"no\",\n        )\n    except DataLoadingError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise DataLoadingError(f\"Failed to load CSV dataset: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset","title":"StreamingCSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>IterableDataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>Streaming PyTorch IterableDataset for large CSV files.</p> <p>This dataset is designed for datasets that are too large to fit in memory. It reads CSV files in chunks (using Pandas) and streams samples one by one. It is compatible with PyTorch's <code>IterableDataset</code> interface.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load.</p> <code>label_name</code> <code>str | None</code> <p>Name of the label column.</p> <code>chunksize</code> <code>int</code> <p>Number of rows to read into memory at a time per file.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features.</p> Example <pre><code>from mlpotion.frameworks.pytorch import StreamingCSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create streaming dataset\ndataset = StreamingCSVDataset(\n    file_pattern=\"data/large_dataset_*.csv\",\n    label_name=\"target\",\n    chunksize=10000\n)\n\n# Create DataLoader (shuffle must be False for IterableDataset)\ndataloader = DataLoader(dataset, batch_size=64)\n\nfor features, labels in dataloader:\n    # Train model...\n    pass\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; (\n    Iterator[\n        tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n    ]\n)\n</code></pre> <p>Yield samples one by one across all CSV files.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]:\n    \"\"\"Yield samples one by one across all CSV files.\"\"\"\n    for file_path in self.files:\n        logger.info(f\"Streaming CSV file: {file_path}\")\n        try:\n            # Use pandas chunked reading\n            chunk_iter = pd.read_csv(\n                file_path,\n                usecols=self.column_names,\n                chunksize=self.chunksize,\n            )\n        except TypeError:\n            # If usecols=None is not accepted by some pandas version\n            chunk_iter = pd.read_csv(\n                file_path,\n                chunksize=self.chunksize,\n            )\n\n        for chunk_df in chunk_iter:\n            # Validate label column if needed\n            if self.label_name:\n                if self.label_name not in chunk_df.columns:\n                    raise DataLoadingError(\n                        f\"Label column '{self.label_name}' not found in \"\n                        f\"file {file_path} (columns: {list(chunk_df.columns)})\"\n                    )\n                labels_np = chunk_df[self.label_name].to_numpy()\n                features_df = chunk_df.drop(columns=[self.label_name])\n            else:\n                labels_np = None\n                features_df = chunk_df\n\n            # Convert whole chunk to numpy once\n            features_np = features_df.to_numpy(dtype=\"float32\", copy=False)\n\n            if labels_np is not None:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    y = torch.as_tensor(labels_np[row_idx], dtype=self.dtype)\n                    yield x, y\n            else:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    yield x\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Resolve files eagerly and log basic configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Resolve files eagerly and log basic configuration.\"\"\"\n    self.files = self._resolve_files()\n    logger.info(\n        \"Initialized StreamingCSVDataset with {n_files} file(s), \"\n        \"chunksize={chunksize}, label_name={label}\",\n        n_files=len(self.files),\n        chunksize=self.chunksize,\n        label=self.label_name,\n    )\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders","title":"mlpotion.frameworks.pytorch.data.loaders","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataLoader","title":"CSVDataLoader  <code>dataclass</code>","text":"<p>         Bases: <code>Generic[T_co]</code></p> <p>Factory for creating configured PyTorch DataLoaders.</p> <p>This class simplifies the creation of <code>torch.utils.data.DataLoader</code> instances by encapsulating common configuration options and handling differences between map-style and iterable datasets (e.g., automatically disabling shuffling for iterables).</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data (ignored for IterableDatasets).</p> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>pin_memory</code> <code>bool</code> <p>Whether to copy tensors into CUDA pinned memory.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch.</p> <code>persistent_workers</code> <code>bool | None</code> <p>Whether to keep workers alive between epochs.</p> <code>prefetch_factor</code> <code>int | None</code> <p>Number of batches loaded in advance by each worker.</p> Example <pre><code>from mlpotion.frameworks.pytorch import CSVDataLoader, CSVDataset\n\n# 1. Create a dataset\ndataset = CSVDataset(\"data.csv\", label_name=\"target\")\n\n# 2. Configure the loader factory\nloader_factory = CSVDataLoader(\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\n# 3. Create the actual DataLoader\ntrain_loader = loader_factory.load(dataset)\n\n# 4. Use it\nfor X, y in train_loader:\n    ...\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataLoader.load","title":"load","text":"<pre><code>load(\n    dataset: Dataset[T_co] | IterableDataset[T_co],\n) -&gt; DataLoader[T_co]\n</code></pre> <p>Load a configured :class:<code>DataLoader</code> from a dataset.</p> <p>This method is aware of :class:<code>IterableDataset</code> vs map-style :class:<code>Dataset</code> and will:</p> <ul> <li>Disable shuffling for iterable datasets (with a warning if   <code>shuffle=True</code> was requested).</li> <li>Apply worker-related options only when valid.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[T_co] | IterableDataset[T_co]</code> <p>PyTorch :class:<code>Dataset</code> or :class:<code>IterableDataset</code>.</p> required <p>Returns:</p> Name Type Description <code>Configured</code> <code>DataLoader[T_co]</code> <p>class:<code>torch.utils.data.DataLoader</code> instance.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully Loading data\",\n)\ndef load(\n    self,\n    dataset: Dataset[T_co] | IterableDataset[T_co],\n) -&gt; DataLoader[T_co]:\n    \"\"\"Load a configured :class:`DataLoader` from a dataset.\n\n    This method is aware of :class:`IterableDataset` vs map-style\n    :class:`Dataset` and will:\n\n    - Disable shuffling for iterable datasets (with a warning if\n      ``shuffle=True`` was requested).\n    - Apply worker-related options only when valid.\n\n    Args:\n        dataset: PyTorch :class:`Dataset` or :class:`IterableDataset`.\n\n    Returns:\n        Configured :class:`torch.utils.data.DataLoader` instance.\n    \"\"\"\n    is_iterable = isinstance(dataset, IterableDataset)\n    effective_shuffle = self._resolve_shuffle(is_iterable=is_iterable)\n\n    loader_kwargs = self._build_loader_kwargs(\n        dataset=dataset,\n        shuffle=effective_shuffle,\n        is_iterable=is_iterable,\n    )\n\n    logger.info(\n        \"Creating DataLoader with config: \"\n        \"batch_size={batch_size}, shuffle={shuffle}, \"\n        \"num_workers={num_workers}, pin_memory={pin_memory}, \"\n        \"drop_last={drop_last}, persistent_workers={persistent_workers}, \"\n        \"prefetch_factor={prefetch_factor}, dataset_type={dtype}\",\n        batch_size=self.batch_size,\n        shuffle=effective_shuffle,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=self.drop_last,\n        persistent_workers=self.persistent_workers,\n        prefetch_factor=self.prefetch_factor,\n        dtype=\"IterableDataset\" if is_iterable else \"Dataset\",\n    )\n\n    return DataLoader(**loader_kwargs)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset","title":"CSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>Dataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>PyTorch Dataset for CSV files with on-demand tensor conversion.</p> <p>This class loads CSV data into memory (using Pandas) and provides a map-style PyTorch Dataset. It supports filtering columns, separating labels, and efficient on-demand tensor conversion to minimize memory usage.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load. If None, all columns are loaded.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label. If None, no labels are returned.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features (default: <code>torch.float32</code>).</p> Example <pre><code>from mlpotion.frameworks.pytorch import CSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create dataset\ndataset = CSVDataset(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    column_names=[\"feature1\", \"feature2\", \"target_class\"]\n)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Iterate\nfor features, labels in dataloader:\n    print(features.shape, labels.shape)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n</code></pre> <p>Get item at index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global row index.</p> required <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor] | torch.Tensor</code> <p>(features, label) tuple if labels exist, else just features.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __getitem__(\n    self,\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n    \"\"\"Get item at index.\n\n    Args:\n        idx: Global row index.\n\n    Returns:\n        (features, label) tuple if labels exist, else just features.\n    \"\"\"\n    if self._features_df is None:\n        raise IndexError(\"Dataset is empty or not properly initialized.\")\n\n    row = self._features_df.iloc[idx]\n\n    # Convert to numpy array and then to tensor\n    features_np: np.ndarray = row.to_numpy(dtype=\"float32\", copy=False)\n    features = torch.as_tensor(features_np, dtype=self._dtype)\n\n    if self._labels is not None:\n        label_val = self._labels[idx]\n        label = torch.as_tensor(label_val, dtype=self._dtype)\n        return features, label\n\n    return features\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return dataset length.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return dataset length.\"\"\"\n    if self._features_df is None:\n        return 0\n    return len(self._features_df)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Eagerly load CSV files into a DataFrame and validate configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Eagerly load CSV files into a DataFrame and validate configuration.\"\"\"\n    try:\n        files = self._resolve_files()\n        df = self._load_dataframe(files)\n        df = self._select_columns(df)\n        self._split_features_labels(df)\n\n        logger.info(\n            \"Initialized CSVDataset with \"\n            \"n_rows={rows}, n_features={features}, labels={labels}\",\n            rows=len(self._features_df) if self._features_df is not None else 0,\n            features=len(self._feature_cols),\n            labels=\"yes\" if self._labels is not None else \"no\",\n        )\n    except DataLoadingError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise DataLoadingError(f\"Failed to load CSV dataset: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset","title":"StreamingCSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>IterableDataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>Streaming PyTorch IterableDataset for large CSV files.</p> <p>This dataset is designed for datasets that are too large to fit in memory. It reads CSV files in chunks (using Pandas) and streams samples one by one. It is compatible with PyTorch's <code>IterableDataset</code> interface.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load.</p> <code>label_name</code> <code>str | None</code> <p>Name of the label column.</p> <code>chunksize</code> <code>int</code> <p>Number of rows to read into memory at a time per file.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features.</p> Example <pre><code>from mlpotion.frameworks.pytorch import StreamingCSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create streaming dataset\ndataset = StreamingCSVDataset(\n    file_pattern=\"data/large_dataset_*.csv\",\n    label_name=\"target\",\n    chunksize=10000\n)\n\n# Create DataLoader (shuffle must be False for IterableDataset)\ndataloader = DataLoader(dataset, batch_size=64)\n\nfor features, labels in dataloader:\n    # Train model...\n    pass\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; (\n    Iterator[\n        tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n    ]\n)\n</code></pre> <p>Yield samples one by one across all CSV files.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]:\n    \"\"\"Yield samples one by one across all CSV files.\"\"\"\n    for file_path in self.files:\n        logger.info(\"Streaming CSV file: {path}\", path=file_path)\n        try:\n            chunk_iter = pd.read_csv(\n                file_path,\n                usecols=self.column_names,\n                chunksize=self.chunksize,\n            )\n        except TypeError:\n            # If usecols=None is not accepted by some pandas version\n            chunk_iter = pd.read_csv(\n                file_path,\n                chunksize=self.chunksize,\n            )\n\n        for chunk_df in chunk_iter:\n            if self.label_name:\n                if self.label_name not in chunk_df.columns:\n                    raise DataLoadingError(\n                        f\"Label column '{self.label_name}' not found in \"\n                        f\"file {file_path} (columns: {list(chunk_df.columns)})\"\n                    )\n                labels_np = chunk_df[self.label_name].to_numpy()\n                features_df = chunk_df.drop(columns=[self.label_name])\n            else:\n                labels_np = None\n                features_df = chunk_df\n\n            features_np = features_df.to_numpy(dtype=\"float32\", copy=False)\n\n            if labels_np is not None:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    y = torch.as_tensor(labels_np[row_idx], dtype=self.dtype)\n                    yield x, y\n            else:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    yield x\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Resolve files eagerly and log basic configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Resolve files eagerly and log basic configuration.\"\"\"\n    self.files = self._resolve_files()\n    logger.info(\n        \"Initialized StreamingCSVDataset with {n_files} file(s), \"\n        \"chunksize={chunksize}, label_name={label}\",\n        n_files=len(self.files),\n        chunksize=self.chunksize,\n        label=self.label_name,\n    )\n</code></pre>"},{"location":"api/frameworks/pytorch.html#training","title":"Training","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers","title":"mlpotion.frameworks.pytorch.training.trainers","text":"<p>PyTorch model training.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers.ModelTrainer","title":"ModelTrainer","text":"<p>         Bases: <code>ModelTrainerProtocol[nn.Module, DataLoader]</code></p> <p>Generic trainer for PyTorch models.</p> <p>This class implements the <code>ModelTrainerProtocol</code> for PyTorch models. It handles the training loop, device placement, loss calculation, backpropagation, and validation.</p> <p>It supports: - Supervised learning (batch is <code>(inputs, targets)</code>). - Unsupervised/Self-supervised learning (batch is <code>inputs</code> only, loss is <code>fn(outputs, inputs)</code>). - Custom loss functions (string alias, <code>nn.Module</code>, or callable). - Automatic device management (CPU/GPU).</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>nn.Module</code> <p>The PyTorch model to train.</p> <code>dataloader</code> <code>DataLoader</code> <p>The training data loader.</p> <code>config</code> <code>ModelTrainingConfig</code> <p>Configuration for training (epochs, optimizer, etc.).</p> Example <pre><code>import torch\nimport torch.nn as nn\nfrom mlpotion.frameworks.pytorch import ModelTrainer\nfrom mlpotion.frameworks.pytorch.config import ModelTrainingConfig\n\n# Define model\nmodel = nn.Linear(10, 1)\n\n# Define config\nconfig = ModelTrainingConfig(\n    epochs=5,\n    learning_rate=0.01,\n    optimizer=\"adam\",\n    loss_fn=\"mse\",\n    device=\"cpu\"\n)\n\n# Initialize trainer\ntrainer = ModelTrainer()\n\n# Train\nresult = trainer.train(model, train_loader, config, val_loader)\nprint(result.metrics)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelTrainingConfig,\n    validation_dataloader: DataLoader[Any] | None = None,\n) -&gt; TrainingResult[nn.Module]\n</code></pre> <p>Train a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model (<code>nn.Module</code>) to train.</p> required <code>dataloader</code> <code>DataLoader[Any]</code> <p>The <code>DataLoader</code> providing training data.</p> required <code>config</code> <code>ModelTrainingConfig</code> <p>A <code>ModelTrainingConfig</code> object containing training parameters.</p> required <code>validation_dataloader</code> <code>DataLoader[Any] | None</code> <p>Optional <code>DataLoader</code> for validation.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[nn.Module]</code> <p>TrainingResult[nn.Module]: A dataclass containing the trained model,</p> <code>TrainingResult[nn.Module]</code> <p>training history (loss/metrics per epoch), and final metrics.</p> <p>Raises:</p> Type Description <code>TrainingError</code> <p>If the training loop encounters an error (e.g., NaN loss,</p> Source code in <code>mlpotion/frameworks/pytorch/training/trainers.py</code> <pre><code>@trycatch(\n    error=ModelTrainerError,\n    success_msg=\"\u2705 Successfully trained PyTorch model\",\n)\ndef train(\n    self,\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelTrainingConfig,\n    validation_dataloader: DataLoader[Any] | None = None,\n) -&gt; TrainingResult[nn.Module]:\n    \"\"\"Train a PyTorch model.\n\n    Args:\n        model: The PyTorch model (`nn.Module`) to train.\n        dataloader: The `DataLoader` providing training data.\n        config: A `ModelTrainingConfig` object containing training parameters.\n        validation_dataloader: Optional `DataLoader` for validation.\n\n    Returns:\n        TrainingResult[nn.Module]: A dataclass containing the trained model,\n        training history (loss/metrics per epoch), and final metrics.\n\n    Raises:\n        TrainingError: If the training loop encounters an error (e.g., NaN loss,\n        device mismatch, empty dataloader).\n    \"\"\"\n    try:\n        logger.info(\"Starting PyTorch model training...\")\n        logger.info(\n            \"Config: epochs={epochs}, lr={lr}, optimizer={opt}, \"\n            \"loss_fn={loss_fn}, device={device}\",\n            epochs=config.epochs,\n            lr=config.learning_rate,\n            opt=config.optimizer,\n            loss_fn=config.loss_fn,\n            device=config.device,\n        )\n\n        # Setup device\n        device = torch.device(config.device)\n        model = model.to(device)\n\n        # Setup optimizer and loss\n        optimizer = self._create_optimizer(model, config)\n        criterion = self._create_loss_fn(config)\n\n        # Optional limit on batches per epoch\n        max_batches_per_epoch = getattr(config, \"max_batches_per_epoch\", None)\n        if max_batches_per_epoch is None:\n            max_batches_per_epoch = getattr(config, \"max_batches\", None)\n\n        history: dict[str, list[float]] = {\"loss\": []}\n        if validation_dataloader is not None:\n            history[\"val_loss\"] = []\n\n        # Initialize callbacks and TensorBoard\n        callbacks = self._prepare_callbacks(config)\n        tensorboard_writer = self._setup_tensorboard(config)\n\n        start_time = time.time()\n\n        # Call on_train_begin callbacks\n        for callback in callbacks:\n            if hasattr(callback, \"on_train_begin\"):\n                callback.on_train_begin()\n\n        for epoch in range(config.epochs):\n            model.train()\n            epoch_loss = 0.0\n            num_batches = 0\n\n            for batch in dataloader:\n                inputs, targets = self._prepare_batch(batch, device=device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n\n                # Supervised vs unsupervised / autoencoder\n                if targets is not None:\n                    loss = criterion(outputs, targets)\n                else:\n                    loss = criterion(outputs, inputs)\n\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += float(loss.item())\n                num_batches += 1\n\n                if (\n                    max_batches_per_epoch is not None\n                    and num_batches &gt;= max_batches_per_epoch\n                ):\n                    logger.info(\n                        \"Reached max_batches_per_epoch={mb}; \"\n                        \"stopping epoch {epoch} early.\",\n                        mb=max_batches_per_epoch,\n                        epoch=epoch + 1,\n                    )\n                    break\n\n            if num_batches == 0:\n                raise TrainingError(\"Training dataloader yielded no batches.\")\n\n            avg_loss = epoch_loss / num_batches\n            history[\"loss\"].append(avg_loss)\n\n            # Validation phase\n            if validation_dataloader is not None:\n                val_loss = self._validate(\n                    model=model,\n                    dataloader=validation_dataloader,\n                    criterion=criterion,\n                    device=device,\n                )\n                history[\"val_loss\"].append(val_loss)\n            else:\n                val_loss = None\n\n            # Logging\n            if getattr(config, \"verbose\", True):\n                msg = f\"Epoch {epoch + 1}/{config.epochs} - loss: {avg_loss:.4f}\"\n                if val_loss is not None:\n                    msg += f\" - val_loss: {val_loss:.4f}\"\n                logger.info(msg)\n\n            # TensorBoard logging\n            if tensorboard_writer is not None:\n                tensorboard_writer.add_scalar(\"loss\", avg_loss, epoch)\n                if val_loss is not None:\n                    tensorboard_writer.add_scalar(\"val_loss\", val_loss, epoch)\n\n            # Call on_epoch_end callbacks\n            for callback in callbacks:\n                if hasattr(callback, \"on_epoch_end\"):\n                    callback.on_epoch_end(\n                        epoch, {\"loss\": avg_loss, \"val_loss\": val_loss}\n                    )\n\n        training_time = time.time() - start_time\n\n        # Final metrics\n        metrics: dict[str, float] = {\"loss\": float(history[\"loss\"][-1])}\n        if \"val_loss\" in history and history[\"val_loss\"]:\n            metrics[\"val_loss\"] = float(history[\"val_loss\"][-1])\n\n        best_epoch = self._find_best_epoch(history)\n\n        # Call on_train_end callbacks\n        for callback in callbacks:\n            if hasattr(callback, \"on_train_end\"):\n                callback.on_train_end()\n\n        # Close TensorBoard writer\n        if tensorboard_writer is not None:\n            tensorboard_writer.close()\n\n        logger.info(\"Training completed in {t:.2f}s\", t=training_time)\n        logger.info(\"Final metrics: {metrics}\", metrics=metrics)\n\n        return TrainingResult(\n            model=model,\n            history=history,\n            metrics=metrics,\n            config=config,\n            training_time=training_time,\n            best_epoch=best_epoch,\n        )\n\n    except TrainingError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise TrainingError(f\"Training failed: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#evaluation","title":"Evaluation","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators","title":"mlpotion.frameworks.pytorch.evaluation.evaluators","text":"<p>PyTorch model evaluation.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators.ModelEvaluator","title":"ModelEvaluator","text":"<p>         Bases: <code>ModelEvaluatorProtocol[nn.Module, DataLoader]</code></p> <p>Generic evaluator for PyTorch models.</p> <p>This class implements the <code>ModelEvaluatorProtocol</code> for PyTorch models. It performs a full pass over the evaluation dataset, computing the average loss.</p> <p>It supports: - Supervised and unsupervised evaluation. - Custom loss functions. - Automatic device management.</p> Example <pre><code>from mlpotion.frameworks.pytorch import ModelEvaluator\nfrom mlpotion.frameworks.pytorch.config import ModelEvaluationConfig\n\nevaluator = ModelEvaluator()\nconfig = ModelEvaluationConfig(loss_fn=\"cross_entropy\", device=\"cuda\")\n\nresult = evaluator.evaluate(model, test_loader, config)\nprint(f\"Test Loss: {result.metrics['loss']}\")\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to evaluate.</p> required <code>dataloader</code> <code>DataLoader[Any]</code> <p>The <code>DataLoader</code> providing evaluation data.</p> required <code>config</code> <code>ModelEvaluationConfig</code> <p>A <code>ModelEvaluationConfig</code> object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>A dataclass containing the computed metrics (e.g., average loss)</p> <code>EvaluationResult</code> <p>and execution time.</p> <p>Raises:</p> Type Description <code>EvaluationError</code> <p>If evaluation fails.</p> Source code in <code>mlpotion/frameworks/pytorch/evaluation/evaluators.py</code> <pre><code>@trycatch(\n    error=ModelEvaluatorError,\n    success_msg=\"\u2705 Successfully evaluated PyTorch model\",\n)\ndef evaluate(\n    self,\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a PyTorch model.\n\n    Args:\n        model: The PyTorch model to evaluate.\n        dataloader: The `DataLoader` providing evaluation data.\n        config: A `ModelEvaluationConfig` object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: A dataclass containing the computed metrics (e.g., average loss)\n        and execution time.\n\n    Raises:\n        EvaluationError: If evaluation fails.\n    \"\"\"\n    try:\n        device_str = getattr(config, \"device\", \"cpu\")\n        logger.info(\"Starting PyTorch model evaluation...\")\n        logger.info(\n            f\"Config: device={device_str}, loss_fn={getattr(config, 'loss_fn', 'mse')}\"\n        )\n\n        device = torch.device(device_str)\n        model = model.to(device)\n        model.eval()\n\n        criterion = self._create_loss_fn(config)\n\n        # Support optional max_batches on the config\n        max_batches = getattr(config, \"max_batches\", None)\n\n        total_loss = 0.0\n        num_batches = 0\n        start_time = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                inputs, targets = self._prepare_batch(batch, device=device)\n\n                outputs = model(inputs)\n\n                if targets is not None:\n                    loss = criterion(outputs, targets)\n                else:\n                    loss = criterion(outputs, inputs)\n\n                total_loss += float(loss.item())\n                num_batches += 1\n\n                if max_batches is not None and num_batches &gt;= max_batches:\n                    logger.info(\n                        f\"Reached max_batches={max_batches}; \"\n                        \"stopping evaluation early.\"\n                    )\n                    break\n\n        if num_batches == 0:\n            raise EvaluationError(\"Evaluation dataloader yielded no batches.\")\n\n        avg_loss = total_loss / num_batches\n        evaluation_time = time.time() - start_time\n\n        metrics = {\"loss\": float(avg_loss)}\n\n        logger.info(f\"Evaluation completed in {evaluation_time:.2f}s\")\n        logger.info(f\"Metrics: {metrics}\")\n\n        return EvaluationResult(\n            metrics=metrics,\n            config=config,\n            evaluation_time=evaluation_time,\n        )\n\n    except EvaluationError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise EvaluationError(f\"Evaluation failed: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#persistence","title":"Persistence","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence","title":"mlpotion.frameworks.pytorch.deployment.persistence","text":"<p>PyTorch model persistence.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence","title":"ModelPersistence  <code>dataclass</code>","text":"<p>         Bases: <code>ModelPersistenceProtocol[nn.Module]</code></p> <p>Persistence helper for PyTorch models.</p> <p>This class manages saving and loading of PyTorch models. It supports two modes: 1. State Dict (Recommended): Saves only the model parameters (<code>model.state_dict()</code>).    Requires the model class to be available when loading. 2. Full Model: Saves the entire model object using pickle. Less portable but easier to load.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str | Path</code> <p>The file path for the model artifact.</p> <code>model</code> <code>nn.Module | None</code> <p>The PyTorch model instance.</p> Example <p>Saving and Loading State Dict (Recommended): <pre><code>from mlpotion.frameworks.pytorch import ModelPersistence\nimport torch.nn as nn\n\n# Define model\nclass MyModel(nn.Module):\n    def __init__(self): super().__init__(); self.l = nn.Linear(1, 1)\n\nmodel = MyModel()\n\n# Save\nsaver = ModelPersistence(path=\"model.pth\", model=model)\nsaver.save(save_full_model=False)\n\n# Load\nloader = ModelPersistence(path=\"model.pth\")\n# We must provide the model class or an instance for state_dict loading\nloaded_model = loader.load(model_class=MyModel)\n</code></pre></p> Example <p>Saving and Loading Full Model: <pre><code># Save\nsaver.save(save_full_model=True)\n\n# Load (no model class needed)\nloader = ModelPersistence(path=\"model.pth\")\nloaded_model = loader.load()\n</code></pre></p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence-attributes","title":"Attributes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence.path_obj","title":"path_obj  <code>writable</code> <code>property</code>","text":"<pre><code>path_obj: Path\n</code></pre> <p>Return the model path as a <code>Path</code>.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence.load","title":"load","text":"<pre><code>load(\n    *,\n    model_class: type[nn.Module] | None = None,\n    map_location: str | torch.device | None = \"cpu\",\n    strict: bool = True,\n    model_kwargs: dict[str, Any] | None = None,\n    **torch_load_kwargs: Any\n) -&gt; tuple[nn.Module, dict[str, Any] | None]\n</code></pre> <p>Load a PyTorch model from disk.</p> <p>This method automatically detects if the file is a full model checkpoint or a state dict.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>type[nn.Module] | None</code> <p>The model class to instantiate if loading a state dict and no model instance is currently attached.</p> <code>None</code> <code>map_location</code> <code>str | torch.device | None</code> <p>Device to load the model onto (default: \"cpu\").</p> <code>'cpu'</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce state dict keys match the model.</p> <code>True</code> <code>model_kwargs</code> <code>dict[str, Any] | None</code> <p>Arguments to pass to <code>model_class</code> constructor.</p> <code>None</code> <code>**torch_load_kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>torch.load()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[nn.Module, dict[str, Any] | None]</code> <p>nn.Module: The loaded PyTorch model.</p> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If loading fails, or if <code>model_class</code> is missing</p> Source code in <code>mlpotion/frameworks/pytorch/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully loaded PyTorch model\",\n)\ndef load(\n    self,\n    *,\n    model_class: type[nn.Module] | None = None,\n    map_location: str | torch.device | None = \"cpu\",\n    strict: bool = True,\n    model_kwargs: dict[str, Any] | None = None,\n    **torch_load_kwargs: Any,\n) -&gt; tuple[nn.Module, dict[str, Any] | None]:\n    \"\"\"Load a PyTorch model from disk.\n\n    This method automatically detects if the file is a full model checkpoint or a\n    state dict.\n\n    Args:\n        model_class: The model class to instantiate if loading a state dict and no\n            model instance is currently attached.\n        map_location: Device to load the model onto (default: \"cpu\").\n        strict: Whether to strictly enforce state dict keys match the model.\n        model_kwargs: Arguments to pass to `model_class` constructor.\n        **torch_load_kwargs: Additional arguments passed to `torch.load()`.\n\n    Returns:\n        nn.Module: The loaded PyTorch model.\n\n    Raises:\n        ModelPersistenceError: If loading fails, or if `model_class` is missing\n        when required.\n    \"\"\"\n    path = self._ensure_path_exists()\n\n    logger.info(\"Loading PyTorch model from {path}\", path=str(path))\n\n    checkpoint = torch.load(path, map_location=map_location, **torch_load_kwargs)\n\n    # Case 1: full model was saved\n    if isinstance(checkpoint, nn.Module):\n        logger.info(\"Detected full-model checkpoint (nn.Module).\")\n        self.model = checkpoint\n        logger.info(\"PyTorch model loaded successfully from full-model checkpoint.\")\n        return checkpoint, None\n\n    # Case 2: dict-like checkpoint (state_dict or wrapped)\n    if isinstance(checkpoint, dict):\n        logger.info(\"Detected dict-like checkpoint; treating as state_dict.\")\n        state_dict = self._extract_state_dict(checkpoint)\n\n        # If we already have a model attached, reuse it; otherwise, we need model_class\n        if self.model is not None:\n            model = self.model\n            logger.info(\n                \"Using attached model instance of type {cls} for state_dict loading.\",\n                cls=type(model).__name__,\n            )\n        else:\n            if model_class is None:\n                raise ModelPersistenceError(\n                    \"model_class is required when loading from a state_dict \"\n                    \"checkpoint if no model is attached.\"\n                )\n            model = self._instantiate_model(model_class, model_kwargs)\n            self.model = model\n\n        missing, unexpected = model.load_state_dict(state_dict, strict=strict)\n\n        if strict:\n            logger.debug(\n                \"State_dict loaded with strict=True (no mismatch error raised).\"\n            )\n        else:\n            if missing:\n                logger.warning(f\"Missing keys in state_dict: {missing}\")\n            if unexpected:\n                logger.warning(f\"Unexpected keys in state_dict: {unexpected}\")\n\n        logger.info(\"PyTorch model loaded successfully from state_dict checkpoint.\")\n        return model, None\n\n    # Case 3: unsupported checkpoint structure\n    raise ModelPersistenceError(\n        f\"Unsupported checkpoint type: {type(checkpoint)!r}. \"\n        \"Expected nn.Module or dict-like object.\"\n    )\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence.save","title":"save","text":"<pre><code>save(\n    *,\n    save_full_model: bool = False,\n    **torch_save_kwargs: Any\n) -&gt; None\n</code></pre> <p>Save the attached PyTorch model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>save_full_model</code> <code>bool</code> <p>If True, saves the entire model object (pickle). If False (default), saves only the <code>state_dict</code>.</p> <code>False</code> <code>**torch_save_kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>torch.save()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If no model is attached or saving fails.</p> Source code in <code>mlpotion/frameworks/pytorch/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully saved PyTorch model\",\n)\ndef save(\n    self,\n    *,\n    save_full_model: bool = False,\n    **torch_save_kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the attached PyTorch model to disk.\n\n    Args:\n        save_full_model: If True, saves the entire model object (pickle).\n            If False (default), saves only the `state_dict`.\n        **torch_save_kwargs: Additional arguments passed to `torch.save()`.\n\n    Raises:\n        ModelPersistenceError: If no model is attached or saving fails.\n    \"\"\"\n    model = self._ensure_model()\n    path = self.path_obj\n\n    logger.info(\n        \"Saving PyTorch model to {path} ({mode})\",\n        path=str(path),\n        mode=\"full model\" if save_full_model else \"state_dict\",\n    )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    if save_full_model:\n        logger.warning(\n            \"Saving a full model object. This is less portable and may break \"\n            \"if the code structure changes. Prefer saving a state_dict for \"\n            \"long-term storage.\"\n        )\n        torch.save(model, path, **torch_save_kwargs)\n    else:\n        torch.save(model.state_dict(), path, **torch_save_kwargs)\n\n    logger.info(\"PyTorch model saved successfully.\")\n</code></pre>"},{"location":"api/frameworks/pytorch.html#export","title":"Export","text":"<p> See the PyTorch Guide for usage examples </p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters","title":"mlpotion.frameworks.pytorch.deployment.exporters","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters.ModelExporter","title":"ModelExporter  <code>dataclass</code>","text":"<p>         Bases: <code>ModelExporterProtocol[nn.Module]</code></p> <p>Export PyTorch models to TorchScript, ONNX, or state_dict formats.</p> <p>This class implements the <code>ModelExporterProtocol</code> for PyTorch. It supports exporting models for deployment or interoperability.</p> <p>Supported formats: - torchscript: Exports via <code>torch.jit.script</code> or <code>torch.jit.trace</code>. - onnx: Exports to ONNX format (requires <code>example_input</code>). - state_dict: Saves the model parameters.</p> Example <pre><code>from mlpotion.frameworks.pytorch import ModelExporter\nfrom mlpotion.frameworks.pytorch.config import ModelExportConfig\nimport torch\n\n# Prepare model and input\nmodel = ...\nexample_input = torch.randn(1, 3, 224, 224)\n\n# Export to ONNX\nexporter = ModelExporter()\nconfig = ModelExportConfig(\n    export_path=\"models/model.onnx\",\n    format=\"onnx\",\n    example_input=example_input\n)\n\nresult = exporter.export(model, config)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters.ModelExporter-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters.ModelExporter.export","title":"export","text":"<pre><code>export(\n    model: nn.Module, config: ModelExportConfig\n) -&gt; ExportResult\n</code></pre> <p>Export a PyTorch model to the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to export.</p> required <code>config</code> <code>ModelExportConfig</code> <p>Configuration object specifying format, path, and other options.</p> required <p>Returns:</p> Name Type Description <code>ExportResult</code> <code>ExportResult</code> <p>A dataclass containing the path to the exported artifact and metadata.</p> <p>Raises:</p> Type Description <code>ExportError</code> <p>If the export process fails (e.g., invalid format, missing example input).</p> Source code in <code>mlpotion/frameworks/pytorch/deployment/exporters.py</code> <pre><code>@trycatch(\n    error=ModelExporterError,\n    success_msg=\"\u2705 Successfully Exported model\",\n)\ndef export(\n    self,\n    model: nn.Module,\n    config: ModelExportConfig,\n) -&gt; ExportResult:\n    \"\"\"Export a PyTorch model to the specified format.\n\n    Args:\n        model: The PyTorch model to export.\n        config: Configuration object specifying format, path, and other options.\n\n    Returns:\n        ExportResult: A dataclass containing the path to the exported artifact and metadata.\n\n    Raises:\n        ExportError: If the export process fails (e.g., invalid format, missing example input).\n    \"\"\"\n    try:\n        export_root = Path(config.export_path)\n        export_root.parent.mkdir(parents=True, exist_ok=True)\n\n        fmt = config.format.lower()\n        device_str = getattr(config, \"device\", \"cpu\")\n        device = torch.device(device_str)\n\n        logger.info(\n            \"Exporting PyTorch model \"\n            f\"[format={fmt}, device={device_str}, target={export_root}]\"\n        )\n\n        model = model.to(device)\n        model.eval()\n\n        # Dispatch\n        if fmt == \"torchscript\":\n            final_path = self._export_torchscript(\n                model=model,\n                export_root=export_root,\n                config=config,\n                device=device,\n            )\n        elif fmt == \"onnx\":\n            final_path = self._export_onnx(\n                model=model,\n                export_root=export_root,\n                config=config,\n                device=device,\n            )\n        elif fmt == \"state_dict\":\n            final_path = self._export_state_dict(\n                model=model,\n                export_root=export_root,\n            )\n        else:\n            raise ExportError(f\"Unknown export format: {config.format!r}\")\n\n        logger.success(f\"Model successfully exported \u2192 {final_path}\")\n\n        metadata: dict[str, Any] = {\n            \"model_type\": \"pytorch\",\n            \"format\": fmt,\n            \"device\": device_str,\n        }\n\n        return ExportResult(\n            export_path=str(final_path),\n            format=fmt,\n            config=config,\n            metadata=metadata,\n        )\n\n    except ExportError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise ExportError(f\"Export failed: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/tensorflow.html","title":"TensorFlow API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's TensorFlow components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p> <p>Extensibility</p> <p>These components are built using protocol-based design, making MLPotion easy to extend. Want to add new data sources, training methods, or integrations? See Contributing Guide.</p>"},{"location":"api/frameworks/tensorflow.html#data-loading","title":"Data Loading","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders","title":"mlpotion.frameworks.tensorflow.data.loaders","text":"<p>TensorFlow data loaders.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.CSVDataLoader","title":"CSVDataLoader","text":"<pre><code>CSVDataLoader(\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[dict[str, Any]], dict[str, Any]]\n    | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>         Bases: <code>DataLoader[tf.data.Dataset]</code></p> <p>Load CSV files into TensorFlow datasets.</p> <p>This class provides a convenient wrapper around <code>tf.data.experimental.make_csv_dataset</code>, adding validation, logging, and configuration management. It handles file pattern matching, column selection, and label separation.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load. If None, all columns are loaded.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label. If None, no labels are returned.</p> <code>map_fn</code> <code>Callable | None</code> <p>Optional function to map over the dataset (e.g., for preprocessing).</p> <code>config</code> <code>dict | None</code> <p>Additional configuration passed to <code>make_csv_dataset</code>.</p> Example <pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\n# Simple usage\nloader = CSVDataLoader(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    batch_size=64,\n    config={\"num_epochs\": 5, \"shuffle\": True}\n)\n\ndataset = loader.load()\n\n# Iterate\nfor features, labels in dataset:\n    print(features['some_column'].shape)\n    break\n</code></pre> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[dict[str, Any]], dict[str, Any]] | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None:\n    self.file_pattern = file_pattern\n    self.column_names = column_names\n    self.label_name = label_name\n    self.batch_size = batch_size\n    self.map_fn = map_fn\n\n    # set default config\n    _default_config = {\"ignore_errors\": True, \"num_epochs\": 1}\n    self.config: dict[str, Any] = dict(config or _default_config)\n\n    # Extract and validate num_epochs *once* so we don't risk duplicating kwargs\n    self.num_epochs = self._extract_and_validate_num_epochs()\n\n    self._validate_files_exist()\n    self._validate_finite_dataset()\n\n    logger.info(\n        \"{class_name} initialized with attrs: {attrs}\",\n        class_name=self.__class__.__name__,\n        attrs=vars(self),\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.CSVDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.CSVDataLoader.load","title":"load","text":"<pre><code>load() -&gt; tf.data.Dataset\n</code></pre> <p>Load CSV files into a TensorFlow dataset.</p> <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: A <code>tf.data.Dataset</code> yielding tuples of <code>(features, labels)</code> if <code>label_name</code></p> <code>tf.data.Dataset</code> <p>is provided, or just <code>features</code> (dict) if not.</p> <p>Raises:</p> Type Description <code>DataLoadingError</code> <p>If no files match the pattern, or if <code>num_epochs</code> is invalid.</p> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully loaded dataset\",\n)\ndef load(self) -&gt; tf.data.Dataset:\n    \"\"\"Load CSV files into a TensorFlow dataset.\n\n    Returns:\n        tf.data.Dataset: A `tf.data.Dataset` yielding tuples of `(features, labels)` if `label_name`\n        is provided, or just `features` (dict) if not.\n\n    Raises:\n        DataLoadingError: If no files match the pattern, or if `num_epochs` is invalid.\n    \"\"\"\n    dataset = tf.data.experimental.make_csv_dataset(\n        file_pattern=self.file_pattern,\n        batch_size=self.batch_size,\n        label_name=self.label_name,\n        column_names=self.column_names,\n        num_epochs=self.num_epochs,  # extracted and validated\n        **self.config,\n    )\n\n    if self.map_fn:\n        logger.info(\"Applying mapping function to dataset\")\n        dataset = dataset.map(self.map_fn)\n\n    # Attach metadata for CSV materializer\n    # This allows ZenML to efficiently serialize/deserialize the dataset\n    # by storing just the configuration instead of the actual data\n    dataset._csv_config = {\n        \"file_pattern\": self.file_pattern,\n        \"batch_size\": self.batch_size,\n        \"label_name\": self.label_name,\n        \"column_names\": self.column_names,\n        \"num_epochs\": self.num_epochs,\n        \"extra_params\": self.config,\n        \"transformations\": [],  # Will be populated by optimizer if used\n    }\n\n    return dataset\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.RecordDataLoader","title":"RecordDataLoader","text":"<pre><code>RecordDataLoader(\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[tf.Tensor], Any] | None = None,\n    element_spec_json: str | dict[str, Any] | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>         Bases: <code>DataLoader[tf.data.Dataset]</code></p> <p>Loader for TFRecord files into tf.data.Dataset.</p> <p>This class facilitates loading data from TFRecord files, which is the recommended format for high-performance TensorFlow pipelines. It supports parsing examples, handling nested structures via <code>element_spec</code>, and applying common dataset optimizations.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the TFRecord files.</p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific feature keys to extract.</p> <code>label_name</code> <code>str | None</code> <p>Key of the label feature.</p> <code>map_fn</code> <code>Callable | None</code> <p>Optional function to map over the dataset.</p> <code>element_spec_json</code> <code>str | dict | None</code> <p>JSON or dict describing the data structure (optional).</p> <code>config</code> <code>dict | None</code> <p>Configuration for reading (e.g., <code>num_parallel_reads</code>, <code>compression_type</code>).</p> Example <pre><code>from mlpotion.frameworks.tensorflow import RecordDataLoader\n\nloader = RecordDataLoader(\n    file_pattern=\"data/records/*.tfrecord\",\n    batch_size=128,\n    label_name=\"label\",\n    config={\n        \"compression_type\": \"GZIP\",\n        \"num_parallel_reads\": tf.data.AUTOTUNE\n    }\n)\n\ndataset = loader.load()\n</code></pre> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[tf.Tensor], Any] | None = None,\n    element_spec_json: str | dict[str, Any] | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None:\n    self.file_pattern = file_pattern\n    self.batch_size = batch_size\n    self.map_fn = map_fn\n    self.element_spec_json = element_spec_json\n    self.column_names = column_names\n    self.label_name = label_name\n\n    # set config\n    self.config = config or {}\n\n    # validate files exist\n    self._validate_files_exist()\n\n    logger.info(\n        \"{class_name} initialized with attrs: {attrs}\",\n        class_name=self.__class__.__name__,\n        attrs=vars(self),\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.RecordDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.RecordDataLoader.load","title":"load","text":"<pre><code>load() -&gt; tf.data.Dataset\n</code></pre> <p>Load TFRecord files into a tf.data.Dataset.</p> <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: Parsed and optionally mapped dataset of (features, label) or features only.</p> <p>Raises:</p> Type Description <code>DataLoadingError</code> <p>on failure.</p> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully loaded TFRecord dataset\",\n)\ndef load(self) -&gt; tf.data.Dataset:\n    \"\"\"Load TFRecord files into a tf.data.Dataset.\n\n    Returns:\n        tf.data.Dataset: Parsed and optionally mapped dataset of (features, label) or features only.\n    Raises:\n        DataLoadingError: on failure.\n    \"\"\"\n    filenames = self._get_files_matching_pattern()\n\n    ds = tf.data.TFRecordDataset(\n        filenames=filenames,\n        compression_type=self.config.get(\"compression_type\", \"\"),\n        buffer_size=self.config.get(\"buffer_size\", None),\n        num_parallel_reads=self.config.get(\"num_parallel_reads\", tf.data.AUTOTUNE),\n    )\n\n    # Optionally repeat\n    if \"repeat_count\" in self.config:\n        ds = ds.repeat(self.config[\"repeat_count\"])\n\n    # Apply column/label selection\n    ds = ds.map(\n        self._apply_column_label_selection,\n        num_parallel_calls=self.config.get(\"num_parallel_reads\", tf.data.AUTOTUNE),\n    )\n\n    # Shuffle if requested\n    if \"shuffle_buffer_size\" in self.config:\n        ds = ds.shuffle(self.config[\"shuffle_buffer_size\"])\n\n    # Batch\n    ds = ds.batch(\n        self.batch_size, drop_remainder=self.config.get(\"drop_remainder\", False)\n    )\n\n    # Prefetch\n    ds = ds.prefetch(self.config.get(\"prefetch_buffer_size\", tf.data.AUTOTUNE))\n\n    # Apply mapping function\n    if self.map_fn:\n        logger.info(\"Applying mapping function to dataset\")\n        ds = ds.map(self.map_fn)\n\n    return ds\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers","title":"mlpotion.frameworks.tensorflow.data.optimizers","text":"<p>TensorFlow dataset optimization.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer","title":"DatasetOptimizer","text":"<pre><code>DatasetOptimizer(\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n) -&gt; None\n</code></pre> <p>         Bases: <code>DatasetOptimizerProtocol[tf.data.Dataset]</code></p> <p>Optimize TensorFlow datasets for training performance.</p> <p>This class applies a standard set of performance optimizations to a <code>tf.data.Dataset</code>: caching, shuffling, batching, and prefetching. These are critical for preventing data loading bottlenecks during training.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The number of samples per batch.</p> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Size of the shuffle buffer. If None, shuffling is disabled.</p> <code>prefetch</code> <code>bool</code> <p>Whether to prefetch data (uses <code>tf.data.AUTOTUNE</code>).</p> <code>cache</code> <code>bool</code> <p>Whether to cache the dataset in memory.</p> Example <pre><code>from mlpotion.frameworks.tensorflow import DatasetOptimizer\n\n# Create optimizer\noptimizer = DatasetOptimizer(\n    batch_size=32,\n    shuffle_buffer_size=1000,\n    cache=True,\n    prefetch=True\n)\n\n# Apply to a raw dataset\noptimized_dataset = optimizer.optimize(raw_dataset)\n</code></pre> <p>Initialize dataset optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>32</code> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Buffer size for shuffling (None = no shuffle)</p> <code>None</code> <code>prefetch</code> <code>bool</code> <p>Whether to prefetch batches</p> <code>True</code> <code>cache</code> <code>bool</code> <p>Whether to cache dataset in memory</p> <code>False</code> Source code in <code>mlpotion/frameworks/tensorflow/data/optimizers.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n) -&gt; None:\n    \"\"\"Initialize dataset optimizer.\n\n    Args:\n        batch_size: Batch size\n        shuffle_buffer_size: Buffer size for shuffling (None = no shuffle)\n        prefetch: Whether to prefetch batches\n        cache: Whether to cache dataset in memory\n    \"\"\"\n    self.batch_size = batch_size\n    self.shuffle_buffer_size = shuffle_buffer_size\n    self.prefetch = prefetch\n    self.cache = cache\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: DataOptimizationConfig,\n) -&gt; DatasetOptimizer\n</code></pre> <p>Create optimizer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DataOptimizationConfig</code> <p>Optimization configuration</p> required <p>Returns:</p> Type Description <code>DatasetOptimizer</code> <p>Configured optimizer instance</p> Source code in <code>mlpotion/frameworks/tensorflow/data/optimizers.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DataOptimizationConfig) -&gt; \"DatasetOptimizer\":\n    \"\"\"Create optimizer from configuration.\n\n    Args:\n        config: Optimization configuration\n\n    Returns:\n        Configured optimizer instance\n    \"\"\"\n    return cls(\n        batch_size=config.batch_size,\n        shuffle_buffer_size=config.shuffle_buffer_size,\n        prefetch=config.prefetch,\n        cache=config.cache,\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: tf.data.Dataset) -&gt; tf.data.Dataset\n</code></pre> <p>Optimize dataset for training.</p> <p>Applies optimizations in the following order: 1. Cache: Caches data in memory (if enabled). 2. Shuffle: Randomizes data order (if <code>shuffle_buffer_size</code> is set). 3. Batch: Groups data into batches. 4. Prefetch: Prepares the next batch while the current one is being processed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset</code> <p>The input <code>tf.data.Dataset</code>.</p> required <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: The optimized dataset pipeline.</p> Source code in <code>mlpotion/frameworks/tensorflow/data/optimizers.py</code> <pre><code>def optimize(self, dataset: tf.data.Dataset) -&gt; tf.data.Dataset:\n    \"\"\"Optimize dataset for training.\n\n    Applies optimizations in the following order:\n    1. **Cache**: Caches data in memory (if enabled).\n    2. **Shuffle**: Randomizes data order (if `shuffle_buffer_size` is set).\n    3. **Batch**: Groups data into batches.\n    4. **Prefetch**: Prepares the next batch while the current one is being processed.\n\n    Args:\n        dataset: The input `tf.data.Dataset`.\n\n    Returns:\n        tf.data.Dataset: The optimized dataset pipeline.\n    \"\"\"\n    logger.info(\"Applying dataset optimizations...\")\n\n    # Track transformations for CSV materializer\n    transformations = []\n    if hasattr(dataset, \"_csv_config\"):\n        transformations = dataset._csv_config.get(\"transformations\", [])\n\n    # Cache first (before shuffling/batching)\n    if self.cache:\n        logger.info(\"Caching dataset in memory\")\n        dataset = dataset.cache()\n        # Note: cache() doesn't need to be recorded as it's a performance optimization\n\n    # Shuffle before batching\n    if self.shuffle_buffer_size:\n        logger.info(f\"Shuffling with buffer size {self.shuffle_buffer_size}\")\n        dataset = dataset.shuffle(\n            buffer_size=self.shuffle_buffer_size,\n            reshuffle_each_iteration=True,\n        )\n        transformations.append(\n            {\n                \"type\": \"shuffle\",\n                \"params\": {\"buffer_size\": self.shuffle_buffer_size},\n            }\n        )\n\n    # Batch\n    logger.info(f\"Batching with size {self.batch_size}\")\n    dataset = dataset.batch(self.batch_size)\n    transformations.append(\n        {\n            \"type\": \"batch\",\n            \"params\": {\"batch_size\": self.batch_size},\n        }\n    )\n\n    # Prefetch last for best performance\n    if self.prefetch:\n        logger.info(\"Prefetching with AUTOTUNE\")\n        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        transformations.append(\n            {\n                \"type\": \"prefetch\",\n                \"params\": {\"buffer_size\": \"AUTOTUNE\"},\n            }\n        )\n\n    # Preserve CSV config if it exists\n    if hasattr(dataset, \"_csv_config\"):\n        dataset._csv_config[\"transformations\"] = transformations\n\n    return dataset\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#training","title":"Training","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers","title":"mlpotion.frameworks.tensorflow.training.trainers","text":"<p>TensorFlow model trainers.</p> <p>This module re-exports the Keras <code>ModelTrainer</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers.ModelTrainer","title":"ModelTrainer  <code>dataclass</code>","text":"<p>         Bases: <code>ModelTrainerProtocol[Model, Sequence]</code></p> <p>Generic trainer for Keras 3 models.</p> <p>This class implements the <code>ModelTrainerProtocol</code> for Keras models, providing a standardized interface for training. It wraps the standard <code>model.fit()</code> method but adds flexibility and consistency checks.</p> <p>It supports: - Automatic model compilation if <code>compile_params</code> are provided. - Handling of various data formats (tuples, dicts, generators). - Standardized return format (dictionary of history metrics).</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelTrainer\n\n# Prepare data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize trainer\ntrainer = ModelTrainer()\n\n# Train\nhistory = trainer.train(\n    model=model,\n    data=(X_train, y_train),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    fit_params={\n        \"epochs\": 5,\n        \"batch_size\": 32,\n        \"verbose\": 1\n    }\n)\n\nprint(history['loss'])\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]\n</code></pre> <p>Train a Keras model using the provided dataset and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to train.</p> required <code>dataset</code> <code>Any</code> <p>The training data. Can be a tuple <code>(x, y)</code>, a dictionary, a <code>Sequence</code>, or a generator.</p> required <code>config</code> <code>ModelTrainingConfig</code> <p>Configuration object containing training parameters.</p> required <code>validation_dataset</code> <code>Any | None</code> <p>Optional validation data.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[Model]</code> <p>TrainingResult[Model]: An object containing the trained model, training history, and metrics.</p> Source code in <code>mlpotion/frameworks/keras/training/trainers.py</code> <pre><code>@trycatch(\n    error=ModelTrainerError,\n    success_msg=\"\u2705 Successfully trained Keras model\",\n)\ndef train(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]:\n    \"\"\"Train a Keras model using the provided dataset and configuration.\n\n    Args:\n        model: The Keras model to train.\n        dataset: The training data. Can be a tuple `(x, y)`, a dictionary, a `Sequence`, or a generator.\n        config: Configuration object containing training parameters.\n        validation_dataset: Optional validation data.\n\n    Returns:\n        TrainingResult[Model]: An object containing the trained model, training history, and metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare compile parameters from config\n    compile_params = {\n        \"optimizer\": self._get_optimizer(config),\n        \"loss\": config.loss,\n        \"metrics\": config.metrics,\n    }\n\n    # Compile if needed or if forced by config (though we usually respect existing compilation)\n    # Here we'll ensure it's compiled. If the user wants to use their own compilation,\n    # they should probably compile it before passing it, but our config implies we control it.\n    # However, to be safe and flexible:\n    if not self._is_compiled(model):\n        if not config.optimizer or not config.loss:\n            raise RuntimeError(\n                \"Model is not compiled and config does not provide optimizer and loss. \"\n                \"Either compile the model beforehand or provide optimizer and loss in config.\"\n            )\n        logger.info(\"Compiling model with config parameters.\")\n        model.compile(**compile_params)\n    else:\n        logger.info(\"Model already compiled. Using existing compilation settings.\")\n\n    # Prepare fit parameters\n    fit_kwargs = {\n        \"epochs\": config.epochs,\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"shuffle\": config.shuffle,\n        \"validation_split\": config.validation_split,\n        \"callbacks\": self._prepare_callbacks(config),\n    }\n\n    if validation_dataset is not None:\n        fit_kwargs[\"validation_data\"] = validation_dataset\n\n    # Add any framework-specific options\n    fit_kwargs.update(config.framework_options)\n\n    logger.info(\"Starting Keras model training...\")\n    logger.debug(f\"Training data type: {type(dataset)!r}\")\n    logger.debug(f\"Fit parameters: {fit_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    history_obj = self._call_fit(model=model, data=dataset, fit_kwargs=fit_kwargs)\n\n    training_time = time.time() - start_time\n\n    # Convert History object to dict[str, list[float]]\n    history_dict = self._history_to_dict(history_obj)\n\n    # Extract final metrics\n    final_metrics = {}\n    for k, v in history_dict.items():\n        if v:\n            final_metrics[k] = v[-1]\n\n    logger.info(\"Training completed.\")\n    logger.debug(f\"Training history: {history_dict}\")\n\n    return TrainingResult(\n        model=model,\n        history=history_dict,\n        metrics=final_metrics,\n        config=config,\n        training_time=training_time,\n        best_epoch=None,  # Keras history doesn't explicitly track \"best\" unless using callbacks\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#evaluation","title":"Evaluation","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators","title":"mlpotion.frameworks.tensorflow.evaluation.evaluators","text":"<p>TensorFlow model evaluators.</p> <p>This module re-exports the Keras <code>ModelEvaluator</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators.ModelEvaluator","title":"ModelEvaluator  <code>dataclass</code>","text":"<p>         Bases: <code>ModelEvaluatorProtocol[Model, Sequence]</code></p> <p>Generic evaluator for Keras 3 models.</p> <p>This class implements the <code>ModelEvaluatorProtocol</code> for Keras models. It wraps the <code>model.evaluate()</code> method to provide a consistent evaluation interface.</p> <p>It ensures that the evaluation result is always returned as a dictionary of metric names to values, regardless of how the model was compiled or what arguments were passed.</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelEvaluator\n\n# Prepare data\nX_test = np.random.rand(20, 10)\ny_test = np.random.randint(0, 2, 20)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize evaluator\nevaluator = ModelEvaluator()\n\n# Evaluate\nmetrics = evaluator.evaluate(\n    model=model,\n    data=(X_test, y_test),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    eval_params={\"batch_size\": 32}\n)\n\nprint(metrics)  # {'loss': 0.693..., 'accuracy': 0.5...}\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a Keras model on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to evaluate.</p> required <code>dataset</code> <code>Any</code> <p>The evaluation data. Can be a tuple <code>(x, y)</code>, a dictionary, or a <code>Sequence</code>.</p> required <code>config</code> <code>ModelEvaluationConfig</code> <p>Configuration object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>An object containing the evaluation metrics.</p> Source code in <code>mlpotion/frameworks/keras/evaluation/evaluators.py</code> <pre><code>@trycatch(\n    error=ModelEvaluatorError,\n    success_msg=\"\u2705 Successfully evaluated Keras model\",\n)\ndef evaluate(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a Keras model on the given data.\n\n    Args:\n        model: The Keras model to evaluate.\n        dataset: The evaluation data. Can be a tuple `(x, y)`, a dictionary, or a `Sequence`.\n        config: Configuration object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: An object containing the evaluation metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare eval parameters\n    eval_kwargs = {\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"return_dict\": True,\n    }\n\n    # Add any framework-specific options\n    eval_kwargs.update(config.framework_options)\n\n    # We assume the model is already compiled. If not, Keras will raise an error\n    # unless we provide compile params, but EvaluationConfig doesn't typically carry them.\n    # The user should ensure the model is compiled (e.g. after loading or training).\n    if not self._is_compiled(model):\n        logger.warning(\n            \"Model is not compiled. Evaluation might fail if loss/metrics are not defined.\"\n        )\n\n    logger.info(\"Evaluating Keras model...\")\n    logger.debug(f\"Evaluation data type: {type(dataset)!r}\")\n    logger.debug(f\"Evaluation parameters: {eval_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    result = self._call_evaluate(model=model, data=dataset, eval_kwargs=eval_kwargs)\n\n    evaluation_time = time.time() - start_time\n\n    # At this point, result should be a dict[str, float]\n    if not isinstance(result, dict):\n        # Defensive fallback if user or Keras changed behavior\n        logger.warning(\n            f\"`model.evaluate` did not return a dict (got {type(result)!r}). \"\n            \"Wrapping into a dict under key 'metric_0'.\"\n        )\n        result = {\"metric_0\": float(result)}\n\n    metrics = {str(k): float(v) for k, v in result.items()}\n    logger.info(f\"Evaluation result: {metrics}\")\n\n    return EvaluationResult(\n        metrics=metrics,\n        config=config,\n        evaluation_time=evaluation_time,\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#persistence","title":"Persistence","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence","title":"mlpotion.frameworks.tensorflow.deployment.persistence","text":"<p>TensorFlow model persistence.</p> <p>This module re-exports the Keras <code>ModelPersistence</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence.ModelPersistence","title":"ModelPersistence","text":"<pre><code>ModelPersistence(\n    path: str | Path, model: Model | None = None\n) -&gt; None\n</code></pre> <p>         Bases: <code>ModelPersistenceProtocol[Model]</code></p> <p>Persistence helper for Keras models.</p> <p>This class manages saving and loading of Keras models. It supports standard Keras formats (<code>.keras</code>, <code>.h5</code>) and SavedModel directories. It also integrates with <code>ModelInspector</code> to provide model metadata upon loading.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Path</code> <p>The file path for the model artifact.</p> <code>model</code> <code>Model | None</code> <p>The Keras model instance (optional).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelPersistence\n\n# Define model\nmodel = keras.Sequential([keras.layers.Dense(1)])\n\n# Save\nsaver = ModelPersistence(path=\"models/my_model.keras\", model=model)\nsaver.save()\n\n# Load\nloader = ModelPersistence(path=\"models/my_model.keras\")\nloaded_model, metadata = loader.load(inspect=True)\nprint(metadata['parameters'])\n</code></pre> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>def __init__(self, path: str | Path, model: Model | None = None) -&gt; None:\n    self._path = Path(path)\n    self._model = model\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence.ModelPersistence-attributes","title":"Attributes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.model","title":"model  <code>writable</code> <code>property</code>","text":"<pre><code>model: Model | None\n</code></pre> <p>Currently attached Keras model (may be None before loading).</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.path","title":"path  <code>writable</code> <code>property</code>","text":"<pre><code>path: Path\n</code></pre> <p>Filesystem path where the model is saved/loaded.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.load","title":"load","text":"<pre><code>load(\n    *, inspect: bool = True, **kwargs: Any\n) -&gt; tuple[Model, dict[str, Any] | None]\n</code></pre> <p>Load a Keras model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inspect</code> <code>bool</code> <p>Whether to inspect the loaded model and return metadata.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>keras.models.load_model()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Model</code> <p>tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and</p> <code>dict[str, Any] | None</code> <p>optional inspection metadata.</p> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If the model file cannot be found or loaded.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully loaded Keras model\",\n)\ndef load(\n    self,\n    *,\n    inspect: bool = True,\n    **kwargs: Any,\n) -&gt; tuple[Model, dict[str, Any] | None]:\n    \"\"\"Load a Keras model from disk.\n\n    Args:\n        inspect: Whether to inspect the loaded model and return metadata.\n        **kwargs: Additional arguments passed to `keras.models.load_model()`.\n\n    Returns:\n        tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and\n        optional inspection metadata.\n\n    Raises:\n        ModelPersistenceError: If the model file cannot be found or loaded.\n    \"\"\"\n    path = self._ensure_path_exists()\n\n    logger.info(f\"Loading Keras model from: {path!s}\")\n    model = keras.models.load_model(path.as_posix(), **kwargs)\n\n    self._model = model  # keep instance in sync\n\n    inspection_result: dict[str, Any] | None = None\n    if inspect:\n        logger.info(\"Inspecting loaded Keras model with ModelInspector.\")\n        inspector = ModelInspector()\n        inspection_result = inspector.inspect(model)\n\n    return model, inspection_result\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.save","title":"save","text":"<pre><code>save(overwrite: bool = True, **kwargs: Any) -&gt; None\n</code></pre> <p>Save the attached model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>model.save()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If no model is attached or if the file exists and <code>overwrite</code> is False.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully saved Keras model\",\n)\ndef save(\n    self,\n    overwrite: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the attached model to disk.\n\n    Args:\n        overwrite: Whether to overwrite the file if it already exists.\n        **kwargs: Additional arguments passed to `model.save()`.\n\n    Raises:\n        ModelPersistenceError: If no model is attached or if the file exists and `overwrite` is False.\n    \"\"\"\n    model = self._ensure_model()\n    target = self._path\n\n    if target.exists() and not overwrite:\n        raise ModelPersistenceError(\n            f\"Target path already exists and overwrite=False: {target!s}\"\n        )\n\n    logger.info(f\"Saving Keras model to: {target!s}\")\n    target.parent.mkdir(parents=True, exist_ok=True)\n\n    # Keras 3 generally infers format from the path; `save_format` is\n    # deprecated / discouraged in newer APIs, so we do NOT pass it.\n    model.save(target.as_posix(), **kwargs)\n    logger.info(\"Keras model saved successfully.\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#export","title":"Export","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters","title":"mlpotion.frameworks.tensorflow.deployment.exporters","text":"<p>TensorFlow model exporters.</p> <p>This module re-exports the Keras <code>ModelExporter</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters.ModelExporter","title":"ModelExporter","text":"<p>         Bases: <code>ModelExporterProtocol[Model]</code></p> <p>Generic exporter for Keras 3 models.</p> <p>This class implements <code>ModelExporterProtocol</code> and supports exporting Keras models to various formats, including native Keras formats (<code>.keras</code>, <code>.h5</code>) and inference formats like TensorFlow SavedModel or ONNX (via <code>model.export</code>).</p> <p>It also supports creating export archives with custom endpoints using <code>keras.export.ExportArchive</code>.</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelExporter\n\nmodel = keras.Sequential([keras.layers.Dense(1)])\nexporter = ModelExporter()\n\n# Export as standard Keras file\nexporter.export(model, \"models/model.keras\")\n\n# Export for serving (TF SavedModel)\nexporter.export(model, \"models/serving\", export_format=\"tf_saved_model\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters.ModelExporter-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter.export","title":"export","text":"<pre><code>export(model: Model, path: str, **kwargs: Any) -&gt; None\n</code></pre> <p>Export a Keras model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to export.</p> required <code>path</code> <code>str</code> <p>The destination path or directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional export options: - <code>export_format</code> (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc. - <code>dataset</code> (Iterable): Optional data for model warmup. - <code>endpoint_name</code> (str): Name for custom endpoint (uses ExportArchive). - <code>input_specs</code> (list[InputSpec]): Input signatures for custom endpoint. - <code>config</code> (dict): Extra arguments for the underlying save/export method.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelExporterError</code> <p>If export fails.</p> Source code in <code>mlpotion/frameworks/keras/deployment/exporters.py</code> <pre><code>@trycatch(\n    error=ModelExporterError,\n    success_msg=\"\u2705 Successfully Exported model\",\n)\ndef export(self, model: Model, path: str, **kwargs: Any) -&gt; None:\n    \"\"\"Export a Keras model to disk.\n\n    Args:\n        model: The Keras model to export.\n        path: The destination path or directory.\n        **kwargs: Additional export options:\n            - `export_format` (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc.\n            - `dataset` (Iterable): Optional data for model warmup.\n            - `endpoint_name` (str): Name for custom endpoint (uses ExportArchive).\n            - `input_specs` (list[InputSpec]): Input signatures for custom endpoint.\n            - `config` (dict): Extra arguments for the underlying save/export method.\n\n    Raises:\n        ModelExporterError: If export fails.\n    \"\"\"\n    export_path = Path(path)\n\n    export_format: str | None = kwargs.pop(\"export_format\", None)\n    dataset: Iterable[Any] | None = kwargs.pop(\"dataset\", None)\n    endpoint_name: str | None = kwargs.pop(\"endpoint_name\", None)\n    input_specs: Sequence[InputSpec] | None = kwargs.pop(\"input_specs\", None)\n    config: Mapping[str, Any] | None = kwargs.pop(\"config\", None)\n\n    if kwargs:\n        logger.warning(\n            \"Unused export kwargs passed to ModelExporter: \"\n            f\"{list(kwargs.keys())}\"\n        )\n\n    self._validate_model(model)\n    self._validate_config(config)\n\n    # Determine mode if export_format isn't explicitly set\n    if export_format is None:\n        export_format = self._infer_export_format_from_path(export_path)\n\n    logger.info(\n        f\"Exporting Keras model '{model.name}' to {export_path!s} \"\n        f\"with format '{export_format}'\"\n    )\n\n    # Optional warm-up pass\n    self._warmup_if_needed(model=model, dataset=dataset)\n\n    # Choose strategy\n    try:\n        if self._is_native_keras_format(export_format):\n            self._save_native_keras(model=model, path=export_path, config=config)\n        elif endpoint_name is not None or input_specs is not None:\n            self._export_with_export_archive(\n                model=model,\n                path=export_path,\n                endpoint_name=endpoint_name or self.default_endpoint_name,\n                input_specs=input_specs,\n                export_format=export_format,\n            )\n        else:\n            self._export_with_model_export(\n                model=model,\n                path=export_path,\n                export_format=export_format,\n                config=config,\n            )\n    except ValueError as err:\n        logger.warning(\n            f\"Export error: {err} \"\n            \"(you may need to build the model by calling it on example data \"\n            \"before exporting)\"\n        )\n\n    logger.info(f\"Model export completed: {export_path!s}\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#model-inspection","title":"Model Inspection","text":"<p> See the TensorFlow Guide for usage examples </p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection","title":"mlpotion.frameworks.tensorflow.models.inspection","text":"<p>TensorFlow model inspection.</p> <p>This module re-exports the Keras <code>ModelInspector</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection.ModelInspector","title":"ModelInspector  <code>dataclass</code>","text":"<p>         Bases: <code>ModelInspectorProtocol[ModelLike]</code></p> <p>Inspector for Keras models.</p> <p>This class analyzes Keras models to extract metadata such as input/output shapes, parameter counts, layer details, and signatures. It is useful for validating models before training or deployment, and for generating model reports.</p> <p>Attributes:</p> Name Type Description <code>include_layers</code> <code>bool</code> <p>Whether to include detailed information about each layer.</p> <code>include_signatures</code> <code>bool</code> <p>Whether to include model signatures (if available).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelInspector\n\nmodel = keras.Sequential([keras.layers.Dense(1, input_shape=(10,))])\ninspector = ModelInspector()\n\ninfo = inspector.inspect(model)\nprint(f\"Total params: {info['parameters']['total']}\")\nprint(f\"Inputs: {info['inputs']}\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection.ModelInspector-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.models.inspection.ModelInspector.inspect","title":"inspect","text":"<pre><code>inspect(model: ModelLike) -&gt; dict[str, Any]\n</code></pre> <p>Inspect a Keras model and return structured metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelLike</code> <p>The Keras model to inspect.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing model metadata: - <code>name</code>: Model name. - <code>backend</code>: Keras backend used. - <code>trainable</code>: Whether the model is trainable. - <code>inputs</code>: List of input specifications. - <code>outputs</code>: List of output specifications. - <code>parameters</code>: Dictionary of parameter counts. - <code>layers</code>: List of layer details (if <code>include_layers=True</code>). - <code>signatures</code>: Model signatures (if <code>include_signatures=True</code>).</p> Source code in <code>mlpotion/frameworks/keras/models/inspection.py</code> <pre><code>@trycatch(\n    error=ModelInspectorError,\n    success_msg=\"\u2705 Successfully inspected Keras model\",\n)\ndef inspect(self, model: ModelLike) -&gt; dict[str, Any]:\n    \"\"\"Inspect a Keras model and return structured metadata.\n\n    Args:\n        model: The Keras model to inspect.\n\n    Returns:\n        dict[str, Any]: A dictionary containing model metadata:\n            - `name`: Model name.\n            - `backend`: Keras backend used.\n            - `trainable`: Whether the model is trainable.\n            - `inputs`: List of input specifications.\n            - `outputs`: List of output specifications.\n            - `parameters`: Dictionary of parameter counts.\n            - `layers`: List of layer details (if `include_layers=True`).\n            - `signatures`: Model signatures (if `include_signatures=True`).\n    \"\"\"\n    if not isinstance(model, keras.Model):\n        raise TypeError(\n            f\"ModelInspector expects a keras.Model, got {type(model)!r}\"\n        )\n\n    logger.info(\"Inspecting Keras model...\")\n\n    backend_name = self._get_backend_name()\n\n    info: dict[str, Any] = {\n        \"name\": model.name,\n        \"backend\": backend_name,\n        \"trainable\": model.trainable,\n    }\n\n    info[\"inputs\"] = self._get_inputs(model)\n    info[\"input_names\"] = [input[\"name\"] for input in info[\"inputs\"]]\n    info[\"outputs\"] = self._get_outputs(model)\n    info[\"output_names\"] = [output[\"name\"] for output in info[\"outputs\"]]\n    info[\"parameters\"] = self._get_param_counts(model)\n\n    if self.include_signatures:\n        info[\"signatures\"] = self._get_signatures(model)\n\n    if self.include_layers:\n        info[\"layers\"] = self._get_layers_summary(model)\n\n    logger.debug(f\"Keras model inspection result: {info}\")\n    return info\n</code></pre>"},{"location":"api/integrations/flowyml.html","title":"FlowyML Integration API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's FlowyML integration \u2014 auto-generated from source code docstrings.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code. See the FlowyML Integration Guide for usage examples and tutorials.</p>"},{"location":"api/integrations/flowyml.html#core-adapter","title":"Core Adapter","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters","title":"mlpotion.integrations.flowyml.adapters","text":"<p>FlowyML Adapter \u2014 Bridge MLPotion protocols to FlowyML steps.</p> <p>Provides factory methods that wrap MLPotion's protocol-compliant components (DataLoader, ModelTrainer, ModelEvaluator) into fully-configured FlowyML Step objects with asset outputs, caching, retry, resource specs, and tags.</p>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters-classes","title":"Classes","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters.FlowyMLAdapter","title":"FlowyMLAdapter","text":"<p>Adapt MLPotion components into FlowyML pipeline steps.</p> <p>Unlike the ZenML adapter which returns raw objects, these steps return FlowyML Asset objects (Dataset, Model, Metrics) with automatic metadata extraction and lineage tracking.</p> <p>Example::</p> <pre><code>from mlpotion.frameworks.keras.data.loaders import CSVDataLoader\nfrom mlpotion.integrations.flowyml import FlowyMLAdapter\n\nloader = CSVDataLoader(file_path=\"data.csv\", batch_size=32)\nload_step = FlowyMLAdapter.create_data_loader_step(loader)\n\n# Use in a FlowyML pipeline\npipeline = Pipeline(\"my_pipeline\")\npipeline.add_step(load_step)\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters.FlowyMLAdapter-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters.FlowyMLAdapter.create_data_loader_step","title":"create_data_loader_step  <code>staticmethod</code>","text":"<pre><code>create_data_loader_step(\n    loader: DataLoader[DatasetT],\n    *,\n    name: str | None = None,\n    cache: bool | str | Callable = \"code_hash\",\n    retry: int = 0,\n    resources: Any | None = None,\n    tags: dict[str, str] | None = None\n) -&gt; Step\n</code></pre> <p>Wrap a DataLoader as a FlowyML step returning a Dataset asset.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>DataLoader[DatasetT]</code> <p>Any MLPotion DataLoader protocol implementation.</p> required <code>name</code> <code>str | None</code> <p>Step name (defaults to 'load_data').</p> <code>None</code> <code>cache</code> <code>bool | str | Callable</code> <p>Caching strategy ('code_hash', 'input_hash', False).</p> <code>'code_hash'</code> <code>retry</code> <code>int</code> <p>Number of retry attempts on failure.</p> <code>0</code> <code>resources</code> <code>Any | None</code> <p>ResourceRequirements for this step.</p> <code>None</code> <code>tags</code> <code>dict[str, str] | None</code> <p>Metadata tags for observability.</p> <code>None</code> <p>Returns:</p> Type Description <code>Step</code> <p>A FlowyML Step that loads data and returns a Dataset asset.</p> Source code in <code>mlpotion/integrations/flowyml/adapters.py</code> <pre><code>@staticmethod\ndef create_data_loader_step(\n    loader: DataLoader[DatasetT],\n    *,\n    name: str | None = None,\n    cache: bool | str | Callable = \"code_hash\",\n    retry: int = 0,\n    resources: Any | None = None,\n    tags: dict[str, str] | None = None,\n) -&gt; Step:\n    \"\"\"Wrap a DataLoader as a FlowyML step returning a Dataset asset.\n\n    Args:\n        loader: Any MLPotion DataLoader protocol implementation.\n        name: Step name (defaults to 'load_data').\n        cache: Caching strategy ('code_hash', 'input_hash', False).\n        retry: Number of retry attempts on failure.\n        resources: ResourceRequirements for this step.\n        tags: Metadata tags for observability.\n\n    Returns:\n        A FlowyML Step that loads data and returns a Dataset asset.\n    \"\"\"\n\n    @step(\n        name=name or \"load_data\",\n        outputs=[\"dataset\"],\n        cache=cache,\n        retry=retry,\n        resources=resources,\n        tags=tags or {\"component\": \"data_loader\"},\n    )\n    def load_data() -&gt; Dataset:\n        raw_data = loader.load()\n        return Dataset.create(\n            data=raw_data,\n            name=\"loaded_dataset\",\n            tags={\"source\": type(loader).__name__},\n        )\n\n    return load_data\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters.FlowyMLAdapter.create_training_step","title":"create_training_step  <code>staticmethod</code>","text":"<pre><code>create_training_step(\n    trainer: ModelTrainer[ModelT, DatasetT],\n    *,\n    name: str | None = None,\n    cache: bool | str | Callable = False,\n    retry: int = 0,\n    resources: Any | None = None,\n    tags: dict[str, str] | None = None\n) -&gt; Step\n</code></pre> <p>Wrap a ModelTrainer as a FlowyML step returning a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>ModelTrainer[ModelT, DatasetT]</code> <p>Any MLPotion ModelTrainer protocol implementation.</p> required <code>name</code> <code>str | None</code> <p>Step name (defaults to 'train_model').</p> <code>None</code> <code>cache</code> <code>bool | str | Callable</code> <p>Caching strategy (default: False for training).</p> <code>False</code> <code>retry</code> <code>int</code> <p>Number of retry attempts on failure.</p> <code>0</code> <code>resources</code> <code>Any | None</code> <p>ResourceRequirements (e.g., GPU config).</p> <code>None</code> <code>tags</code> <code>dict[str, str] | None</code> <p>Metadata tags for observability.</p> <code>None</code> <p>Returns:</p> Type Description <code>Step</code> <p>A FlowyML Step that trains a model and returns a Model asset.</p> Source code in <code>mlpotion/integrations/flowyml/adapters.py</code> <pre><code>@staticmethod\ndef create_training_step(\n    trainer: ModelTrainer[ModelT, DatasetT],\n    *,\n    name: str | None = None,\n    cache: bool | str | Callable = False,\n    retry: int = 0,\n    resources: Any | None = None,\n    tags: dict[str, str] | None = None,\n) -&gt; Step:\n    \"\"\"Wrap a ModelTrainer as a FlowyML step returning a Model asset.\n\n    Args:\n        trainer: Any MLPotion ModelTrainer protocol implementation.\n        name: Step name (defaults to 'train_model').\n        cache: Caching strategy (default: False for training).\n        retry: Number of retry attempts on failure.\n        resources: ResourceRequirements (e.g., GPU config).\n        tags: Metadata tags for observability.\n\n    Returns:\n        A FlowyML Step that trains a model and returns a Model asset.\n    \"\"\"\n\n    @step(\n        name=name or \"train_model\",\n        inputs=[\"model\", \"dataset\", \"config\"],\n        outputs=[\"trained_model\"],\n        cache=cache,\n        retry=retry,\n        resources=resources,\n        tags=tags or {\"component\": \"model_trainer\"},\n    )\n    def train_model(\n        model: ModelT,\n        dataset: DatasetT,\n        config: Any,\n        validation_dataset: DatasetT | None = None,\n    ) -&gt; Model:\n        result = trainer.train(model, dataset, config, validation_dataset)\n        return Model.create(\n            data=result.model,\n            name=\"trained_model\",\n            training_history=getattr(result, \"history\", None),\n            tags={\"trainer\": type(trainer).__name__},\n        )\n\n    return train_model\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.adapters.FlowyMLAdapter.create_evaluation_step","title":"create_evaluation_step  <code>staticmethod</code>","text":"<pre><code>create_evaluation_step(\n    evaluator: ModelEvaluator[ModelT, DatasetT],\n    *,\n    name: str | None = None,\n    cache: bool | str | Callable = \"input_hash\",\n    retry: int = 0,\n    resources: Any | None = None,\n    tags: dict[str, str] | None = None\n) -&gt; Step\n</code></pre> <p>Wrap a ModelEvaluator as a FlowyML step returning a Metrics asset.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>ModelEvaluator[ModelT, DatasetT]</code> <p>Any MLPotion ModelEvaluator protocol implementation.</p> required <code>name</code> <code>str | None</code> <p>Step name (defaults to 'evaluate_model').</p> <code>None</code> <code>cache</code> <code>bool | str | Callable</code> <p>Caching strategy (default: 'input_hash').</p> <code>'input_hash'</code> <code>retry</code> <code>int</code> <p>Number of retry attempts on failure.</p> <code>0</code> <code>resources</code> <code>Any | None</code> <p>ResourceRequirements for this step.</p> <code>None</code> <code>tags</code> <code>dict[str, str] | None</code> <p>Metadata tags for observability.</p> <code>None</code> <p>Returns:</p> Type Description <code>Step</code> <p>A FlowyML Step that evaluates a model and returns a Metrics asset.</p> Source code in <code>mlpotion/integrations/flowyml/adapters.py</code> <pre><code>@staticmethod\ndef create_evaluation_step(\n    evaluator: ModelEvaluator[ModelT, DatasetT],\n    *,\n    name: str | None = None,\n    cache: bool | str | Callable = \"input_hash\",\n    retry: int = 0,\n    resources: Any | None = None,\n    tags: dict[str, str] | None = None,\n) -&gt; Step:\n    \"\"\"Wrap a ModelEvaluator as a FlowyML step returning a Metrics asset.\n\n    Args:\n        evaluator: Any MLPotion ModelEvaluator protocol implementation.\n        name: Step name (defaults to 'evaluate_model').\n        cache: Caching strategy (default: 'input_hash').\n        retry: Number of retry attempts on failure.\n        resources: ResourceRequirements for this step.\n        tags: Metadata tags for observability.\n\n    Returns:\n        A FlowyML Step that evaluates a model and returns a Metrics asset.\n    \"\"\"\n\n    @step(\n        name=name or \"evaluate_model\",\n        inputs=[\"model\", \"dataset\"],\n        outputs=[\"metrics\"],\n        cache=cache,\n        retry=retry,\n        resources=resources,\n        tags=tags or {\"component\": \"model_evaluator\"},\n    )\n    def evaluate_model(\n        model: ModelT,\n        dataset: DatasetT,\n        config: Any | None = None,\n    ) -&gt; Metrics:\n        result = evaluator.evaluate(model, dataset, config)\n        return Metrics.create(\n            name=\"evaluation_metrics\",\n            metrics=result.metrics if hasattr(result, \"metrics\") else result,\n            tags={\"evaluator\": type(evaluator).__name__},\n        )\n\n    return evaluate_model\n</code></pre>"},{"location":"api/integrations/flowyml.html#keras-integration","title":"Keras Integration","text":""},{"location":"api/integrations/flowyml.html#steps","title":"Steps","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps","title":"mlpotion.integrations.flowyml.keras.steps","text":"<p>FlowyML Keras steps \u2014 Full-featured pipeline steps for Keras workflows.</p> <p>Each step leverages FlowyML's native capabilities: - Artifact-centric design: returns Dataset, Model, Metrics with auto-extraction - Supports caching, retry, GPU resources, tags, DAG wiring, and execution groups - train_model integrates FlowymlKerasCallback for automatic tracking</p>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps-classes","title":"Classes","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.load_data","title":"load_data","text":"<pre><code>load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    dtype: str = \"float32\",\n) -&gt; Dataset\n</code></pre> <p>Load CSV data into a Keras-compatible CSVSequence, wrapped as a Dataset asset.</p> <p>Automatic metadata extraction captures batch count, batch size, source path, column names, and label information.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the sequence.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the label/target column.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load (None = all).</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>dtype</code> <code>str</code> <p>Data type for numeric conversion.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset wrapping the CSVSequence with auto-extracted metadata.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_load_data\",\n    outputs=[\"dataset\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"keras\", \"component\": \"data_loader\"},\n)\ndef load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    dtype: str = \"float32\",\n) -&gt; Dataset:\n    \"\"\"Load CSV data into a Keras-compatible CSVSequence, wrapped as a Dataset asset.\n\n    Automatic metadata extraction captures batch count, batch size, source path,\n    column names, and label information.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Batch size for the sequence.\n        label_name: Name of the label/target column.\n        column_names: Specific columns to load (None = all).\n        shuffle: Whether to shuffle the data.\n        dtype: Data type for numeric conversion.\n\n    Returns:\n        Dataset asset wrapping the CSVSequence with auto-extracted metadata.\n    \"\"\"\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        column_names=column_names,\n        label_name=label_name,\n        shuffle=shuffle,\n        dtype=dtype,\n    )\n    loader = CSVDataLoader(**config.dict())\n    sequence = loader.load()\n\n    dataset = Dataset.create(\n        data=sequence,\n        name=\"keras_csv_dataset\",\n        properties={\n            \"source\": file_path,\n            \"batch_size\": batch_size,\n            \"batches\": len(sequence),\n            \"label_name\": label_name,\n            \"shuffle\": shuffle,\n            \"dtype\": dtype,\n        },\n        source=file_path,\n        loader=\"CSVDataLoader\",\n        framework=\"keras\",\n    )\n\n    logger.info(\n        f\"\ud83d\udce6 Loaded dataset: {len(sequence)} batches, \"\n        f\"batch_size={batch_size}, source={file_path}\"\n    )\n    return dataset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    dataset: Dataset,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    batch_size: int | None = None,\n    feature_names: list[str] | None = None,\n    input_columns: list[str] | None = None,\n) -&gt; Dataset\n</code></pre> <p>Transform data using a Keras model and save predictions to CSV.</p> <p>Returns a Dataset asset with lineage linked to the input dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Input Dataset asset wrapping a CSVSequence.</p> required <code>model</code> <code>keras.Model</code> <p>Keras model for generating predictions.</p> required <code>data_output_path</code> <code>str</code> <p>Output path for transformed data.</p> required <code>data_output_per_batch</code> <code>bool</code> <p>If True, output one file per batch.</p> <code>False</code> <code>batch_size</code> <code>int | None</code> <p>Optional batch size override.</p> <code>None</code> <code>feature_names</code> <code>list[str] | None</code> <p>Optional feature names for output CSV.</p> <code>None</code> <code>input_columns</code> <code>list[str] | None</code> <p>Optional input columns to pass to model.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset pointing to the output CSV with parent lineage.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_transform_data\",\n    inputs=[\"dataset\"],\n    outputs=[\"transformed\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"keras\", \"component\": \"data_transformer\"},\n)\ndef transform_data(\n    dataset: Dataset,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    batch_size: int | None = None,\n    feature_names: list[str] | None = None,\n    input_columns: list[str] | None = None,\n) -&gt; Dataset:\n    \"\"\"Transform data using a Keras model and save predictions to CSV.\n\n    Returns a Dataset asset with lineage linked to the input dataset.\n\n    Args:\n        dataset: Input Dataset asset wrapping a CSVSequence.\n        model: Keras model for generating predictions.\n        data_output_path: Output path for transformed data.\n        data_output_per_batch: If True, output one file per batch.\n        batch_size: Optional batch size override.\n        feature_names: Optional feature names for output CSV.\n        input_columns: Optional input columns to pass to model.\n\n    Returns:\n        Dataset asset pointing to the output CSV with parent lineage.\n    \"\"\"\n    # Extract raw sequence from Dataset if wrapped\n    raw_dataset = dataset.data if isinstance(dataset, Dataset) else dataset\n\n    config = DataTransformationConfig(\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        input_columns=input_columns,\n    )\n    transformer = CSVDataTransformer(\n        dataset=raw_dataset,\n        model=model,\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        input_columns=input_columns,\n    )\n    transformer.transform(dataset=raw_dataset, model=model, config=config)\n\n    # Wrap output as Dataset with parent lineage\n    parent = dataset if isinstance(dataset, Dataset) else None\n    transformed = Dataset.create(\n        data={\"output_path\": data_output_path},\n        name=\"keras_transformed_data\",\n        parent=parent,\n        properties={\n            \"output_path\": data_output_path,\n            \"per_batch\": data_output_per_batch,\n            \"feature_names\": feature_names,\n        },\n        source=data_output_path,\n        transformer=\"CSVDataTransformer\",\n    )\n\n    logger.info(f\"\ud83d\udd04 Transformed data saved to: {data_output_path}\")\n    return transformed\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: keras.Model,\n    data: CSVSequence | Dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    validation_data: CSVSequence | Dataset | None = None,\n    callbacks: list[keras.callbacks.Callback] | None = None,\n    experiment_name: str | None = None,\n    project: str | None = None,\n    log_model: bool = True,\n) -&gt; tuple[Model, Metrics]\n</code></pre> <p>Train a Keras model with FlowyML tracking integration.</p> <p>Automatically attaches a FlowymlKerasCallback for: - Dynamic capture of ALL training metrics - Live dashboard updates - Model artifact logging</p> <p>Returns a Model asset (via Model.from_keras with auto-extracted metadata) and a Metrics asset with training history.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>Compiled Keras model.</p> required <code>data</code> <code>CSVSequence | Dataset</code> <p>Training data as CSVSequence or Dataset asset.</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>0.001</code> <code>verbose</code> <code>int</code> <p>Keras verbosity level.</p> <code>1</code> <code>validation_data</code> <code>CSVSequence | Dataset | None</code> <p>Optional validation CSVSequence or Dataset.</p> <code>None</code> <code>callbacks</code> <code>list[keras.callbacks.Callback] | None</code> <p>Additional Keras callbacks (FlowyML callback auto-added).</p> <code>None</code> <code>experiment_name</code> <code>str | None</code> <p>Experiment name for FlowyML tracking.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>Project name for FlowyML dashboard.</p> <code>None</code> <code>log_model</code> <code>bool</code> <p>Whether to save model artifact after training.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Model, Metrics]</code> <p>Tuple of (Model asset, Metrics asset).</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_train_model\",\n    inputs=[\"dataset\"],\n    outputs=[\"model\", \"training_metrics\"],\n    cache=False,\n    retry=1,\n    tags={\"framework\": \"keras\", \"component\": \"model_trainer\"},\n)\ndef train_model(\n    model: keras.Model,\n    data: CSVSequence | Dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    validation_data: CSVSequence | Dataset | None = None,\n    callbacks: list[keras.callbacks.Callback] | None = None,\n    experiment_name: str | None = None,\n    project: str | None = None,\n    log_model: bool = True,\n) -&gt; tuple[Model, Metrics]:\n    \"\"\"Train a Keras model with FlowyML tracking integration.\n\n    Automatically attaches a FlowymlKerasCallback for:\n    - Dynamic capture of ALL training metrics\n    - Live dashboard updates\n    - Model artifact logging\n\n    Returns a Model asset (via Model.from_keras with auto-extracted metadata)\n    and a Metrics asset with training history.\n\n    Args:\n        model: Compiled Keras model.\n        data: Training data as CSVSequence or Dataset asset.\n        epochs: Number of training epochs.\n        learning_rate: Learning rate.\n        verbose: Keras verbosity level.\n        validation_data: Optional validation CSVSequence or Dataset.\n        callbacks: Additional Keras callbacks (FlowyML callback auto-added).\n        experiment_name: Experiment name for FlowyML tracking.\n        project: Project name for FlowyML dashboard.\n        log_model: Whether to save model artifact after training.\n\n    Returns:\n        Tuple of (Model asset, Metrics asset).\n    \"\"\"\n    # Extract raw data from Dataset if wrapped\n    raw_data = data.data if isinstance(data, Dataset) else data\n    raw_val = (\n        validation_data.data\n        if isinstance(validation_data, Dataset)\n        else validation_data\n    )\n\n    all_callbacks = list(callbacks or [])\n\n    # Auto-attach FlowyML callback for tracking\n    flowyml_callback = FlowymlKerasCallback(\n        experiment_name=experiment_name or f\"keras_train_{uuid.uuid4()}\",\n        project=project,\n        log_model=log_model,\n    )\n    all_callbacks.append(flowyml_callback)\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metrics=[\"mae\"],\n        framework_options={\"callbacks\": all_callbacks} if all_callbacks else {},\n    )\n\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model=model,\n        dataset=raw_data,\n        config=config,\n        validation_dataset=raw_val,\n    )\n\n    # Collect raw metrics\n    raw_metrics: dict[str, Any] = result.metrics if hasattr(result, \"metrics\") else {}\n    if hasattr(result, \"history\") and result.history:\n        raw_metrics[\"history\"] = result.history\n    raw_metrics[\"epochs_completed\"] = epochs\n    raw_metrics[\"learning_rate\"] = learning_rate\n\n    # Wrap as Model asset using from_keras for full auto-extraction\n    model_asset = Model.from_keras(\n        model,\n        name=\"keras_trained_model\",\n        callback=flowyml_callback,\n        epochs_requested=epochs,\n        batch_size=getattr(raw_data, \"batch_size\", None),\n    )\n\n    # Wrap as Metrics asset\n    metrics_asset = Metrics.create(\n        metrics=raw_metrics,\n        name=\"keras_training_metrics\",\n        tags={\"stage\": \"training\", \"framework\": \"keras\"},\n        properties={\n            \"epochs\": epochs,\n            \"learning_rate\": learning_rate,\n            **{k: v for k, v in raw_metrics.items() if k != \"history\"},\n        },\n    )\n\n    logger.info(\n        f\"\ud83c\udfaf Training complete: {epochs} epochs, \"\n        f\"metrics captured: {list(raw_metrics.keys())}\"\n    )\n    return model_asset, metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: keras.Model | Model,\n    data: CSVSequence | Dataset,\n    verbose: int = 0,\n) -&gt; Metrics\n</code></pre> <p>Evaluate a Keras model and return a Metrics asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Trained Keras model or Model asset.</p> required <code>data</code> <code>CSVSequence | Dataset</code> <p>Evaluation data as CSVSequence or Dataset asset.</p> required <code>verbose</code> <code>int</code> <p>Keras verbosity level.</p> <code>0</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics asset with evaluation results.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_evaluate_model\",\n    inputs=[\"model\", \"dataset\"],\n    outputs=[\"metrics\"],\n    cache=\"input_hash\",\n    tags={\"framework\": \"keras\", \"component\": \"model_evaluator\"},\n)\ndef evaluate_model(\n    model: keras.Model | Model,\n    data: CSVSequence | Dataset,\n    verbose: int = 0,\n) -&gt; Metrics:\n    \"\"\"Evaluate a Keras model and return a Metrics asset.\n\n    Args:\n        model: Trained Keras model or Model asset.\n        data: Evaluation data as CSVSequence or Dataset asset.\n        verbose: Keras verbosity level.\n\n    Returns:\n        Metrics asset with evaluation results.\n    \"\"\"\n    # Extract raw objects from assets\n    raw_model = model.data if isinstance(model, Model) else model\n    raw_data = data.data if isinstance(data, Dataset) else data\n\n    config = ModelEvaluationConfig(verbose=verbose)\n    evaluator = ModelEvaluator()\n    result = evaluator.evaluate(model=raw_model, dataset=raw_data, config=config)\n\n    raw_metrics = result.metrics if hasattr(result, \"metrics\") else {}\n\n    metrics_asset = Metrics.create(\n        metrics=raw_metrics,\n        name=\"keras_evaluation_metrics\",\n        tags={\"stage\": \"evaluation\", \"framework\": \"keras\"},\n        properties=raw_metrics,\n    )\n\n    logger.info(f\"\ud83d\udcca Evaluation: {raw_metrics}\")\n    return metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: keras.Model | Model,\n    export_path: str,\n    export_format: str | None = None,\n) -&gt; Model\n</code></pre> <p>Export a Keras model to the specified format, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Keras model or Model asset to export.</p> required <code>export_path</code> <code>str</code> <p>Destination path.</p> required <code>export_format</code> <code>str | None</code> <p>Format ('keras', 'saved_model', 'tflite').</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model asset with export metadata.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_export_model\",\n    inputs=[\"model\"],\n    outputs=[\"exported_model\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"keras\", \"component\": \"model_exporter\"},\n)\ndef export_model(\n    model: keras.Model | Model,\n    export_path: str,\n    export_format: str | None = None,\n) -&gt; Model:\n    \"\"\"Export a Keras model to the specified format, returned as a Model asset.\n\n    Args:\n        model: Keras model or Model asset to export.\n        export_path: Destination path.\n        export_format: Format ('keras', 'saved_model', 'tflite').\n\n    Returns:\n        Model asset with export metadata.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    exporter = ModelExporter()\n    config = {}\n    if export_format:\n        config[\"export_format\"] = export_format\n    exporter.export(model=raw_model, path=export_path, **config)\n\n    model_asset = Model.from_keras(\n        raw_model,\n        name=\"keras_exported_model\",\n        export_path=export_path,\n        export_format=export_format or \"keras\",\n    )\n\n    logger.info(f\"\ud83d\udce4 Exported model to: {export_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: keras.Model | Model, save_path: str\n) -&gt; Model\n</code></pre> <p>Save a Keras model to disk, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Keras model or Model asset to save.</p> required <code>save_path</code> <code>str</code> <p>Destination file path.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>Model asset with save location metadata.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_save_model\",\n    inputs=[\"model\"],\n    outputs=[\"saved_model\"],\n    tags={\"framework\": \"keras\", \"component\": \"model_persistence\"},\n)\ndef save_model(\n    model: keras.Model | Model,\n    save_path: str,\n) -&gt; Model:\n    \"\"\"Save a Keras model to disk, returned as a Model asset.\n\n    Args:\n        model: Keras model or Model asset to save.\n        save_path: Destination file path.\n\n    Returns:\n        Model asset with save location metadata.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    persistence = ModelPersistence(path=save_path, model=raw_model)\n    persistence.save()\n\n    model_asset = Model.from_keras(\n        raw_model,\n        name=\"keras_saved_model\",\n        save_path=save_path,\n    )\n\n    logger.info(f\"\ud83d\udcbe Saved model to: {save_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.load_model","title":"load_model","text":"<pre><code>load_model(model_path: str, inspect: bool = False) -&gt; Model\n</code></pre> <p>Load a Keras model from disk, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the saved model.</p> required <code>inspect</code> <code>bool</code> <p>If True, log model inspection info.</p> <code>False</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model asset wrapping the loaded Keras model.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_load_model\",\n    outputs=[\"model\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"keras\", \"component\": \"model_persistence\"},\n)\ndef load_model(\n    model_path: str,\n    inspect: bool = False,\n) -&gt; Model:\n    \"\"\"Load a Keras model from disk, returned as a Model asset.\n\n    Args:\n        model_path: Path to the saved model.\n        inspect: If True, log model inspection info.\n\n    Returns:\n        Model asset wrapping the loaded Keras model.\n    \"\"\"\n    persistence = ModelPersistence(path=model_path)\n    raw_model, inspection = persistence.load(inspect=inspect)\n\n    model_asset = Model.from_keras(\n        raw_model,\n        name=\"keras_loaded_model\",\n        source_path=model_path,\n    )\n\n    if inspect and inspection:\n        logger.info(f\"\ud83d\udd0d Loaded model from: {model_path}, inspection: {inspection}\")\n    else:\n        logger.info(f\"\ud83d\udd0d Loaded model from: {model_path}\")\n\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.steps.inspect_model","title":"inspect_model","text":"<pre><code>inspect_model(\n    model: keras.Model | Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n) -&gt; Metrics\n</code></pre> <p>Inspect a Keras model and return detailed metadata as a Metrics asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Keras model or Model asset to inspect.</p> required <code>include_layers</code> <code>bool</code> <p>Include per-layer information.</p> <code>True</code> <code>include_signatures</code> <code>bool</code> <p>Include input/output signatures.</p> <code>True</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics asset with model inspection details.</p> Source code in <code>mlpotion/integrations/flowyml/keras/steps.py</code> <pre><code>@step(\n    name=\"keras_inspect_model\",\n    inputs=[\"model\"],\n    outputs=[\"inspection\"],\n    tags={\"framework\": \"keras\", \"component\": \"model_inspector\"},\n)\ndef inspect_model(\n    model: keras.Model | Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n) -&gt; Metrics:\n    \"\"\"Inspect a Keras model and return detailed metadata as a Metrics asset.\n\n    Args:\n        model: Keras model or Model asset to inspect.\n        include_layers: Include per-layer information.\n        include_signatures: Include input/output signatures.\n\n    Returns:\n        Metrics asset with model inspection details.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    inspector = ModelInspector(\n        include_layers=include_layers,\n        include_signatures=include_signatures,\n    )\n    inspection = inspector.inspect(raw_model)\n\n    metrics_asset = Metrics.create(\n        metrics=inspection,\n        name=\"keras_model_inspection\",\n        tags={\"stage\": \"inspection\", \"framework\": \"keras\"},\n        properties={\n            \"model_name\": inspection.get(\"name\", \"unknown\"),\n            \"total_params\": inspection.get(\"parameters\", {}).get(\"total\"),\n        },\n    )\n\n    logger.info(\n        f\"\ud83d\udd0e Model: {inspection.get('name', 'unknown')}, \"\n        f\"params: {inspection.get('parameters', {}).get('total', '?')}\"\n    )\n    return metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#pipelines","title":"Pipelines","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines","title":"mlpotion.integrations.flowyml.keras.pipelines","text":"<p>Pre-built FlowyML pipeline templates for Keras workflows.</p> <p>Provides ready-to-run pipelines that wire MLPotion Keras steps together with proper DAG dependencies, context injection, experiment tracking, and optional scheduling.</p> <p>Available pipelines:</p> <ul> <li>create_keras_training_pipeline  \u2014 Load \u2192 Train \u2192 Evaluate</li> <li>create_keras_full_pipeline      \u2014 Load \u2192 Transform \u2192 Train \u2192 Evaluate \u2192 Export</li> <li>create_keras_evaluation_pipeline \u2014 Load model + data \u2192 Evaluate \u2192 Inspect</li> <li>create_keras_export_pipeline    \u2014 Load model \u2192 Export + Save</li> <li>create_keras_experiment_pipeline \u2014 Full pipeline with experiment tracking &amp; conditional deploy</li> </ul>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines.create_keras_training_pipeline","title":"create_keras_training_pipeline","text":"<pre><code>create_keras_training_pipeline(\n    name: str = \"keras_training\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a ready-to-run Keras training pipeline.</p> <p>DAG: <code>load_data \u2192 train_model \u2192 evaluate_model</code></p> <p>Provide hyperparameters via the context object::</p> <pre><code>from flowyml.core.context import Context\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=10,\n    learning_rate=0.001,\n    experiment_name=\"my-experiment\",\n)\n\npipeline = create_keras_training_pipeline(\n    name=\"my_training\",\n    context=ctx,\n    project_name=\"my_project\",\n)\nresult = pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'keras_training'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters to inject.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string for versioned pipeline.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/keras/pipelines.py</code> <pre><code>def create_keras_training_pipeline(\n    name: str = \"keras_training\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a ready-to-run Keras training pipeline.\n\n    **DAG**: ``load_data \u2192 train_model \u2192 evaluate_model``\n\n    Provide hyperparameters via the context object::\n\n        from flowyml.core.context import Context\n\n        ctx = Context(\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            epochs=10,\n            learning_rate=0.001,\n            experiment_name=\"my-experiment\",\n        )\n\n        pipeline = create_keras_training_pipeline(\n            name=\"my_training\",\n            context=ctx,\n            project_name=\"my_project\",\n        )\n        result = pipeline.run()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters to inject.\n        enable_cache: Whether to enable step caching.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string for versioned pipeline.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        project_name=project_name,\n        version=version,\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines.create_keras_full_pipeline","title":"create_keras_full_pipeline","text":"<pre><code>create_keras_full_pipeline(\n    name: str = \"keras_full\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    enable_checkpointing: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a full Keras pipeline covering the entire ML lifecycle.</p> <p>DAG: <code>load_data \u2192 transform_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></p> <p>Includes checkpointing for long-running training steps so the pipeline can resume from the last checkpoint on failure.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    # Data loading\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    # Transformation\n    data_output_path=\"data/transformed/\",\n    # Training\n    epochs=50,\n    learning_rate=0.001,\n    experiment_name=\"full-run\",\n    project=\"my_project\",\n    # Export\n    export_path=\"models/production/\",\n    export_format=\"keras\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'keras_full'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>enable_checkpointing</code> <code>bool</code> <p>Whether to enable checkpointing.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/keras/pipelines.py</code> <pre><code>def create_keras_full_pipeline(\n    name: str = \"keras_full\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    enable_checkpointing: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a full Keras pipeline covering the entire ML lifecycle.\n\n    **DAG**: ``load_data \u2192 transform_data \u2192 train_model \u2192 evaluate_model \u2192 export_model``\n\n    Includes checkpointing for long-running training steps so the pipeline\n    can resume from the last checkpoint on failure.\n\n    Context parameters::\n\n        ctx = Context(\n            # Data loading\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            # Transformation\n            data_output_path=\"data/transformed/\",\n            # Training\n            epochs=50,\n            learning_rate=0.001,\n            experiment_name=\"full-run\",\n            project=\"my_project\",\n            # Export\n            export_path=\"models/production/\",\n            export_format=\"keras\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        enable_cache: Whether to enable step caching.\n        enable_checkpointing: Whether to enable checkpointing.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        enable_checkpointing=enable_checkpointing,\n        project_name=project_name,\n        version=version,\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(transform_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines.create_keras_evaluation_pipeline","title":"create_keras_evaluation_pipeline","text":"<pre><code>create_keras_evaluation_pipeline(\n    name: str = \"keras_evaluation\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a pipeline for evaluating an existing Keras model.</p> <p>DAG: <code>load_model \u2192 load_data \u2192 evaluate_model \u2192 inspect_model</code></p> <p>Useful for model validation, A/B testing, and periodic evaluation against new data without retraining.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    model_path=\"models/production/model.keras\",\n    file_path=\"data/test.csv\",\n    label_name=\"target\",\n    batch_size=64,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'keras_evaluation'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/keras/pipelines.py</code> <pre><code>def create_keras_evaluation_pipeline(\n    name: str = \"keras_evaluation\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a pipeline for evaluating an existing Keras model.\n\n    **DAG**: ``load_model \u2192 load_data \u2192 evaluate_model \u2192 inspect_model``\n\n    Useful for model validation, A/B testing, and periodic evaluation\n    against new data without retraining.\n\n    Context parameters::\n\n        ctx = Context(\n            model_path=\"models/production/model.keras\",\n            file_path=\"data/test.csv\",\n            label_name=\"target\",\n            batch_size=64,\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        enable_cache: Whether to enable step caching.\n        project_name: Project to attach this pipeline to.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_model)\n    pipeline.add_step(load_data)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(inspect_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines.create_keras_export_pipeline","title":"create_keras_export_pipeline","text":"<pre><code>create_keras_export_pipeline(\n    name: str = \"keras_export\",\n    context: Context | None = None,\n    project_name: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a pipeline for exporting and saving an existing model.</p> <p>DAG: <code>load_model \u2192 export_model, save_model</code></p> <p>Useful for converting a trained model to multiple formats (SavedModel, TFLite, Keras) and persisting to different locations.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    model_path=\"models/trained/model.keras\",\n    export_path=\"models/exported/\",\n    export_format=\"saved_model\",\n    save_path=\"models/backup/model.keras\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'keras_export'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/keras/pipelines.py</code> <pre><code>def create_keras_export_pipeline(\n    name: str = \"keras_export\",\n    context: Context | None = None,\n    project_name: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a pipeline for exporting and saving an existing model.\n\n    **DAG**: ``load_model \u2192 export_model, save_model``\n\n    Useful for converting a trained model to multiple formats\n    (SavedModel, TFLite, Keras) and persisting to different locations.\n\n    Context parameters::\n\n        ctx = Context(\n            model_path=\"models/trained/model.keras\",\n            export_path=\"models/exported/\",\n            export_format=\"saved_model\",\n            save_path=\"models/backup/model.keras\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,  # Always re-export\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_model)\n    pipeline.add_step(export_model)\n    pipeline.add_step(save_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines.create_keras_experiment_pipeline","title":"create_keras_experiment_pipeline","text":"<pre><code>create_keras_experiment_pipeline(\n    name: str = \"keras_experiment\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    version: str | None = None,\n    deploy_threshold: float = 0.8,\n    threshold_metric: str = \"accuracy\",\n) -&gt; Pipeline\n</code></pre> <p>Create a full experiment pipeline with tracking and conditional deployment.</p> <p>DAG::</p> <pre><code>load_data \u2192 train_model \u2192 evaluate_model\n                               \u2193\n                      [if metric &gt; threshold]\n                               \u2193\n                      export_model \u2192 save_model\n</code></pre> <p>Integrates FlowyML experiment tracking and conditionally exports the model only if validation metrics exceed the given threshold.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=30,\n    learning_rate=0.001,\n    experiment_name=\"experiment-v1\",\n    project=\"my_project\",\n    export_path=\"models/production/\",\n    save_path=\"models/checkpoints/model.keras\",\n)\n\npipeline = create_keras_experiment_pipeline(\n    context=ctx,\n    project_name=\"my_project\",\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\nresult = pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'keras_experiment'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string.</p> <code>None</code> <code>deploy_threshold</code> <code>float</code> <p>Minimum metric value to trigger deployment.</p> <code>0.8</code> <code>threshold_metric</code> <code>str</code> <p>Which metric to check against the threshold.</p> <code>'accuracy'</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/keras/pipelines.py</code> <pre><code>def create_keras_experiment_pipeline(\n    name: str = \"keras_experiment\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    version: str | None = None,\n    deploy_threshold: float = 0.8,\n    threshold_metric: str = \"accuracy\",\n) -&gt; Pipeline:\n    \"\"\"Create a full experiment pipeline with tracking and conditional deployment.\n\n    **DAG**::\n\n        load_data \u2192 train_model \u2192 evaluate_model\n                                       \u2193\n                              [if metric &gt; threshold]\n                                       \u2193\n                              export_model \u2192 save_model\n\n    Integrates FlowyML experiment tracking and conditionally exports the\n    model only if validation metrics exceed the given threshold.\n\n    Context parameters::\n\n        ctx = Context(\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            epochs=30,\n            learning_rate=0.001,\n            experiment_name=\"experiment-v1\",\n            project=\"my_project\",\n            export_path=\"models/production/\",\n            save_path=\"models/checkpoints/model.keras\",\n        )\n\n        pipeline = create_keras_experiment_pipeline(\n            context=ctx,\n            project_name=\"my_project\",\n            deploy_threshold=0.85,\n            threshold_metric=\"accuracy\",\n        )\n        result = pipeline.run()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string.\n        deploy_threshold: Minimum metric value to trigger deployment.\n        threshold_metric: Which metric to check against the threshold.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    from flowyml.core.conditional import If\n\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,\n        enable_experiment_tracking=True,\n        enable_checkpointing=True,\n        project_name=project_name,\n        version=version,\n    )\n\n    # Core training DAG\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    # Conditional deployment: only export if metric exceeds threshold\n    deploy_condition = If(\n        condition=lambda metrics: (\n            metrics.get_metric(threshold_metric, 0) &gt;= deploy_threshold\n            if hasattr(metrics, \"get_metric\")\n            else metrics.get(threshold_metric, 0) &gt;= deploy_threshold\n        ),\n        then_steps=[export_model, save_model],\n        name=f\"deploy_if_{threshold_metric}_above_{deploy_threshold}\",\n    )\n    pipeline.control_flows.append(deploy_condition)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.keras.pipelines.create_keras_scheduled_pipeline","title":"create_keras_scheduled_pipeline","text":"<pre><code>create_keras_scheduled_pipeline(\n    name: str = \"keras_scheduled_retraining\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    schedule: str = \"0 2 * * 0\",\n    timezone: str = \"UTC\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a scheduled retraining pipeline.</p> <p>Returns both the pipeline and a configured scheduler so you can register periodic retraining (e.g., weekly) with a single call.</p> <p>DAG: <code>load_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></p> <p>Schedule format uses cron syntax (default: every Sunday at 2 AM)::</p> <pre><code>pipeline_info = create_keras_scheduled_pipeline(\n    context=ctx,\n    project_name=\"my_project\",\n    schedule=\"0 2 * * 0\",  # Weekly\n)\n\n# Access the components\npipeline = pipeline_info[\"pipeline\"]\nscheduler = pipeline_info[\"scheduler\"]\n\n# Run once immediately\nresult = pipeline.run()\n\n# Or start the scheduler for automatic retraining\nscheduler.start()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'keras_scheduled_retraining'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>schedule</code> <code>str</code> <p>Cron expression for scheduling.</p> <code>'0 2 * * 0'</code> <code>timezone</code> <code>str</code> <p>Timezone for the schedule.</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with <code>pipeline</code> and <code>scheduler</code> keys.</p> Source code in <code>mlpotion/integrations/flowyml/keras/pipelines.py</code> <pre><code>def create_keras_scheduled_pipeline(\n    name: str = \"keras_scheduled_retraining\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    schedule: str = \"0 2 * * 0\",\n    timezone: str = \"UTC\",\n) -&gt; dict[str, Any]:\n    \"\"\"Create a scheduled retraining pipeline.\n\n    Returns both the pipeline and a configured scheduler so you can\n    register periodic retraining (e.g., weekly) with a single call.\n\n    **DAG**: ``load_data \u2192 train_model \u2192 evaluate_model \u2192 export_model``\n\n    Schedule format uses cron syntax (default: every Sunday at 2 AM)::\n\n        pipeline_info = create_keras_scheduled_pipeline(\n            context=ctx,\n            project_name=\"my_project\",\n            schedule=\"0 2 * * 0\",  # Weekly\n        )\n\n        # Access the components\n        pipeline = pipeline_info[\"pipeline\"]\n        scheduler = pipeline_info[\"scheduler\"]\n\n        # Run once immediately\n        result = pipeline.run()\n\n        # Or start the scheduler for automatic retraining\n        scheduler.start()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n        schedule: Cron expression for scheduling.\n        timezone: Timezone for the schedule.\n\n    Returns:\n        Dict with ``pipeline`` and ``scheduler`` keys.\n    \"\"\"\n    from flowyml.core.scheduler import PipelineScheduler\n\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,  # Fresh data each run\n        enable_checkpointing=True,\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n\n    # Configure scheduler\n    scheduler = PipelineScheduler()\n    scheduler.schedule(\n        pipeline=pipeline,\n        cron=schedule,\n        timezone=timezone,\n    )\n\n    return {\n        \"pipeline\": pipeline,\n        \"scheduler\": scheduler,\n    }\n</code></pre>"},{"location":"api/integrations/flowyml.html#pytorch-integration","title":"PyTorch Integration","text":""},{"location":"api/integrations/flowyml.html#steps_1","title":"Steps","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps","title":"mlpotion.integrations.flowyml.pytorch.steps","text":"<p>FlowyML PyTorch steps \u2014 Full-featured pipeline steps for PyTorch workflows.</p> <p>Each step leverages FlowyML's native capabilities: - Artifact-centric design: returns Dataset, Model, Metrics with auto-extraction - Supports caching, retry, GPU resources, tags, DAG wiring, and execution groups - Returns framework-native objects wrapped as FlowyML assets</p>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps-classes","title":"Classes","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.load_csv_data","title":"load_csv_data","text":"<pre><code>load_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    dtype: str = \"float32\",\n) -&gt; Dataset\n</code></pre> <p>Load CSV data into a PyTorch DataLoader, wrapped as a Dataset asset.</p> <p>Automatic metadata extraction captures batch size, source path, column names, and worker configuration.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Target column name.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Pin memory for faster GPU transfer.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last incomplete batch.</p> <code>False</code> <code>dtype</code> <code>str</code> <p>Data type for tensors.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset wrapping the PyTorch DataLoader.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_load_csv_data\",\n    outputs=[\"dataset\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"pytorch\", \"component\": \"data_loader\"},\n)\ndef load_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    dtype: str = \"float32\",\n) -&gt; Dataset:\n    \"\"\"Load CSV data into a PyTorch DataLoader, wrapped as a Dataset asset.\n\n    Automatic metadata extraction captures batch size, source path,\n    column names, and worker configuration.\n\n    Args:\n        file_path: Glob pattern for CSV files.\n        batch_size: Batch size.\n        label_name: Target column name.\n        column_names: Specific columns to load.\n        shuffle: Whether to shuffle.\n        num_workers: Number of data loading workers.\n        pin_memory: Pin memory for faster GPU transfer.\n        drop_last: Drop the last incomplete batch.\n        dtype: Data type for tensors.\n\n    Returns:\n        Dataset asset wrapping the PyTorch DataLoader.\n    \"\"\"\n    # Convert dtype string to torch.dtype\n    torch_dtype = getattr(torch, dtype)\n\n    # Create dataset\n    csv_dataset = CSVDataset(\n        file_pattern=file_path,\n        column_names=column_names,\n        label_name=label_name,\n        dtype=torch_dtype,\n    )\n\n    # Create DataLoader config\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        drop_last=drop_last,\n    )\n\n    # Create DataLoader using factory\n    loader_factory = CSVDataLoader(**config.dict(exclude={\"file_pattern\", \"config\"}))\n    dataloader = loader_factory.load(csv_dataset)\n\n    dataset_asset = Dataset.create(\n        data=dataloader,\n        name=\"pytorch_csv_dataset\",\n        properties={\n            \"source\": file_path,\n            \"batch_size\": batch_size,\n            \"label_name\": label_name,\n            \"shuffle\": shuffle,\n            \"num_workers\": num_workers,\n            \"pin_memory\": pin_memory,\n            \"dtype\": dtype,\n        },\n        source=file_path,\n        loader=\"CSVDataLoader\",\n        framework=\"pytorch\",\n    )\n\n    logger.info(\n        f\"\ud83d\udce6 Loaded PyTorch DataLoader: batch_size={batch_size}, source={file_path}\"\n    )\n    return dataset_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.load_streaming_csv_data","title":"load_streaming_csv_data","text":"<pre><code>load_streaming_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    chunksize: int = 10000,\n    dtype: str = \"float32\",\n) -&gt; Dataset\n</code></pre> <p>Load large CSV data via streaming into a PyTorch DataLoader, wrapped as a Dataset asset.</p> <p>Uses chunked reading for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Target column name.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Pin memory for faster GPU transfer.</p> <code>False</code> <code>chunksize</code> <code>int</code> <p>Number of rows per chunk.</p> <code>10000</code> <code>dtype</code> <code>str</code> <p>Data type for tensors.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset wrapping the streaming PyTorch DataLoader.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_load_streaming_csv_data\",\n    outputs=[\"dataset\"],\n    cache=False,\n    tags={\"framework\": \"pytorch\", \"component\": \"data_loader\", \"mode\": \"streaming\"},\n)\ndef load_streaming_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    chunksize: int = 10000,\n    dtype: str = \"float32\",\n) -&gt; Dataset:\n    \"\"\"Load large CSV data via streaming into a PyTorch DataLoader, wrapped as a Dataset asset.\n\n    Uses chunked reading for datasets that don't fit in memory.\n\n    Args:\n        file_path: Glob pattern for CSV files.\n        batch_size: Batch size.\n        label_name: Target column name.\n        column_names: Specific columns.\n        num_workers: Number of data loading workers.\n        pin_memory: Pin memory for faster GPU transfer.\n        chunksize: Number of rows per chunk.\n        dtype: Data type for tensors.\n\n    Returns:\n        Dataset asset wrapping the streaming PyTorch DataLoader.\n    \"\"\"\n    # Convert dtype string to torch.dtype\n    torch_dtype = getattr(torch, dtype)\n\n    # Create streaming dataset\n    streaming_dataset = StreamingCSVDataset(\n        file_pattern=file_path,\n        column_names=column_names,\n        label_name=label_name,\n        chunksize=chunksize,\n        dtype=torch_dtype,\n    )\n\n    # Create DataLoader config (no shuffle for streaming)\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        drop_last=False,\n    )\n\n    loader_factory = CSVDataLoader(**config.dict(exclude={\"file_pattern\", \"config\"}))\n    dataloader = loader_factory.load(streaming_dataset)\n\n    dataset_asset = Dataset.create(\n        data=dataloader,\n        name=\"pytorch_streaming_dataset\",\n        properties={\n            \"source\": file_path,\n            \"batch_size\": batch_size,\n            \"chunksize\": chunksize,\n            \"label_name\": label_name,\n            \"num_workers\": num_workers,\n            \"dtype\": dtype,\n            \"mode\": \"streaming\",\n        },\n        source=file_path,\n        loader=\"StreamingCSVDataLoader\",\n        framework=\"pytorch\",\n    )\n\n    logger.info(\n        f\"\ud83d\udce6 Streaming DataLoader: batch_size={batch_size}, \"\n        f\"chunksize={chunksize}, source={file_path}\"\n    )\n    return dataset_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: nn.Module,\n    data: DataLoader | Dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    validation_data: DataLoader | Dataset | None = None,\n    verbose: bool = True,\n    max_batches_per_epoch: int | None = None,\n) -&gt; tuple[Model, Metrics]\n</code></pre> <p>Train a PyTorch model with full configuration.</p> <p>Returns a Model asset (via Model.from_pytorch with auto-extracted metadata) and a Metrics asset with training history.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>PyTorch model (nn.Module).</p> required <code>data</code> <code>DataLoader | Dataset</code> <p>Training DataLoader or Dataset asset.</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>0.001</code> <code>optimizer</code> <code>str</code> <p>Optimizer type ('adam', 'sgd', 'adamw').</p> <code>'adam'</code> <code>loss_fn</code> <code>str</code> <p>Loss function name ('mse', 'cross_entropy').</p> <code>'mse'</code> <code>device</code> <code>str</code> <p>Device to train on ('cuda', 'cpu').</p> <code>'cpu'</code> <code>validation_data</code> <code>DataLoader | Dataset | None</code> <p>Optional validation DataLoader or Dataset.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to log per-epoch metrics.</p> <code>True</code> <code>max_batches_per_epoch</code> <code>int | None</code> <p>Limit batches per epoch (for debugging).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Model, Metrics]</code> <p>Tuple of (Model asset, Metrics asset).</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_train_model\",\n    inputs=[\"dataset\"],\n    outputs=[\"model\", \"training_metrics\"],\n    cache=False,\n    retry=1,\n    tags={\"framework\": \"pytorch\", \"component\": \"model_trainer\"},\n)\ndef train_model(\n    model: nn.Module,\n    data: DataLoader | Dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    validation_data: DataLoader | Dataset | None = None,\n    verbose: bool = True,\n    max_batches_per_epoch: int | None = None,\n) -&gt; tuple[Model, Metrics]:\n    \"\"\"Train a PyTorch model with full configuration.\n\n    Returns a Model asset (via Model.from_pytorch with auto-extracted metadata)\n    and a Metrics asset with training history.\n\n    Args:\n        model: PyTorch model (nn.Module).\n        data: Training DataLoader or Dataset asset.\n        epochs: Number of training epochs.\n        learning_rate: Learning rate.\n        optimizer: Optimizer type ('adam', 'sgd', 'adamw').\n        loss_fn: Loss function name ('mse', 'cross_entropy').\n        device: Device to train on ('cuda', 'cpu').\n        validation_data: Optional validation DataLoader or Dataset.\n        verbose: Whether to log per-epoch metrics.\n        max_batches_per_epoch: Limit batches per epoch (for debugging).\n\n    Returns:\n        Tuple of (Model asset, Metrics asset).\n    \"\"\"\n    # Extract raw data from Dataset if wrapped\n    raw_data = data.data if isinstance(data, Dataset) else data\n    raw_val = (\n        validation_data.data\n        if isinstance(validation_data, Dataset)\n        else validation_data\n    )\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        device=device,\n        verbose=verbose,\n        max_batches_per_epoch=max_batches_per_epoch,\n    )\n\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model=model,\n        dataloader=raw_data,\n        config=config,\n        validation_dataloader=raw_val,\n    )\n\n    # Collect raw metrics\n    raw_metrics: dict[str, Any] = result.metrics if hasattr(result, \"metrics\") else {}\n    raw_metrics[\"epochs_completed\"] = epochs\n    raw_metrics[\"learning_rate\"] = learning_rate\n    raw_metrics[\"optimizer\"] = optimizer\n    raw_metrics[\"device\"] = device\n\n    # Wrap as Model asset using from_pytorch for full auto-extraction\n    model_asset = Model.from_pytorch(\n        result.model,\n        name=\"pytorch_trained_model\",\n        training_history=raw_metrics,\n        epochs_requested=epochs,\n        optimizer_type=optimizer,\n        loss_function=loss_fn,\n    )\n\n    # Wrap as Metrics asset\n    metrics_asset = Metrics.create(\n        metrics=raw_metrics,\n        name=\"pytorch_training_metrics\",\n        tags={\"stage\": \"training\", \"framework\": \"pytorch\"},\n        properties={\n            \"epochs\": epochs,\n            \"learning_rate\": learning_rate,\n            \"optimizer\": optimizer,\n            \"device\": device,\n        },\n    )\n\n    logger.info(f\"\ud83c\udfaf PyTorch training complete: {epochs} epochs, device={device}\")\n    return model_asset, metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: nn.Module | Model,\n    data: DataLoader | Dataset,\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    verbose: bool = True,\n    max_batches: int | None = None,\n) -&gt; Metrics\n</code></pre> <p>Evaluate a PyTorch model and return a Metrics asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module | Model</code> <p>Trained PyTorch model or Model asset.</p> required <code>data</code> <code>DataLoader | Dataset</code> <p>Evaluation DataLoader or Dataset asset.</p> required <code>loss_fn</code> <code>str</code> <p>Loss function name.</p> <code>'mse'</code> <code>device</code> <code>str</code> <p>Device for evaluation.</p> <code>'cpu'</code> <code>verbose</code> <code>bool</code> <p>Whether to log metrics.</p> <code>True</code> <code>max_batches</code> <code>int | None</code> <p>Limit batches to evaluate.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics asset with evaluation results.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_evaluate_model\",\n    inputs=[\"model\", \"dataset\"],\n    outputs=[\"metrics\"],\n    cache=\"input_hash\",\n    tags={\"framework\": \"pytorch\", \"component\": \"model_evaluator\"},\n)\ndef evaluate_model(\n    model: nn.Module | Model,\n    data: DataLoader | Dataset,\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    verbose: bool = True,\n    max_batches: int | None = None,\n) -&gt; Metrics:\n    \"\"\"Evaluate a PyTorch model and return a Metrics asset.\n\n    Args:\n        model: Trained PyTorch model or Model asset.\n        data: Evaluation DataLoader or Dataset asset.\n        loss_fn: Loss function name.\n        device: Device for evaluation.\n        verbose: Whether to log metrics.\n        max_batches: Limit batches to evaluate.\n\n    Returns:\n        Metrics asset with evaluation results.\n    \"\"\"\n    # Extract raw objects from assets\n    raw_model = model.data if isinstance(model, Model) else model\n    raw_data = data.data if isinstance(data, Dataset) else data\n\n    config = ModelEvaluationConfig(\n        batch_size=raw_data.batch_size or 32,\n        verbose=verbose,\n        device=device,\n        framework_options={\"loss_fn\": loss_fn, \"max_batches\": max_batches},\n    )\n\n    evaluator = ModelEvaluator()\n    result = evaluator.evaluate(\n        model=raw_model,\n        dataloader=raw_data,\n        config=config,\n    )\n\n    raw_metrics = result.metrics if hasattr(result, \"metrics\") else {}\n\n    metrics_asset = Metrics.create(\n        metrics=raw_metrics,\n        name=\"pytorch_evaluation_metrics\",\n        tags={\"stage\": \"evaluation\", \"framework\": \"pytorch\"},\n        properties=raw_metrics,\n    )\n\n    logger.info(f\"\ud83d\udcca PyTorch evaluation: {raw_metrics}\")\n    return metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: nn.Module | Model,\n    export_path: str,\n    export_format: str = \"torchscript\",\n    sample_input: torch.Tensor | None = None,\n) -&gt; Model\n</code></pre> <p>Export a PyTorch model to the specified format, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module | Model</code> <p>PyTorch model or Model asset to export.</p> required <code>export_path</code> <code>str</code> <p>Destination path.</p> required <code>export_format</code> <code>str</code> <p>Format ('torchscript', 'onnx').</p> <code>'torchscript'</code> <code>sample_input</code> <code>torch.Tensor | None</code> <p>Sample input tensor (required for ONNX).</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model asset with export metadata.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_export_model\",\n    inputs=[\"model\"],\n    outputs=[\"exported_model\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"pytorch\", \"component\": \"model_exporter\"},\n)\ndef export_model(\n    model: nn.Module | Model,\n    export_path: str,\n    export_format: str = \"torchscript\",\n    sample_input: torch.Tensor | None = None,\n) -&gt; Model:\n    \"\"\"Export a PyTorch model to the specified format, returned as a Model asset.\n\n    Args:\n        model: PyTorch model or Model asset to export.\n        export_path: Destination path.\n        export_format: Format ('torchscript', 'onnx').\n        sample_input: Sample input tensor (required for ONNX).\n\n    Returns:\n        Model asset with export metadata.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    config = ModelExportConfig(\n        export_path=export_path,\n        format=export_format,\n    )\n    exporter = ModelExporter()\n    exporter.export(model=raw_model, config=config, sample_input=sample_input)\n\n    model_asset = Model.from_pytorch(\n        raw_model,\n        name=\"pytorch_exported_model\",\n        export_path=export_path,\n        export_format=export_format,\n    )\n\n    logger.info(f\"\ud83d\udce4 Exported PyTorch model to: {export_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: nn.Module | Model, save_path: str\n) -&gt; Model\n</code></pre> <p>Save a PyTorch model to disk, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module | Model</code> <p>PyTorch model or Model asset to save.</p> required <code>save_path</code> <code>str</code> <p>Destination file path.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>Model asset with save location metadata.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_save_model\",\n    inputs=[\"model\"],\n    outputs=[\"saved_model\"],\n    tags={\"framework\": \"pytorch\", \"component\": \"model_persistence\"},\n)\ndef save_model(\n    model: nn.Module | Model,\n    save_path: str,\n) -&gt; Model:\n    \"\"\"Save a PyTorch model to disk, returned as a Model asset.\n\n    Args:\n        model: PyTorch model or Model asset to save.\n        save_path: Destination file path.\n\n    Returns:\n        Model asset with save location metadata.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    persistence = ModelPersistence()\n    persistence.save(model=raw_model, path=save_path)\n\n    model_asset = Model.from_pytorch(\n        raw_model,\n        name=\"pytorch_saved_model\",\n        save_path=save_path,\n    )\n\n    logger.info(f\"\ud83d\udcbe Saved PyTorch model to: {save_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    model_class: type | None = None,\n    device: str | None = None,\n) -&gt; Model\n</code></pre> <p>Load a PyTorch model from disk, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the saved model.</p> required <code>model_class</code> <code>type | None</code> <p>Model class for state_dict loading.</p> <code>None</code> <code>device</code> <code>str | None</code> <p>Device to load model onto.</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model asset wrapping the loaded PyTorch model.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/steps.py</code> <pre><code>@step(\n    name=\"pytorch_load_model\",\n    outputs=[\"model\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"pytorch\", \"component\": \"model_persistence\"},\n)\ndef load_model(\n    model_path: str,\n    model_class: type | None = None,\n    device: str | None = None,\n) -&gt; Model:\n    \"\"\"Load a PyTorch model from disk, returned as a Model asset.\n\n    Args:\n        model_path: Path to the saved model.\n        model_class: Model class for state_dict loading.\n        device: Device to load model onto.\n\n    Returns:\n        Model asset wrapping the loaded PyTorch model.\n    \"\"\"\n    persistence = ModelPersistence()\n    raw_model = persistence.load(\n        path=model_path, model_class=model_class, device=device\n    )\n\n    model_asset = Model.from_pytorch(\n        raw_model,\n        name=\"pytorch_loaded_model\",\n        source_path=model_path,\n    )\n\n    logger.info(f\"\ud83d\udd0d Loaded PyTorch model from: {model_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#pipelines_1","title":"Pipelines","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines","title":"mlpotion.integrations.flowyml.pytorch.pipelines","text":"<p>Pre-built FlowyML pipeline templates for PyTorch workflows.</p> <p>Provides ready-to-run pipelines that wire MLPotion PyTorch steps together with proper DAG dependencies, context injection, experiment tracking, and optional scheduling.</p> <p>Available pipelines:</p> <ul> <li>create_pytorch_training_pipeline  \u2014 Load \u2192 Train \u2192 Evaluate</li> <li>create_pytorch_full_pipeline      \u2014 Load \u2192 Train \u2192 Evaluate \u2192 Export \u2192 Save</li> <li>create_pytorch_evaluation_pipeline \u2014 Load model + data \u2192 Evaluate</li> <li>create_pytorch_export_pipeline    \u2014 Load model \u2192 Export + Save</li> <li>create_pytorch_experiment_pipeline \u2014 Full pipeline with conditional deploy</li> <li>create_pytorch_scheduled_pipeline \u2014 Scheduled retraining with cron</li> </ul>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines.create_pytorch_training_pipeline","title":"create_pytorch_training_pipeline","text":"<pre><code>create_pytorch_training_pipeline(\n    name: str = \"pytorch_training\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a ready-to-run PyTorch training pipeline.</p> <p>DAG: <code>load_csv_data \u2192 train_model \u2192 evaluate_model</code></p> <p>Provide hyperparameters via the context object::</p> <pre><code>from flowyml.core.context import Context\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=20,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=\"cross_entropy\",\n    device=\"cuda\",\n)\n\npipeline = create_pytorch_training_pipeline(\n    name=\"my_training\",\n    context=ctx,\n    project_name=\"my_project\",\n)\nresult = pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'pytorch_training'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters to inject.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string for versioned pipeline.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/pipelines.py</code> <pre><code>def create_pytorch_training_pipeline(\n    name: str = \"pytorch_training\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a ready-to-run PyTorch training pipeline.\n\n    **DAG**: ``load_csv_data \u2192 train_model \u2192 evaluate_model``\n\n    Provide hyperparameters via the context object::\n\n        from flowyml.core.context import Context\n\n        ctx = Context(\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            epochs=20,\n            learning_rate=0.001,\n            optimizer=\"adam\",\n            loss_fn=\"cross_entropy\",\n            device=\"cuda\",\n        )\n\n        pipeline = create_pytorch_training_pipeline(\n            name=\"my_training\",\n            context=ctx,\n            project_name=\"my_project\",\n        )\n        result = pipeline.run()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters to inject.\n        enable_cache: Whether to enable step caching.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string for versioned pipeline.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        project_name=project_name,\n        version=version,\n    )\n\n    pipeline.add_step(load_csv_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines.create_pytorch_full_pipeline","title":"create_pytorch_full_pipeline","text":"<pre><code>create_pytorch_full_pipeline(\n    name: str = \"pytorch_full\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    enable_checkpointing: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a full PyTorch pipeline covering the entire ML lifecycle.</p> <p>DAG: <code>load_csv_data \u2192 train_model \u2192 evaluate_model \u2192 export_model \u2192 save_model</code></p> <p>Includes checkpointing for long-running training steps so the pipeline can resume from the last checkpoint on failure.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    # Data loading\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=64,\n    # Training\n    epochs=100,\n    learning_rate=0.001,\n    optimizer=\"adamw\",\n    loss_fn=\"cross_entropy\",\n    device=\"cuda\",\n    # Export\n    export_path=\"models/production/model.pt\",\n    export_format=\"torchscript\",\n    save_path=\"models/backup/model.pt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'pytorch_full'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>enable_checkpointing</code> <code>bool</code> <p>Whether to enable checkpointing.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/pipelines.py</code> <pre><code>def create_pytorch_full_pipeline(\n    name: str = \"pytorch_full\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    enable_checkpointing: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a full PyTorch pipeline covering the entire ML lifecycle.\n\n    **DAG**: ``load_csv_data \u2192 train_model \u2192 evaluate_model \u2192 export_model \u2192 save_model``\n\n    Includes checkpointing for long-running training steps so the pipeline\n    can resume from the last checkpoint on failure.\n\n    Context parameters::\n\n        ctx = Context(\n            # Data loading\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=64,\n            # Training\n            epochs=100,\n            learning_rate=0.001,\n            optimizer=\"adamw\",\n            loss_fn=\"cross_entropy\",\n            device=\"cuda\",\n            # Export\n            export_path=\"models/production/model.pt\",\n            export_format=\"torchscript\",\n            save_path=\"models/backup/model.pt\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        enable_cache: Whether to enable step caching.\n        enable_checkpointing: Whether to enable checkpointing.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        enable_checkpointing=enable_checkpointing,\n        project_name=project_name,\n        version=version,\n    )\n\n    pipeline.add_step(load_csv_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n    pipeline.add_step(save_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines.create_pytorch_evaluation_pipeline","title":"create_pytorch_evaluation_pipeline","text":"<pre><code>create_pytorch_evaluation_pipeline(\n    name: str = \"pytorch_evaluation\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a pipeline for evaluating an existing PyTorch model.</p> <p>DAG: <code>load_model \u2192 load_csv_data \u2192 evaluate_model</code></p> <p>Useful for model validation, A/B testing, and periodic evaluation against new data without retraining.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    model_path=\"models/production/model.pt\",\n    file_path=\"data/test.csv\",\n    label_name=\"target\",\n    batch_size=64,\n    device=\"cuda\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'pytorch_evaluation'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/pipelines.py</code> <pre><code>def create_pytorch_evaluation_pipeline(\n    name: str = \"pytorch_evaluation\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a pipeline for evaluating an existing PyTorch model.\n\n    **DAG**: ``load_model \u2192 load_csv_data \u2192 evaluate_model``\n\n    Useful for model validation, A/B testing, and periodic evaluation\n    against new data without retraining.\n\n    Context parameters::\n\n        ctx = Context(\n            model_path=\"models/production/model.pt\",\n            file_path=\"data/test.csv\",\n            label_name=\"target\",\n            batch_size=64,\n            device=\"cuda\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        enable_cache: Whether to enable step caching.\n        project_name: Project to attach this pipeline to.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_model)\n    pipeline.add_step(load_csv_data)\n    pipeline.add_step(evaluate_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines.create_pytorch_export_pipeline","title":"create_pytorch_export_pipeline","text":"<pre><code>create_pytorch_export_pipeline(\n    name: str = \"pytorch_export\",\n    context: Context | None = None,\n    project_name: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a pipeline for exporting and saving an existing model.</p> <p>DAG: <code>load_model \u2192 export_model, save_model</code></p> <p>Useful for converting a trained model to TorchScript or ONNX and persisting to different locations.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    model_path=\"models/trained/model.pt\",\n    export_path=\"models/exported/model.ts\",\n    export_format=\"torchscript\",\n    save_path=\"models/backup/model.pt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'pytorch_export'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/pipelines.py</code> <pre><code>def create_pytorch_export_pipeline(\n    name: str = \"pytorch_export\",\n    context: Context | None = None,\n    project_name: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a pipeline for exporting and saving an existing model.\n\n    **DAG**: ``load_model \u2192 export_model, save_model``\n\n    Useful for converting a trained model to TorchScript or ONNX\n    and persisting to different locations.\n\n    Context parameters::\n\n        ctx = Context(\n            model_path=\"models/trained/model.pt\",\n            export_path=\"models/exported/model.ts\",\n            export_format=\"torchscript\",\n            save_path=\"models/backup/model.pt\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,  # Always re-export\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_model)\n    pipeline.add_step(export_model)\n    pipeline.add_step(save_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines.create_pytorch_experiment_pipeline","title":"create_pytorch_experiment_pipeline","text":"<pre><code>create_pytorch_experiment_pipeline(\n    name: str = \"pytorch_experiment\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    version: str | None = None,\n    deploy_threshold: float = 0.8,\n    threshold_metric: str = \"accuracy\",\n) -&gt; Pipeline\n</code></pre> <p>Create a full experiment pipeline with conditional deployment.</p> <p>DAG::</p> <pre><code>load_csv_data \u2192 train_model \u2192 evaluate_model\n                                   \u2193\n                          [if metric &gt; threshold]\n                                   \u2193\n                          export_model \u2192 save_model\n</code></pre> <p>Conditionally exports the model only if validation metrics exceed the given threshold.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=50,\n    learning_rate=0.001,\n    optimizer=\"adamw\",\n    device=\"cuda\",\n    export_path=\"models/production/model.pt\",\n    save_path=\"models/checkpoints/model.pt\",\n)\n\npipeline = create_pytorch_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\nresult = pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'pytorch_experiment'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string.</p> <code>None</code> <code>deploy_threshold</code> <code>float</code> <p>Minimum metric value to trigger deployment.</p> <code>0.8</code> <code>threshold_metric</code> <code>str</code> <p>Which metric to check against the threshold.</p> <code>'accuracy'</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/pipelines.py</code> <pre><code>def create_pytorch_experiment_pipeline(\n    name: str = \"pytorch_experiment\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    version: str | None = None,\n    deploy_threshold: float = 0.8,\n    threshold_metric: str = \"accuracy\",\n) -&gt; Pipeline:\n    \"\"\"Create a full experiment pipeline with conditional deployment.\n\n    **DAG**::\n\n        load_csv_data \u2192 train_model \u2192 evaluate_model\n                                           \u2193\n                                  [if metric &gt; threshold]\n                                           \u2193\n                                  export_model \u2192 save_model\n\n    Conditionally exports the model only if validation metrics\n    exceed the given threshold.\n\n    Context parameters::\n\n        ctx = Context(\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            epochs=50,\n            learning_rate=0.001,\n            optimizer=\"adamw\",\n            device=\"cuda\",\n            export_path=\"models/production/model.pt\",\n            save_path=\"models/checkpoints/model.pt\",\n        )\n\n        pipeline = create_pytorch_experiment_pipeline(\n            context=ctx,\n            deploy_threshold=0.85,\n            threshold_metric=\"accuracy\",\n        )\n        result = pipeline.run()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string.\n        deploy_threshold: Minimum metric value to trigger deployment.\n        threshold_metric: Which metric to check against the threshold.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    from flowyml.core.conditional import If\n\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,\n        enable_experiment_tracking=True,\n        enable_checkpointing=True,\n        project_name=project_name,\n        version=version,\n    )\n\n    # Core training DAG\n    pipeline.add_step(load_csv_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    # Conditional deployment: only export if metric exceeds threshold\n    deploy_condition = If(\n        condition=lambda metrics: (\n            metrics.get_metric(threshold_metric, 0) &gt;= deploy_threshold\n            if hasattr(metrics, \"get_metric\")\n            else metrics.get(threshold_metric, 0) &gt;= deploy_threshold\n        ),\n        then_steps=[export_model, save_model],\n        name=f\"deploy_if_{threshold_metric}_above_{deploy_threshold}\",\n    )\n    pipeline.control_flows.append(deploy_condition)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.pytorch.pipelines.create_pytorch_scheduled_pipeline","title":"create_pytorch_scheduled_pipeline","text":"<pre><code>create_pytorch_scheduled_pipeline(\n    name: str = \"pytorch_scheduled_retraining\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    schedule: str = \"0 2 * * 0\",\n    timezone: str = \"UTC\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a scheduled retraining pipeline.</p> <p>Returns both the pipeline and a configured scheduler so you can register periodic retraining (e.g., weekly) with a single call.</p> <p>DAG: <code>load_csv_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></p> <p>Schedule format uses cron syntax (default: every Sunday at 2 AM)::</p> <pre><code>pipeline_info = create_pytorch_scheduled_pipeline(\n    context=ctx,\n    project_name=\"my_project\",\n    schedule=\"0 2 * * 0\",  # Weekly\n)\n\n# Access the components\npipeline = pipeline_info[\"pipeline\"]\nscheduler = pipeline_info[\"scheduler\"]\n\n# Run once immediately\nresult = pipeline.run()\n\n# Or start the scheduler for automatic retraining\nscheduler.start()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'pytorch_scheduled_retraining'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>schedule</code> <code>str</code> <p>Cron expression for scheduling.</p> <code>'0 2 * * 0'</code> <code>timezone</code> <code>str</code> <p>Timezone for the schedule.</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with <code>pipeline</code> and <code>scheduler</code> keys.</p> Source code in <code>mlpotion/integrations/flowyml/pytorch/pipelines.py</code> <pre><code>def create_pytorch_scheduled_pipeline(\n    name: str = \"pytorch_scheduled_retraining\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    schedule: str = \"0 2 * * 0\",\n    timezone: str = \"UTC\",\n) -&gt; dict[str, Any]:\n    \"\"\"Create a scheduled retraining pipeline.\n\n    Returns both the pipeline and a configured scheduler so you can\n    register periodic retraining (e.g., weekly) with a single call.\n\n    **DAG**: ``load_csv_data \u2192 train_model \u2192 evaluate_model \u2192 export_model``\n\n    Schedule format uses cron syntax (default: every Sunday at 2 AM)::\n\n        pipeline_info = create_pytorch_scheduled_pipeline(\n            context=ctx,\n            project_name=\"my_project\",\n            schedule=\"0 2 * * 0\",  # Weekly\n        )\n\n        # Access the components\n        pipeline = pipeline_info[\"pipeline\"]\n        scheduler = pipeline_info[\"scheduler\"]\n\n        # Run once immediately\n        result = pipeline.run()\n\n        # Or start the scheduler for automatic retraining\n        scheduler.start()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n        schedule: Cron expression for scheduling.\n        timezone: Timezone for the schedule.\n\n    Returns:\n        Dict with ``pipeline`` and ``scheduler`` keys.\n    \"\"\"\n    from flowyml.core.scheduler import PipelineScheduler\n\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,  # Fresh data each run\n        enable_checkpointing=True,\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_csv_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n\n    # Configure scheduler\n    scheduler = PipelineScheduler()\n    scheduler.schedule(\n        pipeline=pipeline,\n        cron=schedule,\n        timezone=timezone,\n    )\n\n    return {\n        \"pipeline\": pipeline,\n        \"scheduler\": scheduler,\n    }\n</code></pre>"},{"location":"api/integrations/flowyml.html#tensorflow-integration","title":"TensorFlow Integration","text":""},{"location":"api/integrations/flowyml.html#steps_2","title":"Steps","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps","title":"mlpotion.integrations.flowyml.tensorflow.steps","text":"<p>FlowyML TensorFlow steps \u2014 Full-featured pipeline steps for TF/Keras workflows.</p> <p>Each step leverages FlowyML's native capabilities: - Artifact-centric design: returns Dataset, Model, Metrics with auto-extraction - Supports caching, retry, GPU resources, tags, DAG wiring, and execution groups - train_model integrates FlowymlKerasCallback for automatic tracking</p>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps-classes","title":"Classes","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.load_data","title":"load_data","text":"<pre><code>load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str = \"target\",\n    column_names: list[str] | None = None,\n) -&gt; Dataset\n</code></pre> <p>Load CSV data into a tf.data.Dataset, wrapped as a Dataset asset.</p> <p>Automatic metadata extraction captures batch size, source path, label name, and column configuration.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>32</code> <code>label_name</code> <code>str</code> <p>Target column name.</p> <code>'target'</code> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset wrapping the tf.data.Dataset.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_load_data\",\n    outputs=[\"dataset\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"tensorflow\", \"component\": \"data_loader\"},\n)\ndef load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str = \"target\",\n    column_names: list[str] | None = None,\n) -&gt; Dataset:\n    \"\"\"Load CSV data into a tf.data.Dataset, wrapped as a Dataset asset.\n\n    Automatic metadata extraction captures batch size, source path,\n    label name, and column configuration.\n\n    Args:\n        file_path: Glob pattern for CSV files.\n        batch_size: Batch size.\n        label_name: Target column name.\n        column_names: Specific columns to load.\n\n    Returns:\n        Dataset asset wrapping the tf.data.Dataset.\n    \"\"\"\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        label_name=label_name,\n        column_names=column_names,\n    )\n    loader = CSVDataLoader(**config.dict())\n    tf_dataset = loader.load()\n\n    dataset_asset = Dataset.create(\n        data=tf_dataset,\n        name=\"tf_csv_dataset\",\n        properties={\n            \"source\": file_path,\n            \"batch_size\": batch_size,\n            \"label_name\": label_name,\n        },\n        source=file_path,\n        loader=\"CSVDataLoader\",\n        framework=\"tensorflow\",\n    )\n\n    logger.info(f\"\ud83d\udce6 Loaded TF Dataset: batch_size={batch_size}, source={file_path}\")\n    return dataset_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.optimize_data","title":"optimize_data","text":"<pre><code>optimize_data(\n    dataset: tf.data.Dataset | Dataset,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n) -&gt; Dataset\n</code></pre> <p>Optimize a tf.data.Dataset with caching and prefetching, returned as Dataset asset.</p> <p>Returns a Dataset asset with lineage linked to the input dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset | Dataset</code> <p>Input tf.data.Dataset or Dataset asset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>32</code> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Shuffle buffer size.</p> <code>None</code> <code>prefetch</code> <code>bool</code> <p>Enable prefetching.</p> <code>True</code> <code>cache</code> <code>bool</code> <p>Enable dataset caching.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset wrapping the optimized tf.data.Dataset.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_optimize_data\",\n    inputs=[\"dataset\"],\n    outputs=[\"optimized_dataset\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"tensorflow\", \"component\": \"data_optimizer\"},\n)\ndef optimize_data(\n    dataset: tf.data.Dataset | Dataset,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n) -&gt; Dataset:\n    \"\"\"Optimize a tf.data.Dataset with caching and prefetching, returned as Dataset asset.\n\n    Returns a Dataset asset with lineage linked to the input dataset.\n\n    Args:\n        dataset: Input tf.data.Dataset or Dataset asset.\n        batch_size: Batch size.\n        shuffle_buffer_size: Shuffle buffer size.\n        prefetch: Enable prefetching.\n        cache: Enable dataset caching.\n\n    Returns:\n        Dataset asset wrapping the optimized tf.data.Dataset.\n    \"\"\"\n    # Extract raw tf.data.Dataset from Dataset if wrapped\n    raw_dataset = dataset.data if isinstance(dataset, Dataset) else dataset\n\n    config = DataOptimizationConfig(\n        batch_size=batch_size,\n        shuffle_buffer_size=shuffle_buffer_size,\n        prefetch=prefetch,\n        cache=cache,\n    )\n    optimizer = DatasetOptimizer(**config.dict())\n    optimized = optimizer.optimize(raw_dataset)\n\n    parent = dataset if isinstance(dataset, Dataset) else None\n    dataset_asset = Dataset.create(\n        data=optimized,\n        name=\"tf_optimized_dataset\",\n        parent=parent,\n        properties={\n            \"batch_size\": batch_size,\n            \"prefetch\": prefetch,\n            \"cache\": cache,\n            \"shuffle_buffer_size\": shuffle_buffer_size,\n        },\n        optimizer=\"DatasetOptimizer\",\n        framework=\"tensorflow\",\n    )\n\n    logger.info(f\"\u26a1 Optimized TF Dataset: cache={cache}, prefetch={prefetch}\")\n    return dataset_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    dataset: tf.data.Dataset | Dataset,\n    model: keras.Model | Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n) -&gt; Dataset\n</code></pre> <p>Transform data using a model and save predictions to CSV, returned as a Dataset asset.</p> <p>Returns a Dataset asset with lineage linked to the input dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset | Dataset</code> <p>Input tf.data.Dataset or Dataset asset.</p> required <code>model</code> <code>keras.Model | Model</code> <p>Keras/TF model or Model asset for generating predictions.</p> required <code>data_output_path</code> <code>str</code> <p>Output path for transformed data.</p> required <code>data_output_per_batch</code> <code>bool</code> <p>If True, output one file per batch.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset asset pointing to the output CSV with parent lineage.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_transform_data\",\n    inputs=[\"dataset\"],\n    outputs=[\"transformed\"],\n    tags={\"framework\": \"tensorflow\", \"component\": \"data_transformer\"},\n)\ndef transform_data(\n    dataset: tf.data.Dataset | Dataset,\n    model: keras.Model | Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n) -&gt; Dataset:\n    \"\"\"Transform data using a model and save predictions to CSV, returned as a Dataset asset.\n\n    Returns a Dataset asset with lineage linked to the input dataset.\n\n    Args:\n        dataset: Input tf.data.Dataset or Dataset asset.\n        model: Keras/TF model or Model asset for generating predictions.\n        data_output_path: Output path for transformed data.\n        data_output_per_batch: If True, output one file per batch.\n\n    Returns:\n        Dataset asset pointing to the output CSV with parent lineage.\n    \"\"\"\n    # Extract raw objects from assets\n    raw_dataset = dataset.data if isinstance(dataset, Dataset) else dataset\n    raw_model = model.data if isinstance(model, Model) else model\n\n    transformer = DataToCSVTransformer(\n        dataset=raw_dataset,\n        model=raw_model,\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n    )\n\n    # Create minimal config for transform method\n    config = DataTransformationConfig(\n        file_pattern=\"\",  # Not used since dataset is provided\n        model_path=\"\",  # Not used since model is provided\n        model_input_signature={},  # Empty dict as model is provided directly\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n    )\n\n    transformer.transform(dataset=None, model=None, config=config)\n\n    parent = dataset if isinstance(dataset, Dataset) else None\n    transformed = Dataset.create(\n        data={\"output_path\": data_output_path},\n        name=\"tf_transformed_data\",\n        parent=parent,\n        properties={\n            \"output_path\": data_output_path,\n            \"per_batch\": data_output_per_batch,\n        },\n        source=data_output_path,\n        transformer=\"DataToCSVTransformer\",\n    )\n\n    logger.info(f\"\ud83d\udd04 Transformed data saved to: {data_output_path}\")\n    return transformed\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: keras.Model,\n    data: tf.data.Dataset | Dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    validation_data: tf.data.Dataset\n    | Dataset\n    | None = None,\n    callbacks: list[keras.callbacks.Callback] | None = None,\n    experiment_name: str | None = None,\n    project: str | None = None,\n    log_model: bool = True,\n) -&gt; tuple[Model, Metrics]\n</code></pre> <p>Train a TF/Keras model with FlowyML tracking integration.</p> <p>Automatically attaches a FlowymlKerasCallback for: - Dynamic capture of ALL training metrics - Live dashboard updates - Model artifact logging</p> <p>Returns a Model asset (via Model.from_keras with auto-extracted metadata) and a Metrics asset with training history.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>Compiled Keras model.</p> required <code>data</code> <code>tf.data.Dataset | Dataset</code> <p>Training tf.data.Dataset or Dataset asset.</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>0.001</code> <code>verbose</code> <code>int</code> <p>Keras verbosity level.</p> <code>1</code> <code>validation_data</code> <code>tf.data.Dataset | Dataset | None</code> <p>Optional validation dataset or Dataset asset.</p> <code>None</code> <code>callbacks</code> <code>list[keras.callbacks.Callback] | None</code> <p>Additional Keras callbacks.</p> <code>None</code> <code>experiment_name</code> <code>str | None</code> <p>Experiment name for FlowyML tracking.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>Project name for FlowyML dashboard.</p> <code>None</code> <code>log_model</code> <code>bool</code> <p>Whether to save model artifact after training.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Model, Metrics]</code> <p>Tuple of (Model asset, Metrics asset).</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_train_model\",\n    inputs=[\"dataset\"],\n    outputs=[\"model\", \"training_metrics\"],\n    cache=False,\n    retry=1,\n    tags={\"framework\": \"tensorflow\", \"component\": \"model_trainer\"},\n)\ndef train_model(\n    model: keras.Model,\n    data: tf.data.Dataset | Dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    validation_data: tf.data.Dataset | Dataset | None = None,\n    callbacks: list[keras.callbacks.Callback] | None = None,\n    experiment_name: str | None = None,\n    project: str | None = None,\n    log_model: bool = True,\n) -&gt; tuple[Model, Metrics]:\n    \"\"\"Train a TF/Keras model with FlowyML tracking integration.\n\n    Automatically attaches a FlowymlKerasCallback for:\n    - Dynamic capture of ALL training metrics\n    - Live dashboard updates\n    - Model artifact logging\n\n    Returns a Model asset (via Model.from_keras with auto-extracted metadata)\n    and a Metrics asset with training history.\n\n    Args:\n        model: Compiled Keras model.\n        data: Training tf.data.Dataset or Dataset asset.\n        epochs: Number of training epochs.\n        learning_rate: Learning rate.\n        verbose: Keras verbosity level.\n        validation_data: Optional validation dataset or Dataset asset.\n        callbacks: Additional Keras callbacks.\n        experiment_name: Experiment name for FlowyML tracking.\n        project: Project name for FlowyML dashboard.\n        log_model: Whether to save model artifact after training.\n\n    Returns:\n        Tuple of (Model asset, Metrics asset).\n    \"\"\"\n    # Extract raw data from Dataset if wrapped\n    raw_data = data.data if isinstance(data, Dataset) else data\n    raw_val = (\n        validation_data.data\n        if isinstance(validation_data, Dataset)\n        else validation_data\n    )\n\n    all_callbacks = list(callbacks or [])\n\n    # Auto-attach FlowyML callback\n    flowyml_callback = None\n    if experiment_name:\n        flowyml_callback = FlowymlKerasCallback(\n            experiment_name=experiment_name,\n            project=project,\n            log_model=log_model,\n        )\n        all_callbacks.append(flowyml_callback)\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metrics=[\"mae\"],\n    )\n\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model=model,\n        dataset=raw_data,\n        config=config,\n        validation_dataset=raw_val,\n    )\n\n    # Collect raw metrics\n    raw_metrics: dict[str, Any] = result.metrics if hasattr(result, \"metrics\") else {}\n    raw_metrics[\"epochs_completed\"] = epochs\n    raw_metrics[\"learning_rate\"] = learning_rate\n\n    # Wrap as Model asset using from_keras for full auto-extraction\n    model_asset = Model.from_keras(\n        model,\n        name=\"tf_trained_model\",\n        callback=flowyml_callback,\n        epochs_requested=epochs,\n    )\n\n    # Wrap as Metrics asset\n    metrics_asset = Metrics.create(\n        metrics=raw_metrics,\n        name=\"tf_training_metrics\",\n        tags={\"stage\": \"training\", \"framework\": \"tensorflow\"},\n        properties={\n            \"epochs\": epochs,\n            \"learning_rate\": learning_rate,\n            **{k: v for k, v in raw_metrics.items() if k != \"history\"},\n        },\n    )\n\n    logger.info(\n        f\"\ud83c\udfaf TF training complete: {epochs} epochs, \"\n        f\"metrics captured: {list(raw_metrics.keys())}\"\n    )\n    return model_asset, metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: keras.Model | Model,\n    data: tf.data.Dataset | Dataset,\n    verbose: int = 0,\n) -&gt; Metrics\n</code></pre> <p>Evaluate a TF/Keras model and return a Metrics asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Trained Keras model or Model asset.</p> required <code>data</code> <code>tf.data.Dataset | Dataset</code> <p>Evaluation tf.data.Dataset or Dataset asset.</p> required <code>verbose</code> <code>int</code> <p>Keras verbosity level.</p> <code>0</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics asset with evaluation results.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_evaluate_model\",\n    inputs=[\"model\", \"dataset\"],\n    outputs=[\"metrics\"],\n    cache=\"input_hash\",\n    tags={\"framework\": \"tensorflow\", \"component\": \"model_evaluator\"},\n)\ndef evaluate_model(\n    model: keras.Model | Model,\n    data: tf.data.Dataset | Dataset,\n    verbose: int = 0,\n) -&gt; Metrics:\n    \"\"\"Evaluate a TF/Keras model and return a Metrics asset.\n\n    Args:\n        model: Trained Keras model or Model asset.\n        data: Evaluation tf.data.Dataset or Dataset asset.\n        verbose: Keras verbosity level.\n\n    Returns:\n        Metrics asset with evaluation results.\n    \"\"\"\n    # Extract raw objects from assets\n    raw_model = model.data if isinstance(model, Model) else model\n    raw_data = data.data if isinstance(data, Dataset) else data\n\n    config = ModelEvaluationConfig(verbose=verbose)\n    evaluator = ModelEvaluator()\n    result = evaluator.evaluate(model=raw_model, dataset=raw_data, config=config)\n\n    raw_metrics = result.metrics if hasattr(result, \"metrics\") else {}\n\n    metrics_asset = Metrics.create(\n        metrics=raw_metrics,\n        name=\"tf_evaluation_metrics\",\n        tags={\"stage\": \"evaluation\", \"framework\": \"tensorflow\"},\n        properties=raw_metrics,\n    )\n\n    logger.info(f\"\ud83d\udcca TF evaluation: {raw_metrics}\")\n    return metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: keras.Model | Model,\n    export_path: str,\n    export_format: str = \"keras\",\n) -&gt; Model\n</code></pre> <p>Export a TF/Keras model, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Keras model or Model asset to export.</p> required <code>export_path</code> <code>str</code> <p>Destination path.</p> required <code>export_format</code> <code>str</code> <p>Format ('saved_model', 'tflite', 'keras').</p> <code>'keras'</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model asset with export metadata.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_export_model\",\n    inputs=[\"model\"],\n    outputs=[\"exported_model\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"tensorflow\", \"component\": \"model_exporter\"},\n)\ndef export_model(\n    model: keras.Model | Model,\n    export_path: str,\n    export_format: str = \"keras\",\n) -&gt; Model:\n    \"\"\"Export a TF/Keras model, returned as a Model asset.\n\n    Args:\n        model: Keras model or Model asset to export.\n        export_path: Destination path.\n        export_format: Format ('saved_model', 'tflite', 'keras').\n\n    Returns:\n        Model asset with export metadata.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    exporter = ModelExporter()\n    exporter.export(\n        model=raw_model,\n        path=export_path,\n        export_format=export_format,\n    )\n\n    model_asset = Model.from_keras(\n        raw_model,\n        name=\"tf_exported_model\",\n        export_path=export_path,\n        export_format=export_format,\n    )\n\n    logger.info(f\"\ud83d\udce4 Exported TF model to: {export_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: keras.Model | Model, save_path: str\n) -&gt; Model\n</code></pre> <p>Save a TF/Keras model to disk, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Keras model or Model asset to save.</p> required <code>save_path</code> <code>str</code> <p>Destination file path.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>Model asset with save location metadata.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_save_model\",\n    inputs=[\"model\"],\n    outputs=[\"saved_model\"],\n    tags={\"framework\": \"tensorflow\", \"component\": \"model_persistence\"},\n)\ndef save_model(\n    model: keras.Model | Model,\n    save_path: str,\n) -&gt; Model:\n    \"\"\"Save a TF/Keras model to disk, returned as a Model asset.\n\n    Args:\n        model: Keras model or Model asset to save.\n        save_path: Destination file path.\n\n    Returns:\n        Model asset with save location metadata.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    persistence = ModelPersistence(path=save_path, model=raw_model)\n    persistence.save()\n\n    model_asset = Model.from_keras(\n        raw_model,\n        name=\"tf_saved_model\",\n        save_path=save_path,\n    )\n\n    logger.info(f\"\ud83d\udcbe Saved TF model to: {save_path}\")\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.load_model","title":"load_model","text":"<pre><code>load_model(model_path: str, inspect: bool = False) -&gt; Model\n</code></pre> <p>Load a TF/Keras model from disk, returned as a Model asset.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the saved model.</p> required <code>inspect</code> <code>bool</code> <p>If True, log model inspection info.</p> <code>False</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model asset wrapping the loaded Keras model.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_load_model\",\n    outputs=[\"model\"],\n    cache=\"code_hash\",\n    tags={\"framework\": \"tensorflow\", \"component\": \"model_persistence\"},\n)\ndef load_model(\n    model_path: str,\n    inspect: bool = False,\n) -&gt; Model:\n    \"\"\"Load a TF/Keras model from disk, returned as a Model asset.\n\n    Args:\n        model_path: Path to the saved model.\n        inspect: If True, log model inspection info.\n\n    Returns:\n        Model asset wrapping the loaded Keras model.\n    \"\"\"\n    persistence = ModelPersistence(path=model_path)\n    raw_model, inspection = persistence.load(inspect=inspect)\n\n    model_asset = Model.from_keras(\n        raw_model,\n        name=\"tf_loaded_model\",\n        source_path=model_path,\n    )\n\n    if inspect and inspection:\n        logger.info(f\"\ud83d\udd0d Loaded TF model from: {model_path}, inspection: {inspection}\")\n    else:\n        logger.info(f\"\ud83d\udd0d Loaded TF model from: {model_path}\")\n\n    return model_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.steps.inspect_model","title":"inspect_model","text":"<pre><code>inspect_model(\n    model: keras.Model | Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n) -&gt; Metrics\n</code></pre> <p>Inspect a TF/Keras model and return detailed metadata as a Metrics asset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model | Model</code> <p>Keras model or Model asset to inspect.</p> required <code>include_layers</code> <code>bool</code> <p>Include per-layer information.</p> <code>True</code> <code>include_signatures</code> <code>bool</code> <p>Include input/output signatures.</p> <code>True</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics asset with model inspection details.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/steps.py</code> <pre><code>@step(\n    name=\"tf_inspect_model\",\n    inputs=[\"model\"],\n    outputs=[\"inspection\"],\n    tags={\"framework\": \"tensorflow\", \"component\": \"model_inspector\"},\n)\ndef inspect_model(\n    model: keras.Model | Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n) -&gt; Metrics:\n    \"\"\"Inspect a TF/Keras model and return detailed metadata as a Metrics asset.\n\n    Args:\n        model: Keras model or Model asset to inspect.\n        include_layers: Include per-layer information.\n        include_signatures: Include input/output signatures.\n\n    Returns:\n        Metrics asset with model inspection details.\n    \"\"\"\n    raw_model = model.data if isinstance(model, Model) else model\n\n    inspector = ModelInspector(\n        include_layers=include_layers,\n        include_signatures=include_signatures,\n    )\n    inspection = inspector.inspect(raw_model)\n\n    metrics_asset = Metrics.create(\n        metrics=inspection,\n        name=\"tf_model_inspection\",\n        tags={\"stage\": \"inspection\", \"framework\": \"tensorflow\"},\n        properties={\n            \"model_name\": inspection.get(\"name\", \"unknown\"),\n            \"total_params\": inspection.get(\"parameters\", {}).get(\"total\"),\n        },\n    )\n\n    logger.info(\n        f\"\ud83d\udd0e TF Model: {inspection.get('name', 'unknown')}, \"\n        f\"params: {inspection.get('parameters', {}).get('total', '?')}\"\n    )\n    return metrics_asset\n</code></pre>"},{"location":"api/integrations/flowyml.html#pipelines_2","title":"Pipelines","text":"<p> See the FlowyML Integration Guide for complete usage documentation and tutorials </p>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines","title":"mlpotion.integrations.flowyml.tensorflow.pipelines","text":"<p>Pre-built FlowyML pipeline templates for TensorFlow/Keras workflows.</p> <p>Provides ready-to-run pipelines that wire MLPotion TensorFlow steps together with proper DAG dependencies, context injection, experiment tracking, and optional scheduling.</p> <p>Available pipelines:</p> <ul> <li>create_tf_training_pipeline  \u2014 Load \u2192 Train \u2192 Evaluate</li> <li>create_tf_full_pipeline      \u2014 Load \u2192 Optimize \u2192 Transform \u2192 Train \u2192 Evaluate \u2192 Export</li> <li>create_tf_evaluation_pipeline \u2014 Load model + data \u2192 Evaluate \u2192 Inspect</li> <li>create_tf_export_pipeline    \u2014 Load model \u2192 Export + Save</li> <li>create_tf_experiment_pipeline \u2014 Full pipeline with conditional deploy</li> <li>create_tf_scheduled_pipeline \u2014 Scheduled retraining with cron</li> </ul>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines-functions","title":"Functions","text":""},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines.create_tf_training_pipeline","title":"create_tf_training_pipeline","text":"<pre><code>create_tf_training_pipeline(\n    name: str = \"tf_training\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a ready-to-run TensorFlow training pipeline.</p> <p>DAG: <code>load_data \u2192 train_model \u2192 evaluate_model</code></p> <p>Provide hyperparameters via the context object::</p> <pre><code>from flowyml.core.context import Context\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=10,\n    learning_rate=0.001,\n    experiment_name=\"my-experiment\",\n)\n\npipeline = create_tf_training_pipeline(\n    name=\"my_training\",\n    context=ctx,\n    project_name=\"my_project\",\n)\nresult = pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'tf_training'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters to inject.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string for versioned pipeline.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/pipelines.py</code> <pre><code>def create_tf_training_pipeline(\n    name: str = \"tf_training\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a ready-to-run TensorFlow training pipeline.\n\n    **DAG**: ``load_data \u2192 train_model \u2192 evaluate_model``\n\n    Provide hyperparameters via the context object::\n\n        from flowyml.core.context import Context\n\n        ctx = Context(\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            epochs=10,\n            learning_rate=0.001,\n            experiment_name=\"my-experiment\",\n        )\n\n        pipeline = create_tf_training_pipeline(\n            name=\"my_training\",\n            context=ctx,\n            project_name=\"my_project\",\n        )\n        result = pipeline.run()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters to inject.\n        enable_cache: Whether to enable step caching.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string for versioned pipeline.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        project_name=project_name,\n        version=version,\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines.create_tf_full_pipeline","title":"create_tf_full_pipeline","text":"<pre><code>create_tf_full_pipeline(\n    name: str = \"tf_full\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    enable_checkpointing: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a full TensorFlow pipeline covering the entire ML lifecycle.</p> <p>DAG: <code>load_data \u2192 optimize_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></p> <p>Includes data optimization (prefetch/cache/shuffle) and checkpointing for long-running training steps.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    # Data loading\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    # Optimization\n    shuffle_buffer_size=10000,\n    prefetch=True,\n    cache=True,\n    # Training\n    epochs=50,\n    learning_rate=0.001,\n    experiment_name=\"full-run\",\n    project=\"my_project\",\n    # Export\n    export_path=\"models/production/\",\n    export_format=\"saved_model\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'tf_full'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>enable_checkpointing</code> <code>bool</code> <p>Whether to enable checkpointing.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/pipelines.py</code> <pre><code>def create_tf_full_pipeline(\n    name: str = \"tf_full\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    enable_checkpointing: bool = True,\n    project_name: str | None = None,\n    version: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a full TensorFlow pipeline covering the entire ML lifecycle.\n\n    **DAG**: ``load_data \u2192 optimize_data \u2192 train_model \u2192 evaluate_model \u2192 export_model``\n\n    Includes data optimization (prefetch/cache/shuffle) and checkpointing\n    for long-running training steps.\n\n    Context parameters::\n\n        ctx = Context(\n            # Data loading\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            # Optimization\n            shuffle_buffer_size=10000,\n            prefetch=True,\n            cache=True,\n            # Training\n            epochs=50,\n            learning_rate=0.001,\n            experiment_name=\"full-run\",\n            project=\"my_project\",\n            # Export\n            export_path=\"models/production/\",\n            export_format=\"saved_model\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        enable_cache: Whether to enable step caching.\n        enable_checkpointing: Whether to enable checkpointing.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        enable_checkpointing=enable_checkpointing,\n        project_name=project_name,\n        version=version,\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(optimize_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines.create_tf_evaluation_pipeline","title":"create_tf_evaluation_pipeline","text":"<pre><code>create_tf_evaluation_pipeline(\n    name: str = \"tf_evaluation\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a pipeline for evaluating an existing TF/Keras model.</p> <p>DAG: <code>load_model \u2192 load_data \u2192 evaluate_model \u2192 inspect_model</code></p> <p>Useful for model validation, A/B testing, and periodic evaluation against new data without retraining.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    model_path=\"models/production/model.keras\",\n    file_path=\"data/test.csv\",\n    label_name=\"target\",\n    batch_size=64,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'tf_evaluation'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>enable_cache</code> <code>bool</code> <p>Whether to enable step caching.</p> <code>True</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/pipelines.py</code> <pre><code>def create_tf_evaluation_pipeline(\n    name: str = \"tf_evaluation\",\n    context: Context | None = None,\n    enable_cache: bool = True,\n    project_name: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a pipeline for evaluating an existing TF/Keras model.\n\n    **DAG**: ``load_model \u2192 load_data \u2192 evaluate_model \u2192 inspect_model``\n\n    Useful for model validation, A/B testing, and periodic evaluation\n    against new data without retraining.\n\n    Context parameters::\n\n        ctx = Context(\n            model_path=\"models/production/model.keras\",\n            file_path=\"data/test.csv\",\n            label_name=\"target\",\n            batch_size=64,\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        enable_cache: Whether to enable step caching.\n        project_name: Project to attach this pipeline to.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=enable_cache,\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_model)\n    pipeline.add_step(load_data)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(inspect_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines.create_tf_export_pipeline","title":"create_tf_export_pipeline","text":"<pre><code>create_tf_export_pipeline(\n    name: str = \"tf_export\",\n    context: Context | None = None,\n    project_name: str | None = None,\n) -&gt; Pipeline\n</code></pre> <p>Create a pipeline for exporting and saving an existing model.</p> <p>DAG: <code>load_model \u2192 export_model, save_model</code></p> <p>Useful for converting a trained model to multiple formats (SavedModel, TFLite, Keras) and persisting to different locations.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    model_path=\"models/trained/model.keras\",\n    export_path=\"models/exported/\",\n    export_format=\"saved_model\",\n    save_path=\"models/backup/model.keras\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'tf_export'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/pipelines.py</code> <pre><code>def create_tf_export_pipeline(\n    name: str = \"tf_export\",\n    context: Context | None = None,\n    project_name: str | None = None,\n) -&gt; Pipeline:\n    \"\"\"Create a pipeline for exporting and saving an existing model.\n\n    **DAG**: ``load_model \u2192 export_model, save_model``\n\n    Useful for converting a trained model to multiple formats\n    (SavedModel, TFLite, Keras) and persisting to different locations.\n\n    Context parameters::\n\n        ctx = Context(\n            model_path=\"models/trained/model.keras\",\n            export_path=\"models/exported/\",\n            export_format=\"saved_model\",\n            save_path=\"models/backup/model.keras\",\n        )\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,  # Always re-export\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_model)\n    pipeline.add_step(export_model)\n    pipeline.add_step(save_model)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines.create_tf_experiment_pipeline","title":"create_tf_experiment_pipeline","text":"<pre><code>create_tf_experiment_pipeline(\n    name: str = \"tf_experiment\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    version: str | None = None,\n    deploy_threshold: float = 0.8,\n    threshold_metric: str = \"accuracy\",\n) -&gt; Pipeline\n</code></pre> <p>Create a full experiment pipeline with conditional deployment.</p> <p>DAG::</p> <pre><code>load_data \u2192 train_model \u2192 evaluate_model\n                               \u2193\n                      [if metric &gt; threshold]\n                               \u2193\n                      export_model \u2192 save_model\n</code></pre> <p>Integrates FlowyML experiment tracking and conditionally exports the model only if validation metrics exceed the given threshold.</p> <p>Context parameters::</p> <pre><code>ctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=30,\n    learning_rate=0.001,\n    experiment_name=\"experiment-v1\",\n    project=\"my_project\",\n    export_path=\"models/production/\",\n    save_path=\"models/checkpoints/model.keras\",\n)\n\npipeline = create_tf_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\nresult = pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'tf_experiment'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>Optional version string.</p> <code>None</code> <code>deploy_threshold</code> <code>float</code> <p>Minimum metric value to trigger deployment.</p> <code>0.8</code> <code>threshold_metric</code> <code>str</code> <p>Which metric to check against the threshold.</p> <code>'accuracy'</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured FlowyML Pipeline ready for <code>.run()</code>.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/pipelines.py</code> <pre><code>def create_tf_experiment_pipeline(\n    name: str = \"tf_experiment\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    version: str | None = None,\n    deploy_threshold: float = 0.8,\n    threshold_metric: str = \"accuracy\",\n) -&gt; Pipeline:\n    \"\"\"Create a full experiment pipeline with conditional deployment.\n\n    **DAG**::\n\n        load_data \u2192 train_model \u2192 evaluate_model\n                                       \u2193\n                              [if metric &gt; threshold]\n                                       \u2193\n                              export_model \u2192 save_model\n\n    Integrates FlowyML experiment tracking and conditionally exports the\n    model only if validation metrics exceed the given threshold.\n\n    Context parameters::\n\n        ctx = Context(\n            file_path=\"data/train.csv\",\n            label_name=\"target\",\n            batch_size=32,\n            epochs=30,\n            learning_rate=0.001,\n            experiment_name=\"experiment-v1\",\n            project=\"my_project\",\n            export_path=\"models/production/\",\n            save_path=\"models/checkpoints/model.keras\",\n        )\n\n        pipeline = create_tf_experiment_pipeline(\n            context=ctx,\n            deploy_threshold=0.85,\n            threshold_metric=\"accuracy\",\n        )\n        result = pipeline.run()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n        version: Optional version string.\n        deploy_threshold: Minimum metric value to trigger deployment.\n        threshold_metric: Which metric to check against the threshold.\n\n    Returns:\n        Configured FlowyML Pipeline ready for ``.run()``.\n    \"\"\"\n    from flowyml.core.conditional import If\n\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,\n        enable_experiment_tracking=True,\n        enable_checkpointing=True,\n        project_name=project_name,\n        version=version,\n    )\n\n    # Core training DAG\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    # Conditional deployment: only export if metric exceeds threshold\n    deploy_condition = If(\n        condition=lambda metrics: (\n            metrics.get_metric(threshold_metric, 0) &gt;= deploy_threshold\n            if hasattr(metrics, \"get_metric\")\n            else metrics.get(threshold_metric, 0) &gt;= deploy_threshold\n        ),\n        then_steps=[export_model, save_model],\n        name=f\"deploy_if_{threshold_metric}_above_{deploy_threshold}\",\n    )\n    pipeline.control_flows.append(deploy_condition)\n\n    return pipeline\n</code></pre>"},{"location":"api/integrations/flowyml.html#mlpotion.integrations.flowyml.tensorflow.pipelines.create_tf_scheduled_pipeline","title":"create_tf_scheduled_pipeline","text":"<pre><code>create_tf_scheduled_pipeline(\n    name: str = \"tf_scheduled_retraining\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    schedule: str = \"0 2 * * 0\",\n    timezone: str = \"UTC\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a scheduled retraining pipeline.</p> <p>Returns both the pipeline and a configured scheduler so you can register periodic retraining (e.g., weekly) with a single call.</p> <p>DAG: <code>load_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></p> <p>Schedule format uses cron syntax (default: every Sunday at 2 AM)::</p> <pre><code>pipeline_info = create_tf_scheduled_pipeline(\n    context=ctx,\n    project_name=\"my_project\",\n    schedule=\"0 2 * * 0\",  # Weekly\n)\n\n# Access the components\npipeline = pipeline_info[\"pipeline\"]\nscheduler = pipeline_info[\"scheduler\"]\n\n# Run once immediately\nresult = pipeline.run()\n\n# Or start the scheduler for automatic retraining\nscheduler.start()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name.</p> <code>'tf_scheduled_retraining'</code> <code>context</code> <code>Context | None</code> <p>FlowyML Context with parameters.</p> <code>None</code> <code>project_name</code> <code>str | None</code> <p>Project to attach this pipeline to.</p> <code>None</code> <code>schedule</code> <code>str</code> <p>Cron expression for scheduling.</p> <code>'0 2 * * 0'</code> <code>timezone</code> <code>str</code> <p>Timezone for the schedule.</p> <code>'UTC'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with <code>pipeline</code> and <code>scheduler</code> keys.</p> Source code in <code>mlpotion/integrations/flowyml/tensorflow/pipelines.py</code> <pre><code>def create_tf_scheduled_pipeline(\n    name: str = \"tf_scheduled_retraining\",\n    context: Context | None = None,\n    project_name: str | None = None,\n    schedule: str = \"0 2 * * 0\",\n    timezone: str = \"UTC\",\n) -&gt; dict[str, Any]:\n    \"\"\"Create a scheduled retraining pipeline.\n\n    Returns both the pipeline and a configured scheduler so you can\n    register periodic retraining (e.g., weekly) with a single call.\n\n    **DAG**: ``load_data \u2192 train_model \u2192 evaluate_model \u2192 export_model``\n\n    Schedule format uses cron syntax (default: every Sunday at 2 AM)::\n\n        pipeline_info = create_tf_scheduled_pipeline(\n            context=ctx,\n            project_name=\"my_project\",\n            schedule=\"0 2 * * 0\",  # Weekly\n        )\n\n        # Access the components\n        pipeline = pipeline_info[\"pipeline\"]\n        scheduler = pipeline_info[\"scheduler\"]\n\n        # Run once immediately\n        result = pipeline.run()\n\n        # Or start the scheduler for automatic retraining\n        scheduler.start()\n\n    Args:\n        name: Pipeline name.\n        context: FlowyML Context with parameters.\n        project_name: Project to attach this pipeline to.\n        schedule: Cron expression for scheduling.\n        timezone: Timezone for the schedule.\n\n    Returns:\n        Dict with ``pipeline`` and ``scheduler`` keys.\n    \"\"\"\n    from flowyml.core.scheduler import PipelineScheduler\n\n    pipeline = Pipeline(\n        name=name,\n        context=context,\n        enable_cache=False,  # Fresh data each run\n        enable_checkpointing=True,\n        project_name=project_name,\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n\n    # Configure scheduler\n    scheduler = PipelineScheduler()\n    scheduler.schedule(\n        pipeline=pipeline,\n        cron=schedule,\n        timezone=timezone,\n    )\n\n    return {\n        \"pipeline\": pipeline,\n        \"scheduler\": scheduler,\n    }\n</code></pre>"},{"location":"api/integrations/zenml.html","title":"ZenML Integration API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's ZenML integration.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p>"},{"location":"api/integrations/zenml.html#tensorflow-steps","title":"TensorFlow Steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps","title":"mlpotion.integrations.zenml.tensorflow.steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], EvaluationMetrics]\n</code></pre> <p>Evaluate a TensorFlow/Keras model using <code>ModelEvaluator</code>.</p> <p>This step computes metrics on a given dataset using the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to evaluate.</p> required <code>dataset</code> <code>tf.data.Dataset</code> <p>The evaluation <code>tf.data.Dataset</code>.</p> required <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, float], EvaluationMetrics]</code> <p>dict[str, float]: A dictionary of computed metrics.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef evaluate_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], \"EvaluationMetrics\"]:\n    \"\"\"Evaluate a TensorFlow/Keras model using `ModelEvaluator`.\n\n    This step computes metrics on a given dataset using the provided model.\n\n    Args:\n        model: The Keras model to evaluate.\n        dataset: The evaluation `tf.data.Dataset`.\n        verbose: Verbosity mode (0 or 1).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, float]: A dictionary of computed metrics.\n    \"\"\"\n    logger.info(\"Evaluating model\")\n\n    evaluator = ModelEvaluator()\n\n    config = ModelEvaluationConfig(\n        verbose=verbose,\n    )\n\n    result = evaluator.evaluate(\n        model=model,\n        dataset=dataset,\n        config=config,\n    )\n\n    metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"metrics\": metrics})\n\n    return metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str = \"keras\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, ExportPath]\n</code></pre> <p>Export a TensorFlow/Keras model to disk using <code>ModelExporter</code>.</p> <p>This step exports the model to a specified format (e.g., Keras format, SavedModel).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to export.</p> required <code>export_path</code> <code>str</code> <p>The destination path for the exported model.</p> required <code>export_format</code> <code>str</code> <p>The format to export to (default: \"keras\").</p> <code>'keras'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, ExportPath]</code> <p>The path to the exported model artifact.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str = \"keras\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"ExportPath\"]:\n    \"\"\"Export a TensorFlow/Keras model to disk using `ModelExporter`.\n\n    This step exports the model to a specified format (e.g., Keras format, SavedModel).\n\n    Args:\n        model: The Keras model to export.\n        export_path: The destination path for the exported model.\n        export_format: The format to export to (default: \"keras\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the exported model artifact.\n    \"\"\"\n    logger.info(f\"Exporting model to: {export_path}\")\n\n    exporter = ModelExporter()\n\n    exporter.export(\n        model=model,\n        path=export_path,\n        export_format=export_format,\n    )\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"export_path\": export_path})\n\n    return export_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.inspect_model","title":"inspect_model","text":"<pre><code>inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], ModelInspection]\n</code></pre> <p>Inspect a TensorFlow/Keras model using <code>ModelInspector</code>.</p> <p>This step extracts metadata about the model, such as layer configuration, input/output shapes, and parameter counts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to inspect.</p> required <code>include_layers</code> <code>bool</code> <p>Whether to include detailed layer information.</p> <code>True</code> <code>include_signatures</code> <code>bool</code> <p>Whether to include signature information.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, Any], ModelInspection]</code> <p>dict[str, Any]: A dictionary containing the inspection results.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], \"ModelInspection\"]:\n    \"\"\"Inspect a TensorFlow/Keras model using `ModelInspector`.\n\n    This step extracts metadata about the model, such as layer configuration,\n    input/output shapes, and parameter counts.\n\n    Args:\n        model: The Keras model to inspect.\n        include_layers: Whether to include detailed layer information.\n        include_signatures: Whether to include signature information.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the inspection results.\n    \"\"\"\n    logger.info(\"Inspecting model\")\n\n    inspector = ModelInspector(\n        include_layers=include_layers,\n        include_signatures=include_signatures,\n    )\n    inspection = inspector.inspect(model)\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"inspection\": inspection})\n\n    return inspection\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.load_data","title":"load_data","text":"<pre><code>load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str = \"target\",\n    column_names: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, TFDataset]\n</code></pre> <p>Load data from local CSV files using TensorFlow's efficient loading.</p> <p>This step uses <code>CSVDataLoader</code> to create a <code>tf.data.Dataset</code> from CSV files matching the specified pattern.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str</code> <p>Name of the column to use as the label.</p> <code>'target'</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[tf.data.Dataset, TFDataset]</code> <p>tf.data.Dataset: The loaded TensorFlow dataset.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str = \"target\",\n    column_names: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, \"TFDataset\"]:\n    \"\"\"Load data from local CSV files using TensorFlow's efficient loading.\n\n    This step uses `CSVDataLoader` to create a `tf.data.Dataset` from CSV files matching\n    the specified pattern.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        tf.data.Dataset: The loaded TensorFlow dataset.\n    \"\"\"\n    logger.info(f\"Loading data from: {file_path}\")\n\n    # defining configuration\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        label_name=label_name,\n        column_names=column_names,\n    )\n\n    # initializing data loader\n    loader = CSVDataLoader(**config.dict())\n    # loading data\n    dataset = loader.load()\n\n    # adding metadata\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, LoadedModel]\n</code></pre> <p>Load a TensorFlow/Keras model from disk using <code>ModelPersistence</code>.</p> <p>This step loads a previously saved model. It can optionally inspect the loaded model to log metadata about its structure.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> required <code>inspect</code> <code>bool</code> <p>Whether to inspect the model after loading.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[keras.Model, LoadedModel]</code> <p>keras.Model: The loaded Keras model.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, \"LoadedModel\"]:\n    \"\"\"Load a TensorFlow/Keras model from disk using `ModelPersistence`.\n\n    This step loads a previously saved model. It can optionally inspect the loaded model\n    to log metadata about its structure.\n\n    Args:\n        model_path: The path to the saved model.\n        inspect: Whether to inspect the model after loading.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        keras.Model: The loaded Keras model.\n    \"\"\"\n    logger.info(f\"Loading model from: {model_path}\")\n\n    persistence = ModelPersistence(path=model_path)\n    model, inspection = persistence.load(inspect=inspect)\n\n    if metadata:\n        meta = {**metadata}\n        if inspection:\n            meta[\"inspection\"] = inspection\n        log_step_metadata(metadata=meta)\n\n    return model\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.optimize_data","title":"optimize_data","text":"<pre><code>optimize_data(\n    dataset: tf.data.Dataset,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, TFDataset]\n</code></pre> <p>Optimize a TensorFlow dataset for training performance.</p> <p>This step applies optimizations like caching, shuffling, and prefetching to the dataset using <code>DatasetOptimizer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset</code> <p>The input <code>tf.data.Dataset</code>.</p> required <code>batch_size</code> <code>int</code> <p>Batch size (if re-batching is needed).</p> <code>32</code> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Size of the shuffle buffer.</p> <code>None</code> <code>prefetch</code> <code>bool</code> <p>Whether to prefetch data.</p> <code>True</code> <code>cache</code> <code>bool</code> <p>Whether to cache data in memory.</p> <code>False</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[tf.data.Dataset, TFDataset]</code> <p>tf.data.Dataset: The optimized TensorFlow dataset.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef optimize_data(\n    dataset: tf.data.Dataset,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, \"TFDataset\"]:\n    \"\"\"Optimize a TensorFlow dataset for training performance.\n\n    This step applies optimizations like caching, shuffling, and prefetching to the dataset\n    using `DatasetOptimizer`.\n\n    Args:\n        dataset: The input `tf.data.Dataset`.\n        batch_size: Batch size (if re-batching is needed).\n        shuffle_buffer_size: Size of the shuffle buffer.\n        prefetch: Whether to prefetch data.\n        cache: Whether to cache data in memory.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        tf.data.Dataset: The optimized TensorFlow dataset.\n    \"\"\"\n    logger.info(\"Optimizing dataset for training performance\")\n\n    config = DataOptimizationConfig(\n        batch_size=batch_size,\n        shuffle_buffer_size=shuffle_buffer_size,\n        prefetch=prefetch,\n        cache=cache,\n    )\n\n    optimizer = DatasetOptimizer(**config.dict())\n    dataset = optimizer.optimize(dataset)\n\n    # adding metadata\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, SavePath]\n</code></pre> <p>Save a TensorFlow/Keras model to disk using <code>ModelPersistence</code>.</p> <p>This step saves the model for later reloading.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to save.</p> required <code>save_path</code> <code>str</code> <p>The destination path.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, SavePath]</code> <p>The path to the saved model.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"SavePath\"]:\n    \"\"\"Save a TensorFlow/Keras model to disk using `ModelPersistence`.\n\n    This step saves the model for later reloading.\n\n    Args:\n        model: The Keras model to save.\n        save_path: The destination path.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved model.\n    \"\"\"\n    logger.info(f\"Saving model to: {save_path}\")\n\n    persistence = ModelPersistence(path=save_path, model=model)\n    persistence.save()\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"save_path\": save_path})\n\n    return save_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    epochs: int = 10,\n    validation_dataset: tf.data.Dataset | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, TrainedModel],\n    Annotated[dict[str, list[float]], TrainingHistory],\n]\n</code></pre> <p>Train a TensorFlow/Keras model using <code>ModelTrainer</code>.</p> <p>This step configures and runs a training session. It supports validation data and logging of training metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to train.</p> required <code>dataset</code> <code>tf.data.Dataset</code> <p>The training <code>tf.data.Dataset</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>validation_dataset</code> <code>tf.data.Dataset | None</code> <p>Optional validation <code>tf.data.Dataset</code>.</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer.</p> <code>0.001</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0, 1, or 2).</p> <code>1</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Annotated[keras.Model, TrainedModel], Annotated[dict[str, list[float]], TrainingHistory]]</code> <p>Tuple[keras.Model, dict[str, list[float]]]: The trained model and a dictionary of history metrics.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef train_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    epochs: int = 10,\n    validation_dataset: tf.data.Dataset | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, \"TrainedModel\"],\n    Annotated[dict[str, list[float]], \"TrainingHistory\"],\n]:\n    \"\"\"Train a TensorFlow/Keras model using `ModelTrainer`.\n\n    This step configures and runs a training session. It supports validation data\n    and logging of training metrics.\n\n    Args:\n        model: The Keras model to train.\n        dataset: The training `tf.data.Dataset`.\n        epochs: Number of epochs to train.\n        validation_dataset: Optional validation `tf.data.Dataset`.\n        learning_rate: Learning rate for the Adam optimizer.\n        verbose: Verbosity mode (0, 1, or 2).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        Tuple[keras.Model, dict[str, list[float]]]: The trained model and a dictionary of history metrics.\n    \"\"\"\n    logger.info(f\"Training model for {epochs} epochs\")\n\n    trainer = ModelTrainer()\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metrics=[\"mae\"],\n    )\n\n    result = trainer.train(\n        model=model,\n        dataset=dataset,\n        config=config,\n        validation_dataset=validation_dataset,\n    )\n\n    # Result is TrainingResult object\n    training_metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"history\": result.history})\n\n    return model, training_metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    dataset: tf.data.Dataset,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, OutputPath]\n</code></pre> <p>Transform data using a TensorFlow model and save predictions to CSV.</p> <p>This step uses <code>DataToCSVTransformer</code> to run inference on a dataset using a provided model and saves the results to the specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset</code> <p>The input <code>tf.data.Dataset</code>.</p> required <code>model</code> <code>keras.Model</code> <p>The Keras model to use for transformation.</p> required <code>data_output_path</code> <code>str</code> <p>Path to save the transformed data (CSV).</p> required <code>data_output_per_batch</code> <code>bool</code> <p>Whether to save a separate file per batch.</p> <code>False</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, OutputPath]</code> <p>The path to the saved output file(s).</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef transform_data(\n    dataset: tf.data.Dataset,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"OutputPath\"]:\n    \"\"\"Transform data using a TensorFlow model and save predictions to CSV.\n\n    This step uses `DataToCSVTransformer` to run inference on a dataset using a provided model\n    and saves the results to the specified output path.\n\n    Args:\n        dataset: The input `tf.data.Dataset`.\n        model: The Keras model to use for transformation.\n        data_output_path: Path to save the transformed data (CSV).\n        data_output_per_batch: Whether to save a separate file per batch.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved output file(s).\n    \"\"\"\n    logger.info(f\"Transforming data and saving to: {data_output_path}\")\n\n    transformer = DataToCSVTransformer(\n        dataset=dataset,\n        model=model,\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n    )\n\n    # Create minimal config for transform method\n    config = DataTransformationConfig(\n        file_pattern=\"\",  # Not used since dataset is provided\n        model_path=\"\",  # Not used since model is provided\n        model_input_signature={},  # Empty dict as model is provided directly\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n    )\n\n    transformer.transform(dataset=None, model=None, config=config)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return data_output_path\n</code></pre>"},{"location":"api/integrations/zenml.html#pytorch-steps","title":"PyTorch Steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps","title":"mlpotion.integrations.zenml.pytorch.steps","text":"<p>ZenML steps for PyTorch framework.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    verbose: int = 1,\n    max_batches: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], EvaluationMetrics]\n</code></pre> <p>Evaluate a PyTorch model using <code>ModelEvaluator</code>.</p> <p>This step computes metrics on a given dataset using the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to evaluate.</p> required <code>dataloader</code> <code>DataLoader</code> <p>The evaluation <code>DataLoader</code>.</p> required <code>loss_fn</code> <code>str</code> <p>Name of the loss function (e.g., \"mse\", \"cross_entropy\").</p> <code>'mse'</code> <code>device</code> <code>str</code> <p>Device to evaluate on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>max_batches</code> <code>int | None</code> <p>Limit number of batches to evaluate (useful for debugging).</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, float], EvaluationMetrics]</code> <p>dict[str, float]: A dictionary of computed metrics.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef evaluate_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    verbose: int = 1,\n    max_batches: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], \"EvaluationMetrics\"]:\n    \"\"\"Evaluate a PyTorch model using `ModelEvaluator`.\n\n    This step computes metrics on a given dataset using the provided model.\n\n    Args:\n        model: The PyTorch model to evaluate.\n        dataloader: The evaluation `DataLoader`.\n        loss_fn: Name of the loss function (e.g., \"mse\", \"cross_entropy\").\n        device: Device to evaluate on (\"cpu\" or \"cuda\").\n        verbose: Verbosity mode (0 or 1).\n        max_batches: Limit number of batches to evaluate (useful for debugging).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, float]: A dictionary of computed metrics.\n    \"\"\"\n    logger.info(f\"Evaluating model on {device}\")\n\n    config = ModelEvaluationConfig(\n        batch_size=dataloader.batch_size or 32,\n        verbose=verbose,\n        device=device,\n        framework_options={\"loss_fn\": loss_fn, \"max_batches\": max_batches},\n    )\n\n    evaluator = ModelEvaluator()\n    result = evaluator.evaluate(\n        model=model,\n        dataloader=dataloader,\n        config=config,\n    )\n\n    # Extract metrics and evaluation time from result\n    metrics = {**result.metrics, \"evaluation_time\": result.evaluation_time}\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"metrics\": metrics})\n\n    return metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: nn.Module,\n    export_path: str,\n    export_format: str = \"state_dict\",\n    device: str = \"cpu\",\n    example_input: torch.Tensor | None = None,\n    jit_mode: str = \"script\",\n    input_names: list[str] | None = None,\n    output_names: list[str] | None = None,\n    dynamic_axes: dict[str, dict[int, str]] | None = None,\n    opset_version: int = 14,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, ExportPath]\n</code></pre> <p>Export a PyTorch model to disk using <code>ModelExporter</code>.</p> <p>This step exports the model to a specified format (TorchScript, ONNX, or state_dict).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to export.</p> required <code>export_path</code> <code>str</code> <p>The destination path for the exported model.</p> required <code>export_format</code> <code>str</code> <p>The format to export to (\"torchscript\", \"onnx\", \"state_dict\").</p> <code>'state_dict'</code> <code>device</code> <code>str</code> <p>Device to use for export (important for tracing).</p> <code>'cpu'</code> <code>example_input</code> <code>torch.Tensor | None</code> <p>Example input tensor (required for ONNX and TorchScript trace).</p> <code>None</code> <code>jit_mode</code> <code>str</code> <p>TorchScript mode (\"script\" or \"trace\").</p> <code>'script'</code> <code>input_names</code> <code>list[str] | None</code> <p>List of input names for ONNX export.</p> <code>None</code> <code>output_names</code> <code>list[str] | None</code> <p>List of output names for ONNX export.</p> <code>None</code> <code>dynamic_axes</code> <code>dict[str, dict[int, str]] | None</code> <p>Dictionary of dynamic axes for ONNX export.</p> <code>None</code> <code>opset_version</code> <code>int</code> <p>ONNX opset version.</p> <code>14</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, ExportPath]</code> <p>The path to the exported model artifact.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef export_model(\n    model: nn.Module,\n    export_path: str,\n    export_format: str = \"state_dict\",\n    device: str = \"cpu\",\n    example_input: torch.Tensor | None = None,\n    jit_mode: str = \"script\",\n    input_names: list[str] | None = None,\n    output_names: list[str] | None = None,\n    dynamic_axes: dict[str, dict[int, str]] | None = None,\n    opset_version: int = 14,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"ExportPath\"]:\n    \"\"\"Export a PyTorch model to disk using `ModelExporter`.\n\n    This step exports the model to a specified format (TorchScript, ONNX, or state_dict).\n\n    Args:\n        model: The PyTorch model to export.\n        export_path: The destination path for the exported model.\n        export_format: The format to export to (\"torchscript\", \"onnx\", \"state_dict\").\n        device: Device to use for export (important for tracing).\n        example_input: Example input tensor (required for ONNX and TorchScript trace).\n        jit_mode: TorchScript mode (\"script\" or \"trace\").\n        input_names: List of input names for ONNX export.\n        output_names: List of output names for ONNX export.\n        dynamic_axes: Dictionary of dynamic axes for ONNX export.\n        opset_version: ONNX opset version.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the exported model artifact.\n    \"\"\"\n    logger.info(f\"Exporting model to: {export_path} (format: {export_format})\")\n\n    config = ModelExportConfig(\n        export_path=export_path,\n        format=export_format,\n        device=device,\n        jit_mode=jit_mode,\n        example_input=example_input,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        opset_version=opset_version,\n    )\n\n    exporter = ModelExporter()\n    result = exporter.export(model=model, config=config)\n\n    if metadata:\n        log_step_metadata(\n            metadata={\n                **metadata,\n                \"export_path\": result.export_path,\n                \"format\": result.format,\n                \"metadata\": result.metadata,\n            }\n        )\n\n    return str(result.export_path)\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.load_csv_data","title":"load_csv_data","text":"<pre><code>load_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, PyTorchDataLoader]\n</code></pre> <p>Load data from CSV files into a PyTorch DataLoader.</p> <p>This step uses <code>CSVDataset</code> and <code>CSVDataLoader</code> to load data matching the specified file pattern. It returns a configured <code>DataLoader</code> ready for training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to copy tensors into CUDA pinned memory.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch.</p> <code>False</code> <code>dtype</code> <code>str</code> <p>Data type for the features (e.g., \"float32\").</p> <code>'float32'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>Annotated[DataLoader, PyTorchDataLoader]</code> <p>The configured PyTorch DataLoader.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef load_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, \"PyTorchDataLoader\"]:\n    \"\"\"Load data from CSV files into a PyTorch DataLoader.\n\n    This step uses `CSVDataset` and `CSVDataLoader` to load data matching the specified file pattern.\n    It returns a configured `DataLoader` ready for training or evaluation.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        shuffle: Whether to shuffle the data.\n        num_workers: Number of subprocesses to use for data loading.\n        pin_memory: Whether to copy tensors into CUDA pinned memory.\n        drop_last: Whether to drop the last incomplete batch.\n        dtype: Data type for the features (e.g., \"float32\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        DataLoader: The configured PyTorch DataLoader.\n    \"\"\"\n    logger.info(f\"Loading data from: {file_path}\")\n\n    # Convert dtype string to torch.dtype\n    torch_dtype = getattr(torch, dtype)\n\n    # Create dataset\n    dataset = CSVDataset(\n        file_pattern=file_path,\n        column_names=column_names,\n        label_name=label_name,\n        dtype=torch_dtype,\n    )\n\n    # Create DataLoader config\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        drop_last=drop_last,\n    )\n\n    # Create DataLoader using factory (exclude fields not accepted by CSVDataLoader)\n    loader_factory = CSVDataLoader(**config.dict(exclude={\"file_pattern\", \"config\"}))\n    dataloader = loader_factory.load(dataset)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataloader\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    model_class: type[nn.Module] | None = None,\n    map_location: str = \"cpu\",\n    strict: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[nn.Module, LoadedModel]\n</code></pre> <p>Load a PyTorch model from disk using <code>ModelPersistence</code>.</p> <p>This step loads a previously saved model. If loading a state dict, <code>model_class</code> must be provided.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> required <code>model_class</code> <code>type[nn.Module] | None</code> <p>The class of the model (required for state dict loading).</p> <code>None</code> <code>map_location</code> <code>str</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce state dict keys match.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[nn.Module, LoadedModel]</code> <p>nn.Module: The loaded PyTorch model.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef load_model(\n    model_path: str,\n    model_class: type[nn.Module] | None = None,\n    map_location: str = \"cpu\",\n    strict: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[nn.Module, \"LoadedModel\"]:\n    \"\"\"Load a PyTorch model from disk using `ModelPersistence`.\n\n    This step loads a previously saved model. If loading a state dict, `model_class`\n    must be provided.\n\n    Args:\n        model_path: The path to the saved model.\n        model_class: The class of the model (required for state dict loading).\n        map_location: Device to load the model onto.\n        strict: Whether to strictly enforce state dict keys match.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        nn.Module: The loaded PyTorch model.\n    \"\"\"\n    logger.info(f\"Loading model from: {model_path}\")\n\n    persistence = ModelPersistence(path=model_path)\n    model, _ = persistence.load(\n        model_class=model_class,\n        map_location=map_location,\n        strict=strict,\n    )\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"model_path\": model_path})\n\n    return model\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.load_streaming_csv_data","title":"load_streaming_csv_data","text":"<pre><code>load_streaming_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    chunksize: int = 10000,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, PyTorchDataLoader]\n</code></pre> <p>Load large CSV files as a streaming PyTorch DataLoader.</p> <p>This step uses <code>StreamingCSVDataset</code> to load data in chunks, making it suitable for datasets that do not fit in memory. It returns a <code>DataLoader</code> wrapping the iterable dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to copy tensors into CUDA pinned memory.</p> <code>False</code> <code>chunksize</code> <code>int</code> <p>Number of rows to read into memory at a time per file.</p> <code>10000</code> <code>dtype</code> <code>str</code> <p>Data type for the features (e.g., \"float32\").</p> <code>'float32'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>Annotated[DataLoader, PyTorchDataLoader]</code> <p>The configured streaming PyTorch DataLoader.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef load_streaming_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    chunksize: int = 10000,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, \"PyTorchDataLoader\"]:\n    \"\"\"Load large CSV files as a streaming PyTorch DataLoader.\n\n    This step uses `StreamingCSVDataset` to load data in chunks, making it suitable for\n    datasets that do not fit in memory. It returns a `DataLoader` wrapping the iterable dataset.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        num_workers: Number of subprocesses to use for data loading.\n        pin_memory: Whether to copy tensors into CUDA pinned memory.\n        chunksize: Number of rows to read into memory at a time per file.\n        dtype: Data type for the features (e.g., \"float32\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        DataLoader: The configured streaming PyTorch DataLoader.\n    \"\"\"\n    logger.info(f\"Loading streaming data from: {file_path}\")\n\n    # Convert dtype string to torch.dtype\n    torch_dtype = getattr(torch, dtype)\n\n    # Create streaming dataset\n    dataset = StreamingCSVDataset(\n        file_pattern=file_path,\n        column_names=column_names,\n        label_name=label_name,\n        chunksize=chunksize,\n        dtype=torch_dtype,\n    )\n\n    # Create DataLoader config (no shuffle for streaming)\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        shuffle=False,  # Streaming datasets don't support shuffle\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        drop_last=False,\n    )\n\n    # Create DataLoader using factory (exclude fields not accepted by CSVDataLoader)\n    loader_factory = CSVDataLoader(**config.dict(exclude={\"file_pattern\", \"config\"}))\n    dataloader = loader_factory.load(dataset)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataloader\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: nn.Module,\n    save_path: str,\n    save_full_model: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, SavePath]\n</code></pre> <p>Save a PyTorch model to disk using <code>ModelPersistence</code>.</p> <p>This step saves the model for later reloading. It supports saving just the state dict (recommended) or the full model object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to save.</p> required <code>save_path</code> <code>str</code> <p>The destination path.</p> required <code>save_full_model</code> <code>bool</code> <p>Whether to save the full model object (pickle) instead of state dict.</p> <code>False</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, SavePath]</code> <p>The path to the saved model.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef save_model(\n    model: nn.Module,\n    save_path: str,\n    save_full_model: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"SavePath\"]:\n    \"\"\"Save a PyTorch model to disk using `ModelPersistence`.\n\n    This step saves the model for later reloading. It supports saving just the state dict\n    (recommended) or the full model object.\n\n    Args:\n        model: The PyTorch model to save.\n        save_path: The destination path.\n        save_full_model: Whether to save the full model object (pickle) instead of state dict.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved model.\n    \"\"\"\n    logger.info(f\"Saving model to: {save_path}\")\n\n    persistence = ModelPersistence(path=save_path, model=model)\n    persistence.save(save_full_model=save_full_model)\n\n    if metadata:\n        log_step_metadata(\n            metadata={\n                **metadata,\n                \"save_path\": save_path,\n                \"save_full_model\": save_full_model,\n            }\n        )\n\n    return save_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    validation_dataloader: DataLoader | None = None,\n    verbose: int = 1,\n    max_batches_per_epoch: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[nn.Module, TrainedModel],\n    Annotated[dict[str, float], TrainingMetrics],\n]\n</code></pre> <p>Train a PyTorch model using <code>ModelTrainer</code>.</p> <p>This step configures and runs a training session. It supports validation data, custom loss functions, and automatic device management.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to train.</p> required <code>dataloader</code> <code>DataLoader</code> <p>The training <code>DataLoader</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>optimizer</code> <code>str</code> <p>Name of the optimizer (e.g., \"adam\", \"sgd\").</p> <code>'adam'</code> <code>loss_fn</code> <code>str</code> <p>Name of the loss function (e.g., \"mse\", \"cross_entropy\").</p> <code>'mse'</code> <code>device</code> <code>str</code> <p>Device to train on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>validation_dataloader</code> <code>DataLoader | None</code> <p>Optional validation <code>DataLoader</code>.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>max_batches_per_epoch</code> <code>int | None</code> <p>Limit number of batches per epoch (useful for debugging).</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Annotated[nn.Module, TrainedModel], Annotated[dict[str, float], TrainingMetrics]]</code> <p>Tuple[nn.Module, dict[str, float]]: The trained model and a dictionary of final metrics.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef train_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    validation_dataloader: DataLoader | None = None,\n    verbose: int = 1,\n    max_batches_per_epoch: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[nn.Module, \"TrainedModel\"], Annotated[dict[str, float], \"TrainingMetrics\"]\n]:\n    \"\"\"Train a PyTorch model using `ModelTrainer`.\n\n    This step configures and runs a training session. It supports validation data,\n    custom loss functions, and automatic device management.\n\n    Args:\n        model: The PyTorch model to train.\n        dataloader: The training `DataLoader`.\n        epochs: Number of epochs to train.\n        learning_rate: Learning rate for the optimizer.\n        optimizer: Name of the optimizer (e.g., \"adam\", \"sgd\").\n        loss_fn: Name of the loss function (e.g., \"mse\", \"cross_entropy\").\n        device: Device to train on (\"cpu\" or \"cuda\").\n        validation_dataloader: Optional validation `DataLoader`.\n        verbose: Verbosity mode (0 or 1).\n        max_batches_per_epoch: Limit number of batches per epoch (useful for debugging).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        Tuple[nn.Module, dict[str, float]]: The trained model and a dictionary of final metrics.\n    \"\"\"\n    logger.info(f\"Training model for {epochs} epochs on {device}\")\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        device=device,\n        verbose=verbose,\n        max_batches_per_epoch=max_batches_per_epoch,\n    )\n\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model=model,\n        dataloader=dataloader,\n        config=config,\n        validation_dataloader=validation_dataloader,\n    )\n\n    if metadata:\n        log_step_metadata(\n            metadata={\n                **metadata,\n                \"history\": result.history,\n                \"best_epoch\": result.best_epoch,\n                \"final_metrics\": result.metrics,\n            }\n        )\n    logger.info(f\"{result=}\")\n    model = result.model\n    metrics = result.metrics\n\n    return model, metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#keras-steps","title":"Keras Steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps","title":"mlpotion.integrations.zenml.keras.steps","text":"<p>ZenML steps for Keras framework.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: keras.Model,\n    data: CSVSequence,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], EvaluationMetrics]\n</code></pre> <p>Evaluate a Keras model using <code>ModelEvaluator</code>.</p> <p>This step computes metrics on a given dataset using the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to evaluate.</p> required <code>data</code> <code>CSVSequence</code> <p>The evaluation dataset (<code>CSVSequence</code>).</p> required <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, float], EvaluationMetrics]</code> <p>dict[str, float]: A dictionary of computed metrics.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef evaluate_model(\n    model: keras.Model,\n    data: CSVSequence,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], \"EvaluationMetrics\"]:\n    \"\"\"Evaluate a Keras model using `ModelEvaluator`.\n\n    This step computes metrics on a given dataset using the provided model.\n\n    Args:\n        model: The Keras model to evaluate.\n        data: The evaluation dataset (`CSVSequence`).\n        verbose: Verbosity mode (0 or 1).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, float]: A dictionary of computed metrics.\n    \"\"\"\n    logger.info(\"Evaluating model\")\n\n    evaluator = ModelEvaluator()\n\n    config = ModelEvaluationConfig(\n        verbose=verbose,\n    )\n\n    result = evaluator.evaluate(\n        model=model,\n        dataset=data,\n        config=config,\n    )\n\n    metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"metrics\": metrics})\n\n    return metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, ExportPath]\n</code></pre> <p>Export a Keras model to disk using <code>ModelExporter</code>.</p> <p>This step exports the model to a specified format (e.g., SavedModel, H5, TFLite).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to export.</p> required <code>export_path</code> <code>str</code> <p>The destination path for the exported model.</p> required <code>export_format</code> <code>str | None</code> <p>The format to export to (optional).</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, ExportPath]</code> <p>The path to the exported model artifact.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"ExportPath\"]:\n    \"\"\"Export a Keras model to disk using `ModelExporter`.\n\n    This step exports the model to a specified format (e.g., SavedModel, H5, TFLite).\n\n    Args:\n        model: The Keras model to export.\n        export_path: The destination path for the exported model.\n        export_format: The format to export to (optional).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the exported model artifact.\n    \"\"\"\n    logger.info(f\"Exporting model to: {export_path}\")\n\n    exporter = ModelExporter()\n\n    config = {}\n    if export_format:\n        config[\"export_format\"] = export_format\n\n    exporter.export(\n        model=model,\n        path=export_path,\n        **config,\n    )\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"export_path\": export_path})\n\n    return export_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.inspect_model","title":"inspect_model","text":"<pre><code>inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], ModelInspection]\n</code></pre> <p>Inspect a Keras model using <code>ModelInspector</code>.</p> <p>This step extracts metadata about the model, such as layer configuration, input/output shapes, and parameter counts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to inspect.</p> required <code>include_layers</code> <code>bool</code> <p>Whether to include detailed layer information.</p> <code>True</code> <code>include_signatures</code> <code>bool</code> <p>Whether to include signature information.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, Any], ModelInspection]</code> <p>dict[str, Any]: A dictionary containing the inspection results.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], \"ModelInspection\"]:\n    \"\"\"Inspect a Keras model using `ModelInspector`.\n\n    This step extracts metadata about the model, such as layer configuration,\n    input/output shapes, and parameter counts.\n\n    Args:\n        model: The Keras model to inspect.\n        include_layers: Whether to include detailed layer information.\n        include_signatures: Whether to include signature information.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the inspection results.\n    \"\"\"\n    logger.info(\"Inspecting model\")\n\n    inspector = ModelInspector(\n        include_layers=include_layers,\n        include_signatures=include_signatures,\n    )\n    inspection = inspector.inspect(model)\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"inspection\": inspection})\n\n    return inspection\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.load_data","title":"load_data","text":"<pre><code>load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[CSVSequence, CSVSequence]\n</code></pre> <p>Load data from CSV files into a Keras Sequence.</p> <p>This step uses <code>CSVDataLoader</code> to load data matching the specified file pattern. It returns a <code>CSVSequence</code> which can be used for training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>dtype</code> <code>str</code> <p>Data type for the features (e.g., \"float32\").</p> <code>'float32'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CSVSequence</code> <code>Annotated[CSVSequence, CSVSequence]</code> <p>The loaded Keras Sequence.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[CSVSequence, \"CSVSequence\"]:\n    \"\"\"Load data from CSV files into a Keras Sequence.\n\n    This step uses `CSVDataLoader` to load data matching the specified file pattern.\n    It returns a `CSVSequence` which can be used for training or evaluation.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        shuffle: Whether to shuffle the data.\n        dtype: Data type for the features (e.g., \"float32\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        CSVSequence: The loaded Keras Sequence.\n    \"\"\"\n    logger.info(f\"Loading data from: {file_path}\")\n\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        column_names=column_names,\n        label_name=label_name,\n        shuffle=shuffle,\n        dtype=dtype,\n    )\n\n    loader = CSVDataLoader(**config.dict())\n    sequence = loader.load()\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return sequence\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, LoadedModel]\n</code></pre> <p>Load a Keras model from disk using <code>ModelPersistence</code>.</p> <p>This step loads a previously saved model. It can optionally inspect the loaded model to log metadata about its structure.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> required <code>inspect</code> <code>bool</code> <p>Whether to inspect the model after loading.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[keras.Model, LoadedModel]</code> <p>keras.Model: The loaded Keras model.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, \"LoadedModel\"]:\n    \"\"\"Load a Keras model from disk using `ModelPersistence`.\n\n    This step loads a previously saved model. It can optionally inspect the loaded model\n    to log metadata about its structure.\n\n    Args:\n        model_path: The path to the saved model.\n        inspect: Whether to inspect the model after loading.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        keras.Model: The loaded Keras model.\n    \"\"\"\n    logger.info(f\"Loading model from: {model_path}\")\n\n    persistence = ModelPersistence(path=model_path)\n    model, inspection = persistence.load(inspect=inspect)\n\n    if metadata:\n        meta = {**metadata}\n        if inspection:\n            meta[\"inspection\"] = inspection\n        log_step_metadata(metadata=meta)\n\n    return model\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, SavePath]\n</code></pre> <p>Save a Keras model to disk using <code>ModelPersistence</code>.</p> <p>This step saves the model for later reloading, typically preserving the optimizer state.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to save.</p> required <code>save_path</code> <code>str</code> <p>The destination path.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, SavePath]</code> <p>The path to the saved model.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"SavePath\"]:\n    \"\"\"Save a Keras model to disk using `ModelPersistence`.\n\n    This step saves the model for later reloading, typically preserving the optimizer state.\n\n    Args:\n        model: The Keras model to save.\n        save_path: The destination path.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved model.\n    \"\"\"\n    logger.info(f\"Saving model to: {save_path}\")\n\n    persistence = ModelPersistence(path=save_path, model=model)\n    persistence.save()\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"save_path\": save_path})\n\n    return save_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: keras.Model,\n    data: CSVSequence,\n    epochs: int = 10,\n    validation_data: CSVSequence | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    callbacks: list[Any] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, TrainedModel],\n    Annotated[dict[str, float], TrainingMetrics],\n]\n</code></pre> <p>Train a Keras model using <code>ModelTrainer</code>.</p> <p>This step configures and runs a training session. It supports validation data, callbacks, and logging of training metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to train.</p> required <code>data</code> <code>CSVSequence</code> <p>The training dataset (<code>CSVSequence</code>).</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>validation_data</code> <code>CSVSequence | None</code> <p>Optional validation dataset (<code>CSVSequence</code>).</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer.</p> <code>0.001</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0, 1, or 2).</p> <code>1</code> <code>callbacks</code> <code>list[Any] | None</code> <p>List of Keras callbacks to apply during training.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Annotated[keras.Model, TrainedModel], Annotated[dict[str, float], TrainingMetrics]]</code> <p>Tuple[keras.Model, dict[str, float]]: The trained model and a dictionary of final metrics.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef train_model(\n    model: keras.Model,\n    data: CSVSequence,\n    epochs: int = 10,\n    validation_data: CSVSequence | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    callbacks: list[Any] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, \"TrainedModel\"],\n    Annotated[dict[str, float], \"TrainingMetrics\"],\n]:\n    \"\"\"Train a Keras model using `ModelTrainer`.\n\n    This step configures and runs a training session. It supports validation data,\n    callbacks, and logging of training metrics.\n\n    Args:\n        model: The Keras model to train.\n        data: The training dataset (`CSVSequence`).\n        epochs: Number of epochs to train.\n        validation_data: Optional validation dataset (`CSVSequence`).\n        learning_rate: Learning rate for the Adam optimizer.\n        verbose: Verbosity mode (0, 1, or 2).\n        callbacks: List of Keras callbacks to apply during training.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        Tuple[keras.Model, dict[str, float]]: The trained model and a dictionary of final metrics.\n    \"\"\"\n    logger.info(f\"Training model for {epochs} epochs\")\n\n    trainer = ModelTrainer()\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        optimizer=\"adam\",  # Defaulting to adam as per previous logic\n        loss=\"mse\",\n        metrics=[\"mae\"],\n        framework_options={\"callbacks\": callbacks} if callbacks else {},\n    )\n    # If user passed custom optimizer/loss/metrics via some other way, we might need to handle it,\n    # but here we are hardcoding them as per previous implementation.\n    # Actually, the previous implementation created an optimizer instance.\n    # ModelTrainingConfig supports passing instances via arbitrary types if allowed,\n    # or we can pass them via framework_options if the trainer supports it.\n    # But Keras ModelTrainer uses config fields.\n    # Let's stick to the config fields.\n\n    # Note: The previous implementation created a new optimizer instance: keras.optimizers.Adam(learning_rate=learning_rate)\n    # The new ModelTrainer handles optimizer creation from config.\n\n    result = trainer.train(\n        model=model,\n        dataset=data,\n        config=config,\n        validation_dataset=validation_data,\n    )\n\n    # Result is TrainingResult object\n    training_metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"history\": result.history})\n\n    return model, training_metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    dataset: CSVSequence,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    batch_size: int | None = None,\n    feature_names: list[str] | None = None,\n    input_columns: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, OutputPath]\n</code></pre> <p>Transform data using a Keras model and save predictions to CSV.</p> <p>This step uses <code>CSVDataTransformer</code> to run inference on a dataset using a provided model and saves the results to the specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CSVSequence</code> <p>The input dataset (<code>CSVSequence</code>).</p> required <code>model</code> <code>keras.Model</code> <p>The Keras model to use for transformation.</p> required <code>data_output_path</code> <code>str</code> <p>Path to save the transformed data (CSV).</p> required <code>data_output_per_batch</code> <code>bool</code> <p>Whether to save a separate file per batch.</p> <code>False</code> <code>batch_size</code> <code>int | None</code> <p>Batch size for inference (overrides dataset batch size if provided).</p> <code>None</code> <code>feature_names</code> <code>list[str] | None</code> <p>Optional list of feature names for the output CSV.</p> <code>None</code> <code>input_columns</code> <code>list[str] | None</code> <p>Optional list of input columns to pass to the model.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, OutputPath]</code> <p>The path to the saved output file(s).</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef transform_data(\n    dataset: CSVSequence,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    batch_size: int | None = None,\n    feature_names: list[str] | None = None,\n    input_columns: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"OutputPath\"]:\n    \"\"\"Transform data using a Keras model and save predictions to CSV.\n\n    This step uses `CSVDataTransformer` to run inference on a dataset using a provided model\n    and saves the results to the specified output path.\n\n    Args:\n        dataset: The input dataset (`CSVSequence`).\n        model: The Keras model to use for transformation.\n        data_output_path: Path to save the transformed data (CSV).\n        data_output_per_batch: Whether to save a separate file per batch.\n        batch_size: Batch size for inference (overrides dataset batch size if provided).\n        feature_names: Optional list of feature names for the output CSV.\n        input_columns: Optional list of input columns to pass to the model.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved output file(s).\n    \"\"\"\n    logger.info(f\"Transforming data and saving to: {data_output_path}\")\n\n    config = DataTransformationConfig(\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        input_columns=input_columns,\n    )\n\n    transformer = CSVDataTransformer(\n        dataset=dataset,\n        model=model,\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        input_columns=input_columns,\n    )\n    transformer.transform(dataset=dataset, model=model, config=config)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return data_output_path\n</code></pre>"},{"location":"api/integrations/zenml.html#materializers","title":"Materializers","text":"<p> See the ZenML Integration Guide for usage examples </p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers","title":"mlpotion.integrations.zenml.tensorflow.materializers","text":"<p>Custom materializers for TensorFlow types.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer","title":"TFConfigDatasetMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Materializer for tf.data.Dataset created from CSV files.</p> <p>Instead of serializing the entire dataset to TFRecords, this materializer stores only the configuration needed to recreate the dataset using <code>tf.data.experimental.make_csv_dataset</code>. This is much more efficient and avoids shape-related issues during serialization/deserialization.</p> <p>This materializer works specifically with datasets created via: - <code>tf.data.experimental.make_csv_dataset</code> - MLPotion's <code>TFCSVDataLoader</code></p> <p>Advantages: - Lightweight: Only stores config, not data - Fast: No TFRecord serialization overhead - Reliable: Recreates dataset with exact same parameters - Flexible: Works with any subsequent transformations (batching, shuffling, etc.)</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer.load","title":"load","text":"<pre><code>load(data_type: Type[Any]) -&gt; tf.data.Dataset\n</code></pre> <p>Load dataset by recreating it from stored configuration.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Type[Any]</code> <p>The type of the data to load.</p> required <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>Recreated tf.data.Dataset with the same configuration.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: Type[Any]) -&gt; tf.data.Dataset:\n    \"\"\"Load dataset by recreating it from stored configuration.\n\n    Args:\n        data_type: The type of the data to load.\n\n    Returns:\n        Recreated tf.data.Dataset with the same configuration.\n    \"\"\"\n    config_path = Path(self.uri) / \"config.json\"\n\n    logger.info(\"Loading CSV dataset config from: %s\", config_path)\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        config = json.load(f)\n\n    logger.info(\"Recreating dataset with config: %s\", config)\n\n    # Use CSVDataLoader to recreate the dataset\n    # This ensures we handle empty lines correctly (unlike make_csv_dataset)\n    from mlpotion.frameworks.tensorflow.data.loaders import CSVDataLoader\n\n    # Extract parameters for CSVDataLoader\n    loader_config = {\n        \"file_pattern\": config[\"file_pattern\"],\n        \"batch_size\": config[\"batch_size\"],\n        \"label_name\": config.get(\"label_name\"),\n        \"column_names\": config.get(\"column_names\"),\n    }\n\n    # Handle num_epochs and other config\n    extra_params = config.get(\"extra_params\", {})\n    if \"num_epochs\" in config:\n        extra_params[\"num_epochs\"] = config[\"num_epochs\"]\n    elif \"num_epochs\" not in extra_params:\n        extra_params[\"num_epochs\"] = 1\n\n    if extra_params:\n        loader_config[\"config\"] = extra_params\n\n    # Create loader and load dataset\n    loader = CSVDataLoader(**loader_config)\n    dataset = loader.load()\n\n    # Apply any transformations that were recorded\n    transformations = config.get(\"transformations\", [])\n    for transform in transformations:\n        transform_type = transform[\"type\"]\n        params = transform[\"params\"]\n\n        if transform_type == \"batch\":\n            dataset = dataset.batch(params[\"batch_size\"])\n        elif transform_type == \"shuffle\":\n            dataset = dataset.shuffle(params[\"buffer_size\"])\n        elif transform_type == \"prefetch\":\n            buffer_size = params[\"buffer_size\"]\n            if buffer_size == \"AUTOTUNE\":\n                buffer_size = tf.data.AUTOTUNE\n            dataset = dataset.prefetch(buffer_size)\n        elif transform_type == \"unbatch\":\n            dataset = dataset.unbatch()\n        elif transform_type == \"repeat\":\n            count = params.get(\"count\")\n            dataset = dataset.repeat(count)\n        # Add more transformation types as needed\n\n    logger.info(\"\u2705 Successfully recreated CSV dataset\")\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer.save","title":"save","text":"<pre><code>save(data: tf.data.Dataset) -&gt; None\n</code></pre> <p>Save dataset configuration instead of actual data.</p> <p>This method attempts to extract the original CSV loading configuration from the dataset. If the dataset doesn't have this metadata, it falls back to the TFRecord materializer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>tf.data.Dataset</code> <p>The dataset to save configuration for.</p> required Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.data.Dataset) -&gt; None:\n    \"\"\"Save dataset configuration instead of actual data.\n\n    This method attempts to extract the original CSV loading configuration\n    from the dataset. If the dataset doesn't have this metadata, it falls\n    back to the TFRecord materializer.\n\n    Args:\n        data: The dataset to save configuration for.\n    \"\"\"\n    config_path = Path(self.uri) / \"config.json\"\n    config_path.parent.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"\ud83d\udd35 TFConfigDatasetMaterializer.save() called\")\n    logger.info(\"Saving CSV dataset config to: %s\", config_path)\n    logger.debug(\"Dataset type: %s\", type(data))\n    logger.debug(\"URI: %s\", self.uri)\n\n    # Try to extract configuration from the dataset\n    # This requires the dataset to have been created with our loader\n    # or to have metadata attached\n    config = self._extract_config_from_dataset(data)\n\n    if config is None:\n        logger.warning(\n            \"\u274c Could not extract CSV config from dataset. \"\n            \"This materializer only works with datasets created from CSV files. \"\n            \"Falling back to TFRecord materializer.\"\n        )\n        logger.debug(\n            \"Dataset attributes: %s\",\n            [attr for attr in dir(data) if not attr.startswith(\"__\")],\n        )\n        # Fall back to TFRecord materializer\n        from mlpotion.integrations.zenml.tensorflow.materializers import (\n            TFRecordDatasetMaterializer,\n        )\n\n        logger.info(\"\ud83d\udd04 Falling back to TFRecordDatasetMaterializer\")\n        try:\n            tfrecord_materializer = TFRecordDatasetMaterializer(self.uri)\n            tfrecord_materializer.save(data)\n            logger.info(\"\u2705 Successfully saved dataset as TFRecord\")\n        except Exception as e:\n            logger.error(f\"Failed to save as TFRecord: {e}\")\n            raise\n        return\n\n    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=2)\n\n    logger.info(\"\u2705 Successfully saved CSV dataset config to: %s\", config_path)\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer","title":"TFRecordDatasetMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Generic TFRecord materializer for <code>tf.data.Dataset</code>.</p> <p>This materializer is designed to be robust and round-trip safe for datasets produced by <code>tf.data.experimental.make_csv_dataset</code>, and in general for any dataset whose <code>element_spec</code> is a nested structure of:</p> <pre><code>- dict / tuple / list containers\n- `tf.TensorSpec` leaves\n</code></pre> <p>It works as follows:</p> <ul> <li> <p>Save:</p> <ul> <li>Reads <code>dataset.element_spec</code> and serializes it to JSON.</li> <li>For each batch (dataset element), recursively flattens it to a   list of tensors in a deterministic order implied by the spec.</li> <li>Writes a single <code>tf.train.Example</code> per batch, with features   named \"f0\", \"f1\", ... corresponding to each leaf tensor.</li> </ul> </li> <li> <p>Load:</p> <ul> <li>Deserializes <code>element_spec</code> from JSON.</li> <li>Builds a <code>feature_description</code> for <code>tf.io.parse_single_example</code>   using the leaf specs.</li> <li>Parses each example into a list of tensors.</li> <li>Recursively unflattens the list back into the same nested   structure as <code>element_spec</code>.</li> </ul> </li> </ul> <p>This supports all typical <code>make_csv_dataset</code> shapes:</p> <pre><code>1. label_name=None:\n   element: dict[str, Tensor]\n\n2. label_name=\"target\":\n   element: (dict[str, Tensor], Tensor)\n\n3. label_name=[\"t1\", \"t2\"]:\n   element: (dict[str, Tensor], dict[str, Tensor])\n</code></pre> <p>and also more complex nesting as long as it's composed of dict / tuple / list and TensorSpec leaves.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer.load","title":"load","text":"<pre><code>load(data_type: Type[Any]) -&gt; tf.data.Dataset\n</code></pre> <p>Deserialize a <code>tf.data.Dataset</code> from TFRecord + metadata JSON.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: Type[Any]) -&gt; tf.data.Dataset:\n    \"\"\"Deserialize a `tf.data.Dataset` from TFRecord + metadata JSON.\"\"\"\n    dataset_dir = Path(self.uri)\n    tfrecord_path = str(dataset_dir / \"data.tfrecord\")\n    metadata_path = dataset_dir / \"metadata.json\"\n\n    logger.info(\"Loading dataset from TFRecord: %s\", tfrecord_path)\n\n    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n        metadata = json.load(f)\n\n    element_spec = self._deserialize_element_spec(metadata[\"element_spec\"])\n    num_leaves = metadata[\"num_leaves\"]\n    concrete_shapes = metadata.get(\n        \"concrete_shapes\", None\n    )  # May be None for older versions\n\n    logger.info(\"Loaded element_spec: %s\", element_spec)\n    logger.info(\"Expected number of leaves: %s\", num_leaves)\n    if concrete_shapes:\n        logger.info(\"Concrete shapes available: %s\", concrete_shapes)\n\n    flat_spec_leaves = self._flatten_element_spec(element_spec)\n    if len(flat_spec_leaves) != num_leaves:\n        raise ValueError(\n            f\"Metadata num_leaves={num_leaves} but element_spec \"\n            f\"has {len(flat_spec_leaves)} leaves.\"\n        )\n\n    # Build feature description for parsing\n    feature_description = self._build_feature_description(flat_spec_leaves)\n\n    def parse_fn(serialized_example: tf.Tensor) -&gt; Any:\n        parsed = tf.io.parse_single_example(serialized_example, feature_description)\n\n        flat_tensors: list[tf.Tensor] = []\n        for i, (_, leaf_spec) in enumerate(flat_spec_leaves):\n            key = f\"f{i}\"\n\n            if leaf_spec.dtype in (tf.float32, tf.float64, tf.int32, tf.int64):\n                # Numeric: stored as VarLenFeature, results in 1D tensor\n                dense = tf.sparse.to_dense(parsed[key])\n                tensor = tf.cast(dense, leaf_spec.dtype)\n\n                # Use concrete shape if available, otherwise fall back to spec-based logic\n                if concrete_shapes and i &lt; len(concrete_shapes):\n                    # We have the actual shape from when the data was saved\n                    concrete_shape = concrete_shapes[i]\n                    # Replace None with -1 for reshape\n                    target_shape = [\n                        d if d is not None else -1 for d in concrete_shape\n                    ]\n                    tensor = tf.reshape(tensor, target_shape)\n                    # Set the shape with proper None values\n                    tensor.set_shape(concrete_shape)\n                else:\n                    # Fallback to spec-based reshaping (legacy behavior)\n                    if leaf_spec.shape.rank is not None:\n                        if leaf_spec.shape.rank == 1:\n                            # Original was 1D, VarLen already gives us 1D - just set shape\n                            tensor.set_shape(leaf_spec.shape)\n                        elif leaf_spec.shape.rank &gt; 1:\n                            # Original was multi-dimensional - need to reshape from 1D\n                            shape_list = leaf_spec.shape.as_list()\n                            none_indices = [\n                                i for i, d in enumerate(shape_list) if d is None\n                            ]\n\n                            if len(none_indices) &lt;= 1:\n                                # Safe to reshape with at most one -1\n                                target_shape = [\n                                    d if d is not None else -1 for d in shape_list\n                                ]\n                                tensor = tf.reshape(tensor, target_shape)\n                                tensor.set_shape(leaf_spec.shape)\n                    else:\n                        # Unknown rank - just set shape\n                        tensor.set_shape(leaf_spec.shape)\n            else:\n                # Other dtypes: stored as serialized bytes\n                serialized = parsed[key]\n                tensor = tf.io.parse_tensor(serialized, out_type=leaf_spec.dtype)\n                tensor.set_shape(leaf_spec.shape)\n\n            flat_tensors.append(tensor)\n\n        # Rebuild nested structure\n        flat_iter = iter(flat_tensors)\n        return self._unflatten_data_with_spec(element_spec, flat_iter)\n\n    dataset = tf.data.TFRecordDataset(tfrecord_path)\n    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n    logger.info(\"Successfully loaded dataset from TFRecord.\")\n    logger.info(\"Dataset cardinality: %s\", dataset.cardinality().numpy())\n\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer.save","title":"save","text":"<pre><code>save(data: tf.data.Dataset) -&gt; None\n</code></pre> <p>Serialize a <code>tf.data.Dataset</code> to TFRecord + metadata JSON.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.data.Dataset) -&gt; None:\n    \"\"\"Serialize a `tf.data.Dataset` to TFRecord + metadata JSON.\"\"\"\n    dataset_dir = Path(self.uri)\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    tfrecord_path = str(dataset_dir / \"data.tfrecord\")\n    metadata_path = dataset_dir / \"metadata.json\"\n\n    element_spec = data.element_spec\n\n    logger.info(\"Saving dataset to TFRecord: %s\", tfrecord_path)\n    logger.info(\"Dataset element_spec: %s\", element_spec)\n\n    # Handle cardinality\n    cardinality = data.cardinality().numpy()\n    logger.info(\"Dataset cardinality: %s\", cardinality)\n\n    if cardinality == tf.data.INFINITE_CARDINALITY:\n        logger.warning(\"Infinite dataset detected. Taking first 100000 batches.\")\n        data = data.take(100_000)\n    elif cardinality == tf.data.UNKNOWN_CARDINALITY:\n        logger.warning(\"Unknown dataset cardinality. Taking first 100000 batches.\")\n        data = data.take(100_000)\n    else:\n        logger.info(\"Finite dataset with %s batches.\", cardinality)\n\n    # Serialize element_spec so we can restore structure and leaf specs\n    serialized_spec = self._serialize_element_spec(element_spec)\n    flat_spec_leaves = self._flatten_element_spec(element_spec)\n    num_leaves = len(flat_spec_leaves)\n\n    # Get concrete shapes from the first batch element (if available)\n    # We store shapes WITHOUT the batch dimension to handle variable batch sizes\n    concrete_shapes = None\n    try:\n        first_batch = next(iter(data.take(1)))\n        flat_tensors_sample: list[tf.Tensor] = []\n        self._flatten_data_with_spec(first_batch, element_spec, flat_tensors_sample)\n        # Store the shape WITHOUT the first (batch) dimension\n        # This allows the materializer to work with variable batch sizes\n        concrete_shapes = []\n        for t in flat_tensors_sample:\n            shape_list = list(t.shape.as_list())\n            # Remove the first (batch) dimension, keep the rest\n            if len(shape_list) &gt; 1:\n                shape_without_batch = [None] + shape_list[1:]  # None for batch dim\n            else:\n                shape_without_batch = [None]  # Just batch dimension\n            concrete_shapes.append(shape_without_batch)\n    except Exception:\n        # If we can't get a sample, proceed without concrete shapes\n        pass\n\n    metadata = {\n        \"format_version\": \"3.1\",  # Increment version for new feature\n        \"element_spec\": serialized_spec,\n        \"num_leaves\": num_leaves,\n        \"concrete_shapes\": concrete_shapes,  # Store actual shapes if available\n    }\n\n    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=2)\n\n    # Write TFRecord\n    writer = tf.io.TFRecordWriter(tfrecord_path)\n    batch_count = 0\n\n    for batch in data:\n        flat_tensors: list[tf.Tensor] = []\n        self._flatten_data_with_spec(batch, element_spec, flat_tensors)\n\n        if len(flat_tensors) != num_leaves:\n            raise ValueError(\n                f\"Flattened batch has {len(flat_tensors)} leaves but \"\n                f\"element_spec indicates {num_leaves}.\"\n            )\n\n        example = self._flat_tensors_to_example(flat_tensors)\n        writer.write(example.SerializeToString())\n        batch_count += 1\n\n        if batch_count % 100 == 0:\n            logger.info(\"Written %d batches...\", batch_count)\n\n    writer.close()\n    logger.info(\"Successfully saved %d batches to TFRecord.\", batch_count)\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer","title":"TensorMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Materializer for TensorFlow Tensor objects.</p> <p>This materializer handles the serialization and deserialization of <code>tf.Tensor</code> objects. It saves tensors as binary protobuf files (<code>tensor.pb</code>) using <code>tf.io.serialize_tensor</code>.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer.load","title":"load","text":"<pre><code>load(data_type: type[Any]) -&gt; tf.Tensor\n</code></pre> <p>Load a TensorFlow Tensor from the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type[Any]</code> <p>The type of the data to load (should be <code>tf.Tensor</code>).</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>tf.Tensor: The loaded tensor.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: type[Any]) -&gt; tf.Tensor:  # noqa: ARG002\n    \"\"\"Load a TensorFlow Tensor from the artifact store.\n\n    Args:\n        data_type: The type of the data to load (should be `tf.Tensor`).\n\n    Returns:\n        tf.Tensor: The loaded tensor.\n    \"\"\"\n    logger.info(\"Loading TensorFlow tensor...\")\n    try:\n        tensor_path = Path(self.uri) / \"tensor.pb\"\n        return tf.io.parse_tensor(\n            tf.io.read_file(str(tensor_path)), out_type=tf.float32\n        )\n    except Exception as e:\n        logger.error(f\"Failed to load tensor: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer.save","title":"save","text":"<pre><code>save(data: tf.Tensor) -&gt; None\n</code></pre> <p>Save a TensorFlow Tensor to the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>tf.Tensor</code> <p>The tensor to save.</p> required Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.Tensor) -&gt; None:\n    \"\"\"Save a TensorFlow Tensor to the artifact store.\n\n    Args:\n        data: The tensor to save.\n    \"\"\"\n    logger.info(\"Saving TensorFlow tensor...\")\n    try:\n        Path(self.uri).mkdir(parents=True, exist_ok=True)\n        tensor_path = Path(self.uri) / \"tensor.pb\"\n        tf.io.write_file(str(tensor_path), tf.io.serialize_tensor(data))\n        logger.info(\"\u2705 Successfully saved TensorFlow tensor\")\n    except Exception as e:\n        logger.error(f\"Failed to save tensor: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer","title":"TensorSpecMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Materializer for TensorFlow TensorSpec objects.</p> <p>This materializer handles the serialization and deserialization of <code>tf.TensorSpec</code> objects. It saves the spec as a JSON file (<code>spec.json</code>) containing shape, dtype, and other metadata.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer.load","title":"load","text":"<pre><code>load(data_type: type[Any]) -&gt; tf.TensorSpec\n</code></pre> <p>Load a TensorFlow TensorSpec from the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type[Any]</code> <p>The type of the data to load (should be <code>tf.TensorSpec</code>).</p> required <p>Returns:</p> Type Description <code>tf.TensorSpec</code> <p>tf.TensorSpec: The loaded tensor spec.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: type[Any]) -&gt; tf.TensorSpec:  # noqa: ARG002\n    \"\"\"Load a TensorFlow TensorSpec from the artifact store.\n\n    Args:\n        data_type: The type of the data to load (should be `tf.TensorSpec`).\n\n    Returns:\n        tf.TensorSpec: The loaded tensor spec.\n    \"\"\"\n    logger.info(\"Loading TensorFlow TensorSpec...\")\n    try:\n        spec_path = Path(self.uri) / \"spec.json\"\n        with open(spec_path) as f:\n            spec_dict = json.load(f)\n        # Reconstruct TensorSpec from dict representation\n        return tf.TensorSpec.from_spec(spec_dict)\n    except Exception as e:\n        logger.error(f\"Failed to load TensorSpec: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer.save","title":"save","text":"<pre><code>save(data: tf.TensorSpec) -&gt; None\n</code></pre> <p>Save a TensorFlow TensorSpec to the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>tf.TensorSpec</code> <p>The tensor spec to save.</p> required Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.TensorSpec) -&gt; None:\n    \"\"\"Save a TensorFlow TensorSpec to the artifact store.\n\n    Args:\n        data: The tensor spec to save.\n    \"\"\"\n    logger.info(\"Saving TensorFlow TensorSpec...\")\n    try:\n        Path(self.uri).mkdir(parents=True, exist_ok=True)\n        spec_path = Path(self.uri) / \"spec.json\"\n        # Convert TensorSpec to serializable dict format\n        spec_dict = {\n            \"shape\": list(data.shape),\n            \"dtype\": str(data.dtype),\n        }\n        with open(spec_path, \"w\") as f:\n            json.dump(spec_dict, f, indent=2)\n        logger.info(\"\u2705 Successfully saved TensorFlow TensorSpec\")\n    except Exception as e:\n        logger.error(f\"Failed to save TensorSpec: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.materializers","title":"mlpotion.integrations.zenml.pytorch.materializers","text":""},{"location":"contributing/overview.html","title":"Contributing to MLPotion \ud83e\udd1d","text":"<p>Thank you for your interest in contributing to MLPotion! We love community contributions and want to make it as easy as possible.</p>"},{"location":"contributing/overview.html#ways-to-contribute","title":"Ways to Contribute \ud83c\udf1f","text":""},{"location":"contributing/overview.html#1-report-bugs","title":"1. Report Bugs \ud83d\udc1b","text":"<p>Found a bug? Please open an issue with:</p> <ul> <li>Clear description of the bug</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (Python version, OS, framework versions)</li> <li>Minimal reproducible example</li> </ul>"},{"location":"contributing/overview.html#2-suggest-features","title":"2. Suggest Features \ud83d\udca1","text":"<p>Have an idea? Start a discussion or open an issue with:</p> <ul> <li>Clear description of the feature</li> <li>Use case and motivation</li> <li>Example API (if applicable)</li> </ul>"},{"location":"contributing/overview.html#3-submit-pull-requests","title":"3. Submit Pull Requests \ud83d\udd28","text":"<p>We welcome code contributions! See below for guidelines.</p>"},{"location":"contributing/overview.html#4-improve-documentation","title":"4. Improve Documentation \ud83d\udcd6","text":"<p>Documentation improvements are always appreciated:</p> <ul> <li>Fix typos or unclear explanations</li> <li>Add examples</li> <li>Improve API documentation</li> <li>Write tutorials</li> </ul>"},{"location":"contributing/overview.html#5-help-others","title":"5. Help Others \ud83d\ude4b","text":"<ul> <li>Answer questions in Discussions</li> <li>Help review pull requests</li> <li>Share your MLPotion projects</li> </ul>"},{"location":"contributing/overview.html#development-setup","title":"Development Setup \ud83d\udee0\ufe0f","text":""},{"location":"contributing/overview.html#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/MLPotion.git\ncd MLPotion\n</code></pre>"},{"location":"contributing/overview.html#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\n</code></pre>"},{"location":"contributing/overview.html#3-install-development-dependencies","title":"3. Install Development Dependencies","text":"<pre><code># Install dependencies with Poetry (includes dev dependencies and all extras)\npoetry install --with dev -E all\n\n# Activate the virtual environment\npoetry shell\n</code></pre>"},{"location":"contributing/overview.html#4-install-pre-commit-hooks","title":"4. Install Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/overview.html#code-style","title":"Code Style \ud83c\udfa8","text":"<p>We use:</p> <ul> <li>Black for code formatting</li> <li>isort for import sorting</li> <li>Ruff for linting</li> <li>mypy for type checking</li> </ul> <p>Run before committing:</p> <pre><code># Format code\nblack mlpotion tests\n\n# Sort imports\nisort mlpotion tests\n\n# Lint\nruff check mlpotion tests\n\n# Type check\nmypy mlpotion\n</code></pre> <p>Or just let pre-commit handle it:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"contributing/overview.html#testing","title":"Testing \ud83e\uddea","text":""},{"location":"contributing/overview.html#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/overview.html#run-specific-framework-tests","title":"Run Specific Framework Tests","text":"<pre><code># TensorFlow only\npytest -m tensorflow\n\n# PyTorch only\npytest -m pytorch\n\n# Keras only\npytest -m keras\n</code></pre>"},{"location":"contributing/overview.html#run-with-coverage","title":"Run with Coverage","text":"<pre><code>pytest --cov=mlpotion --cov-report=html\n</code></pre>"},{"location":"contributing/overview.html#write-tests","title":"Write Tests","text":"<p>All new features need tests! Place tests in <code>tests/</code> with the same structure as <code>mlpotion/</code>.</p> <p>Example:</p> <pre><code># tests/frameworks/tensorflow/test_loaders.py\nimport pytest\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader\n\ndef test_csv_loader_basic():\n    \"\"\"Test basic CSV loading.\"\"\"\n    loader = TFCSVDataLoader(\"test_data.csv\", label_name=\"target\")\n    dataset = loader.load()\n\n    assert dataset is not None\n    # More assertions...\n</code></pre>"},{"location":"contributing/overview.html#pull-request-process","title":"Pull Request Process \ud83d\udd04","text":""},{"location":"contributing/overview.html#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributing/overview.html#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write clean, documented code</li> <li>Follow existing patterns</li> <li>Add tests for new features</li> <li>Update documentation</li> </ul>"},{"location":"contributing/overview.html#3-commit-changes","title":"3. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add awesome feature\"\n</code></pre> <p>Commit message format:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation</li> <li><code>test:</code> - Tests</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>style:</code> - Code style changes</li> <li><code>chore:</code> - Maintenance</li> </ul>"},{"location":"contributing/overview.html#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with:</p> <ul> <li>Clear description of changes</li> <li>Link to related issue (if any)</li> <li>Screenshots (if UI changes)</li> <li>Checklist of completed items</li> </ul>"},{"location":"contributing/overview.html#project-structure","title":"Project Structure \ud83d\udcc1","text":"<pre><code>mlpotion/\n\u251c\u2500\u2500 core/                    # Framework-agnostic core\n\u2502   \u251c\u2500\u2500 protocols.py        # Protocol definitions\n\u2502   \u251c\u2500\u2500 results.py          # Result types\n\u2502   \u251c\u2500\u2500 config.py           # Configuration classes\n\u2502   \u2514\u2500\u2500 exceptions.py       # Custom exceptions\n\u251c\u2500\u2500 frameworks/              # Framework implementations\n\u2502   \u251c\u2500\u2500 tensorflow/         # TensorFlow components\n\u2502   \u251c\u2500\u2500 pytorch/            # PyTorch components\n\u2502   \u2514\u2500\u2500 keras/              # Keras components\n\u251c\u2500\u2500 integrations/            # Third-party integrations\n\u2502   \u2514\u2500\u2500 zenml/              # ZenML integration\n\u2514\u2500\u2500 utils/                   # Utility functions\n</code></pre>"},{"location":"contributing/overview.html#adding-a-new-component","title":"Adding a New Component \ud83c\udd95","text":""},{"location":"contributing/overview.html#1-define-protocol-if-new","title":"1. Define Protocol (if new)","text":"<pre><code># mlpotion/core/protocols.py\nfrom typing import Protocol, TypeVar\n\nDatasetT = TypeVar(\"DatasetT\")\n\nclass NewComponent(Protocol[DatasetT]):\n    \"\"\"Protocol for new component type.\"\"\"\n\n    def do_something(self, dataset: DatasetT) -&gt; DatasetT:\n        \"\"\"Do something with dataset.\"\"\"\n        ...\n</code></pre>"},{"location":"contributing/overview.html#2-implement-for-each-framework","title":"2. Implement for Each Framework","text":"<pre><code># mlpotion/frameworks/tensorflow/new_module.py\nimport tensorflow as tf\n\nclass TFNewComponent:\n    \"\"\"TensorFlow implementation.\"\"\"\n\n    def do_something(self, dataset: tf.data.Dataset) -&gt; tf.data.Dataset:\n        \"\"\"Implementation for TensorFlow.\"\"\"\n        # Your code here\n        return dataset\n</code></pre>"},{"location":"contributing/overview.html#3-add-tests","title":"3. Add Tests","text":"<pre><code># tests/frameworks/tensorflow/test_new_module.py\ndef test_new_component():\n    \"\"\"Test new component.\"\"\"\n    component = TFNewComponent()\n    result = component.do_something(dataset)\n    assert result is not None\n</code></pre>"},{"location":"contributing/overview.html#4-document","title":"4. Document","text":"<pre><code># Add docstrings\nclass TFNewComponent:\n    \"\"\"TensorFlow implementation of NewComponent.\n\n    This component does X, Y, and Z.\n\n    Args:\n        param1: Description\n        param2: Description\n\n    Example:\n        ```python\n        component = TFNewComponent(param1=\"value\")\n        result = component.do_something(dataset)\n        ```\n    \"\"\"\n</code></pre>"},{"location":"contributing/overview.html#code-review-process","title":"Code Review Process \ud83d\udc40","text":"<ol> <li>Automated Checks: CI must pass</li> <li>Code Review: At least one maintainer approval</li> <li>Documentation: Must be updated if needed</li> <li>Tests: Must pass and have good coverage</li> <li>Breaking Changes: Require discussion</li> </ol>"},{"location":"contributing/overview.html#release-process","title":"Release Process \ud83d\ude80","text":"<ol> <li>Version bump in <code>pyproject.toml</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create release tag</li> <li>GitHub Actions publishes to PyPI</li> </ol>"},{"location":"contributing/overview.html#questions","title":"Questions? \ud83d\udcac","text":"<ul> <li>GitHub Discussions</li> <li>Issues</li> <li>Email: piotr@unicolab.ai</li> </ul>"},{"location":"contributing/overview.html#code-of-conduct","title":"Code of Conduct \ud83d\udcdc","text":"<p>Be respectful, inclusive, and collaborative. We're all here to make ML better!</p> <p> Thank you for contributing to MLPotion! \ud83d\ude4f Built with \u2764\ufe0f by the community for the community </p>"},{"location":"examples/index.html","title":"MLPotion Examples","text":"<p>This directory contains practical examples demonstrating how to use MLPotion with different ML frameworks, both standalone and integrated with ZenML pipelines.</p>"},{"location":"examples/index.html#directory-structure","title":"Directory Structure","text":"<pre><code>examples/\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 sample.csv          # Sample dataset for all examples\n\u251c\u2500\u2500 keras/\n\u2502   \u251c\u2500\u2500 basic_usage.py      # Keras standalone example\n\u2502   \u2514\u2500\u2500 zenml_pipeline.py   # Keras with ZenML orchestration\n\u251c\u2500\u2500 pytorch/\n\u2502   \u251c\u2500\u2500 basic_usage.py      # PyTorch standalone example\n\u2502   \u2514\u2500\u2500 zenml_pipeline.py   # PyTorch with ZenML orchestration\n\u251c\u2500\u2500 tensorflow/\n\u2502   \u251c\u2500\u2500 basic_usage.py      # TensorFlow standalone example\n\u2502   \u2514\u2500\u2500 zenml_pipeline.py   # TensorFlow with ZenML orchestration\n\u2514\u2500\u2500 standalone/\n    \u2514\u2500\u2500 ...                 # Framework-agnostic examples\n</code></pre>"},{"location":"examples/index.html#quick-start","title":"Quick Start","text":""},{"location":"examples/index.html#1-standalone-examples-without-zenml","title":"1. Standalone Examples (Without ZenML)","text":"<p>These examples demonstrate the core MLPotion workflow without any orchestration framework:</p>"},{"location":"examples/index.html#keras","title":"Keras","text":"<pre><code>python examples/keras/basic_usage.py\n</code></pre> <p>Features demonstrated: - Load data from CSV - Create and compile a Keras model - Train the model - Evaluate performance - Save and load models</p>"},{"location":"examples/index.html#pytorch","title":"PyTorch","text":"<pre><code>python examples/pytorch/basic_usage.py\n</code></pre> <p>Features demonstrated: - Load data from CSV using PyTorchCSVDataset - Create DataLoaders with PyTorchDataLoaderFactory - Define and train a PyTorch model - Evaluate model performance - Save and load model state</p>"},{"location":"examples/index.html#tensorflow","title":"TensorFlow","text":"<pre><code>python examples/tensorflow/basic_usage.py\n</code></pre> <p>Features demonstrated: - Load data from CSV with TFCSVDataLoader - Optimize datasets for performance - Build and compile TensorFlow models - Train and evaluate models - Save and export for serving</p>"},{"location":"examples/index.html#2-zenml-pipeline-examples","title":"2. ZenML Pipeline Examples","text":"<p>These examples show how to orchestrate MLPotion components in reproducible ZenML pipelines:</p>"},{"location":"examples/index.html#prerequisites","title":"Prerequisites","text":"<pre><code># Install ZenML\npip install zenml\n\n# Initialize ZenML (first time only)\nzenml init\n\n# For testing without full stack setup\nexport ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true\n</code></pre>"},{"location":"examples/index.html#keras-pipeline","title":"Keras Pipeline","text":"<pre><code>python examples/keras/zenml_pipeline.py\n</code></pre> <p>Pipeline steps: 1. Load data 2. Create model 3. Create training config 4. Train model 5. Evaluate model 6. Save model 7. Export for serving</p>"},{"location":"examples/index.html#pytorch-pipeline","title":"PyTorch Pipeline","text":"<pre><code>python examples/pytorch/zenml_pipeline.py\n</code></pre> <p>Pipeline steps: 1. Load CSV data 2. Create PyTorch model 3. Create training config 4. Train model 5. Evaluate model 6. Save model (state_dict) 7. Export as TorchScript</p>"},{"location":"examples/index.html#tensorflow-pipeline","title":"TensorFlow Pipeline","text":"<pre><code>python examples/tensorflow/zenml_pipeline.py\n</code></pre> <p>Pipeline steps: 1. Load data from CSV 2. Optimize dataset 3. Create TensorFlow model 4. Create training config 5. Train model 6. Evaluate model 7. Save model 8. Export as SavedModel</p>"},{"location":"examples/index.html#common-patterns","title":"Common Patterns","text":""},{"location":"examples/index.html#data-loading","title":"Data Loading","text":"<p>Keras: <pre><code>from mlpotion.frameworks.keras import KerasCSVDataLoader\n\nloader = KerasCSVDataLoader(\n    file_pattern=\"examples/data/sample.csv\",\n    label_name=\"target\",\n    batch_size=8,\n    shuffle=True,\n)\ndataset = loader.load()\n</code></pre></p> <p>PyTorch: <pre><code>from mlpotion.frameworks.pytorch import PyTorchCSVDataset, PyTorchDataLoaderFactory\n\ndataset = PyTorchCSVDataset(\n    file_pattern=\"examples/data/sample.csv\",\n    label_name=\"target\",\n)\nfactory = PyTorchDataLoaderFactory(batch_size=8, shuffle=True)\ndataloader = factory.load(dataset)\n</code></pre></p> <p>TensorFlow: <pre><code>from mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFDatasetOptimizer\n\nloader = TFCSVDataLoader(\n    file_pattern=\"examples/data/sample.csv\",\n    label_name=\"target\",\n)\ndataset = loader.load()\n\noptimizer = TFDatasetOptimizer(batch_size=8, shuffle_buffer_size=100)\ndataset = optimizer.optimize(dataset)\n</code></pre></p>"},{"location":"examples/index.html#training","title":"Training","text":"<p>All frameworks use a similar training pattern:</p> <pre><code>from mlpotion.frameworks.[framework] import [Framework]ModelTrainer, [Framework]TrainingConfig\n\ntrainer = [Framework]ModelTrainer()\nconfig = [Framework]TrainingConfig(\n    epochs=10,\n    batch_size=8,\n    learning_rate=0.001,\n    verbose=1,\n)\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"examples/index.html#evaluation","title":"Evaluation","text":"<pre><code>from mlpotion.frameworks.[framework] import [Framework]ModelEvaluator\n\nevaluator = [Framework]ModelEvaluator()\neval_result = evaluator.evaluate(model, dataset, config)\nprint(eval_result.metrics)\n</code></pre>"},{"location":"examples/index.html#model-persistence","title":"Model Persistence","text":"<pre><code>from mlpotion.frameworks.[framework] import [Framework]ModelPersistence\n\npersistence = [Framework]ModelPersistence()\n\n# Save\npersistence.save(model, \"/path/to/model\", save_format=\"...\")\n\n# Load\nloaded_model = persistence.load(\"/path/to/model\")\n</code></pre>"},{"location":"examples/index.html#sample-data","title":"Sample Data","text":"<p>The <code>examples/data/sample.csv</code> file contains synthetic regression data with: - 10 features (feature_0 through feature_9) - 1 target variable - 50 samples</p> <p>This dataset is used across all examples for consistency.</p>"},{"location":"examples/index.html#zenml-integration-benefits","title":"ZenML Integration Benefits","text":"<p>Using ZenML pipelines provides:</p> <ol> <li>Reproducibility: Track all pipeline runs with versioned artifacts</li> <li>Experiment Tracking: Compare different configurations and results</li> <li>Collaboration: Share pipelines with team members</li> <li>Scalability: Run pipelines on different compute backends</li> <li>Artifact Caching: Skip unchanged steps in subsequent runs</li> </ol>"},{"location":"examples/index.html#customization","title":"Customization","text":"<p>Each example is designed to be easily customizable:</p> <ol> <li>Change model architecture: Modify the model creation code</li> <li>Adjust hyperparameters: Update the training configuration</li> <li>Use your own data: Replace the file path with your CSV file</li> <li>Add validation split: Set <code>validation_split</code> in training config</li> <li>Enable early stopping: Configure callbacks in training config</li> </ol>"},{"location":"examples/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/index.html#import-errors","title":"Import Errors","text":"<p>Make sure MLPotion is installed: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"examples/index.html#zenml-errors","title":"ZenML Errors","text":"<p>If you encounter ZenML initialization errors: <pre><code>export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true\n</code></pre></p>"},{"location":"examples/index.html#data-loading-issues","title":"Data Loading Issues","text":"<p>Ensure the CSV file exists and has the correct format: <pre><code>ls -la examples/data/sample.csv\nhead examples/data/sample.csv\n</code></pre></p>"},{"location":"examples/index.html#next-steps","title":"Next Steps","text":"<ul> <li>Explore the MLPotion documentation</li> <li>Check out the test suite for more usage examples</li> <li>Read about ZenML best practices</li> <li>Customize the examples for your own use cases</li> </ul>"},{"location":"examples/index.html#contributing","title":"Contributing","text":"<p>Found an issue or want to add a new example? Please open an issue or PR!</p>"},{"location":"frameworks/keras.html","title":"Keras Guide \ud83c\udfa8","text":"<p>Complete guide to using MLPotion with Keras 3 - the user-friendly, backend-agnostic ML framework!</p>"},{"location":"frameworks/keras.html#why-keras-mlpotion","title":"Why Keras + MLPotion? \ud83e\udd14","text":"<ul> <li>User-Friendly: Simple, consistent API</li> <li>Backend-Agnostic: Switch between TensorFlow, PyTorch, and JAX</li> <li>Fast Prototyping: Build models quickly</li> <li>Production-Ready: Keras 3 is production-grade</li> <li>MLPotion Benefits: Type-safe, modular components</li> </ul>"},{"location":"frameworks/keras.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code># Keras with TensorFlow backend (default)\npoetry add mlpotion -E tensorflow\n\n# Keras with PyTorch backend\npoetry add mlpotion -E keras-pytorch\n\n# Keras with JAX backend\npoetry add mlpotion -E keras-jax\n</code></pre>"},{"location":"frameworks/keras.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport keras\n\n# Load data\nloader = CSVDataLoader(\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()\n\n# Create model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\nresult = trainer.train(model, dataset, config)\n\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/keras.html#advanced-training","title":"Advanced Training \ud83c\udf93","text":""},{"location":"frameworks/keras.html#custom-optimizers-loss-and-metrics","title":"Custom Optimizers, Loss, and Metrics","text":"<pre><code>import keras\n\n# Custom optimizer instance\ncustom_optimizer = keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    clipnorm=1.0,\n)\n\n# Custom loss instance\ncustom_loss = keras.losses.Huber(delta=1.0)\n\n# Custom metrics\ncustom_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n\nconfig = ModelTrainingConfig(\n    epochs=50,\n    batch_size=32,\n    optimizer=custom_optimizer,  # Optimizer instance\n    loss=custom_loss,            # Loss instance\n    metrics=[custom_metric, \"mse\"],  # Mix of instances and strings\n)\n\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/keras.html#callbacks-and-tensorboard","title":"Callbacks and TensorBoard","text":"<pre><code># Method 1: Pass callback instances\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True,\n)\n\ncsv_logger = keras.callbacks.CSVLogger(\"training.log\")\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[early_stopping, csv_logger],\n    use_tensorboard=True,  # Enabled by default\n    tensorboard_log_dir=\"logs/keras_experiment\",\n)\n\n# Method 2: Pass callback configs as dicts\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[\n        {\n            \"name\": \"EarlyStopping\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"patience\": 5,\n                \"restore_best_weights\": True,\n            }\n        },\n        {\n            \"name\": \"ReduceLROnPlateau\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"factor\": 0.5,\n                \"patience\": 3,\n            }\n        },\n    ],\n)\n\nresult = trainer.train(model, dataset, config, validation_dataset=val_dataset)\n\n# View TensorBoard\n# tensorboard --logdir=logs/keras_experiment\n</code></pre>"},{"location":"frameworks/keras.html#complete-example","title":"Complete Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport keras\n\n# Load data\nloader = CSVDataLoader(\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()\n\n# Create model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\n# Configure training with all features\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    batch_size=32,\n\n    # Custom optimizer\n    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n\n    # Loss and metrics\n    loss=\"mse\",\n    metrics=[\"mae\", keras.metrics.RootMeanSquaredError()],\n\n    # Callbacks\n    callbacks=[\n        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n        {\"name\": \"ReduceLROnPlateau\", \"params\": {\"factor\": 0.5, \"patience\": 5}},\n    ],\n\n    # TensorBoard\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/my_experiment\",\n    tensorboard_params={\n        \"histogram_freq\": 1,\n        \"write_graph\": True,\n    },\n)\n\n# Train\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"Training completed in {result.training_time:.2f}s\")\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre> <p>For complete Keras documentation, see the TensorFlow Guide as Keras components use the same patterns!</p> <p> Keras + MLPotion = Simplicity + Power! \ud83c\udfa8 </p>"},{"location":"frameworks/pytorch.html","title":"PyTorch Guide \ud83d\udd25","text":"<p>Complete guide to using MLPotion with PyTorch - the researcher's favorite framework!</p>"},{"location":"frameworks/pytorch.html#why-pytorch-mlpotion","title":"Why PyTorch + MLPotion? \ud83e\udd14","text":"<ul> <li>Research-Friendly: Dynamic computation graphs, easy debugging</li> <li>Pythonic: Feels like native Python code</li> <li>Flexible: Full control over training loops</li> <li>Ecosystem: Huge community, extensive libraries</li> <li>MLPotion Benefits: Type-safe, modular components with consistent APIs</li> </ul>"},{"location":"frameworks/pytorch.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code>poetry add mlpotion -E pytorch\n</code></pre> <p>This installs: - <code>torch&gt;=2.0</code> - <code>torchvision&gt;=0.16</code> - All PyTorch-specific MLPotion components</p>"},{"location":"frameworks/pytorch.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.pytorch import (\n    CSVDataset,\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport torch.nn as nn\n\n# Load data\ndataset = CSVDataset(\"data.csv\", label_name=\"target\")\nfactory = CSVDataLoader(batch_size=32, shuffle=True)\ndataloader = factory.load(dataset)\n\n# Create model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(10, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nmodel = SimpleModel()\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\nresult = trainer.train(model, dataloader, config)\n\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#data-loading","title":"Data Loading \ud83d\udcca","text":""},{"location":"frameworks/pytorch.html#csv-dataset","title":"CSV Dataset","text":"<pre><code>from mlpotion.frameworks.pytorch import CSVDataset\n\ndataset = CSVDataset(\n    file_pattern=\"data.csv\",        # File path or pattern\n    label_name=\"target\",            # Label column name\n    column_names=None,              # Auto-detect or specify\n    dtype=torch.float32,            # Data type\n)\n\n# Use like any PyTorch dataset\nprint(f\"Dataset size: {len(dataset)}\")\nfeatures, label = dataset[0]\n</code></pre>"},{"location":"frameworks/pytorch.html#dataloader-factory","title":"DataLoader Factory","text":"<pre><code>from mlpotion.frameworks.pytorch import CSVDataLoader\n\nfactory = CSVDataLoader(\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,                  # Parallel data loading\n    pin_memory=True,                # Faster GPU transfer\n    drop_last=False,\n    persistent_workers=True,        # Keep workers alive\n)\n\n# Create dataloaders\ntrain_loader = factory.load(train_dataset)\nval_loader = factory.load(val_dataset)\ntest_loader = factory.load(test_dataset)\n</code></pre>"},{"location":"frameworks/pytorch.html#model-training","title":"Model Training \ud83c\udf93","text":""},{"location":"frameworks/pytorch.html#basic-training","title":"Basic Training","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelTrainer, ModelTrainingConfig\n\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    device=\"cuda\",\n    optimizer=\"adam\",\n    loss_fn=\"mse\",\n    verbose=True,\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, train_loader, config)\n\nprint(f\"Training time: {result.training_time:.2f}s\")\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#advanced-training-configuration","title":"Advanced Training Configuration","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Using string optimizer name\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    optimizer=\"adamw\",  # String name\n    loss_fn=\"mse\",\n    verbose=True,\n)\n\n# Using custom optimizer instance\ncustom_optimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    weight_decay=0.01,\n    amsgrad=True,\n)\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    device=\"cuda\",\n    optimizer=custom_optimizer,  # Pass optimizer instance\n    loss_fn=\"mse\",\n)\n\n# Using custom loss function\ncustom_loss = nn.SmoothL1Loss(beta=1.0)\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=custom_loss,  # Custom loss instance\n)\n\nresult = trainer.train(model, train_loader, config, validation_dataloader=val_loader)\n</code></pre>"},{"location":"frameworks/pytorch.html#callbacks-and-tensorboard","title":"Callbacks and TensorBoard","text":"<pre><code># Custom callback class\nclass TrainingCallback:\n    def on_train_begin(self):\n        print(\"\ud83d\ude80 Training started!\")\n\n    def on_epoch_end(self, epoch, metrics):\n        print(f\"Epoch {epoch + 1} completed: {metrics}\")\n        # Add custom logic (e.g., save checkpoint, adjust LR)\n\n    def on_train_end(self):\n        print(\"\u2705 Training completed!\")\n\n# Early stopping callback example\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.should_stop = False\n\n    def on_epoch_end(self, epoch, metrics):\n        val_loss = metrics.get('val_loss')\n        if val_loss is None:\n            return\n\n        if val_loss &lt; self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n                self.should_stop = True\n\n# Configure with callbacks and TensorBoard\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=\"mse\",\n\n    # Add callbacks\n    callbacks=[\n        TrainingCallback(),\n        EarlyStopping(patience=10),\n    ],\n\n    # Enable TensorBoard\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/pytorch_experiment\",\n    tensorboard_params={\n        \"comment\": \"My experiment\",\n        \"flush_secs\": 30,\n    },\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, train_loader, config, validation_dataloader=val_loader)\n\n# View TensorBoard logs\n# tensorboard --logdir=logs/pytorch_experiment\n</code></pre>"},{"location":"frameworks/pytorch.html#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code># Method 1: Use nn.Module\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.5):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        mse = nn.functional.mse_loss(predictions, targets)\n        mae = nn.functional.l1_loss(predictions, targets)\n        return self.alpha * mse + (1 - self.alpha) * mae\n\n# Method 2: Use callable function\ndef custom_loss_fn(predictions, targets):\n    return torch.mean((predictions - targets) ** 2) + 0.1 * torch.mean(torch.abs(predictions - targets))\n\n# Use in config\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=CustomLoss(alpha=0.7),  # or custom_loss_fn\n)\n</code></pre>"},{"location":"frameworks/pytorch.html#custom-training-loop","title":"Custom Training Loop","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelTrainer\nimport torch\n\nclass CustomTrainer(ModelTrainer):\n    def training_step(self, model, batch, device):\n        \"\"\"Custom training step logic.\"\"\"\n        features, labels = batch\n        features, labels = features.to(device), labels.to(device)\n\n        # Forward pass\n        predictions = model(features)\n        loss = self.criterion(predictions, labels)\n\n        # Add custom regularization\n        l2_reg = sum(p.pow(2).sum() for p in model.parameters())\n        loss = loss + 0.001 * l2_reg\n\n        return loss\n\n    def validation_step(self, model, batch, device):\n        \"\"\"Custom validation step logic.\"\"\"\n        features, labels = batch\n        features, labels = features.to(device), labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(features)\n            loss = self.criterion(predictions, labels)\n\n        return loss\n\n# Use custom trainer\ncustom_trainer = CustomTrainer()\nresult = custom_trainer.train(model, train_loader, config)\n</code></pre>"},{"location":"frameworks/pytorch.html#model-evaluation","title":"Model Evaluation \ud83d\udcca","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelEvaluator, ModelEvaluationConfig\n\nconfig = ModelEvaluationConfig(\n    device=\"cuda\",\n    batch_size=32,\n    metrics=[\"mse\", \"mae\"],\n    verbose=True,\n)\n\nevaluator = ModelEvaluator()\nresult = evaluator.evaluate(model, test_loader, config)\n\nprint(f\"Test loss: {result.metrics['loss']:.4f}\")\nprint(f\"Test MAE: {result.metrics['mae']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#model-persistence","title":"Model Persistence \ud83d\udcbe","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelPersistence\n\npersistence = ModelPersistence(path=\"models/my_model.pth\", model=model)\n\n# Save model (state_dict - recommended)\npersistence.save()\n\n# Save full model\npersistence.save(save_full_model=True)\n\n# Load model\nloader = ModelPersistence(path=\"models/my_model.pth\")\nloaded_model, metadata = loader.load(\n    model_class=SimpleModel,  # Need model class for state_dict\n)\n\n# Load full model (auto-detected)\nloader_full = ModelPersistence(path=\"models/my_model_full.pth\")\nloaded_model_full, _ = loader_full.load()\n</code></pre>"},{"location":"frameworks/pytorch.html#model-export","title":"Model Export \ud83d\udce4","text":""},{"location":"frameworks/pytorch.html#torchscript","title":"TorchScript","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelExporter, ModelExportConfig\n\nexporter = ModelExporter()\n\nconfig = ModelExportConfig(\n    format=\"torchscript\",\n    method=\"trace\",  # or \"script\"\n    example_inputs=torch.randn(1, 10),  # For tracing\n)\n\nresult = exporter.export(model, \"exports/model.pt\", config)\nprint(f\"Exported to: {result.export_path}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#onnx","title":"ONNX","text":"<pre><code>config = ModelExportConfig(\n    format=\"onnx\",\n    input_names=[\"features\"],\n    output_names=[\"predictions\"],\n    dynamic_axes={\"features\": {0: \"batch_size\"}},\n    opset_version=14,\n)\n\nresult = exporter.export(model, \"exports/model.onnx\", config)\n</code></pre>"},{"location":"frameworks/pytorch.html#common-patterns","title":"Common Patterns \ud83c\udfaf","text":""},{"location":"frameworks/pytorch.html#pattern-train-val-test-pipeline","title":"Pattern: Train-Val-Test Pipeline","text":"<pre><code># Load data\ntrain_dataset = CSVDataset(\"train.csv\", label_name=\"target\")\nval_dataset = CSVDataset(\"val.csv\", label_name=\"target\")\ntest_dataset = CSVDataset(\"test.csv\", label_name=\"target\")\n\n# Create dataloaders\nfactory = CSVDataLoader(batch_size=32, shuffle=True, num_workers=4)\ntrain_loader = factory.load(train_dataset)\nval_loader = factory.load(val_dataset)\ntest_loader = factory.load(test_dataset)\n\n# Train with validation\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    early_stopping=True,\n    early_stopping_patience=10,\n)\n\nresult = trainer.train(model, train_loader, config, val_loader=val_loader)\n\n# Evaluate on test set\nevaluator = ModelEvaluator()\ntest_metrics = evaluator.evaluate(result.model, test_loader, config)\n\nprint(f\"Best epoch: {result.best_epoch}\")\nprint(f\"Test loss: {test_metrics.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#pattern-multi-gpu-training","title":"Pattern: Multi-GPU Training","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Wrap model for multi-GPU\nif torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\n    device = \"cuda\"\nelse:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = model.to(device)\n\n# Train as usual\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    device=device,\n)\n\nresult = trainer.train(model, train_loader, config)\n</code></pre>"},{"location":"frameworks/pytorch.html#pattern-mixed-precision-training","title":"Pattern: Mixed Precision Training","text":"<pre><code>config = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    use_amp=True,  # Enable automatic mixed precision\n    device=\"cuda\",\n)\n\nresult = trainer.train(model, train_loader, config)\n</code></pre>"},{"location":"frameworks/pytorch.html#best-practices","title":"Best Practices \ud83d\udca1","text":"<ol> <li>Use DataLoaders: Always use <code>CSVDataLoader</code> for efficient loading</li> <li>Enable num_workers: Set <code>num_workers&gt;0</code> for parallel data loading</li> <li>Pin Memory: Use <code>pin_memory=True</code> for faster GPU transfer</li> <li>AMP Training: Enable mixed precision for faster training</li> <li>Gradient Clipping: Prevent exploding gradients with <code>clip_grad_norm</code></li> <li>State Dict: Save models as state_dict for better compatibility</li> </ol>"},{"location":"frameworks/pytorch.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>TensorFlow Guide \u2192 - Compare with TensorFlow</li> <li>ZenML Integration \u2192 - Add MLOps</li> <li>API Reference \u2192 - Detailed API docs</li> </ul> <p> PyTorch + MLPotion = Research + Production! \ud83d\udd25 </p>"},{"location":"frameworks/tensorflow.html","title":"TensorFlow Guide \ud83d\udd36","text":"<p>Complete guide to using MLPotion with TensorFlow - the production-ready ML framework.</p>"},{"location":"frameworks/tensorflow.html#why-tensorflow-mlpotion","title":"Why TensorFlow + MLPotion? \ud83e\udd14","text":"<ul> <li>Production Ready: Industry-standard deployment</li> <li>Ecosystem: Rich tooling (TensorBoard, TFX, TFLite)</li> <li>Performance: Optimized for TPUs and GPUs</li> <li>Scalability: Distributed training built-in</li> <li>MLPotion Benefits: Type-safe, modular components</li> </ul>"},{"location":"frameworks/tensorflow.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code>poetry add mlpotion -E tensorflow\n</code></pre> <p>This installs: - <code>tensorflow&gt;=2.15</code> - <code>keras&gt;=3.0</code> - All TensorFlow-specific MLPotion components</p>"},{"location":"frameworks/tensorflow.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport tensorflow as tf\n\n# Load data\nloader = CSVDataLoader(\"data.csv\", label_name=\"target\")\ndataset = loader.load()\n\n# Create model\nmodel = tf.keras.Sequential([...])\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(epochs=10, learning_rate=0.001)\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#data-loading","title":"Data Loading \ud83d\udcca","text":""},{"location":"frameworks/tensorflow.html#csv-data-loader","title":"CSV Data Loader","text":"<pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\nloader = CSVDataLoader(\n    file_pattern=\"data/*.csv\",      # Supports glob patterns\n    label_name=\"target\",            # Column to use as label\n    column_names=None,              # Auto-detect or specify list[str]\n    batch_size=32,\n    # Extra options passed to tf.data.experimental.make_csv_dataset\n    config={\n        \"shuffle\": True,\n        \"shuffle_buffer_size\": 10000,\n        \"num_parallel_reads\": 4,\n        \"compression_type\": None,\n        \"header\": True,\n        \"field_delim\": \",\",\n        \"use_quote_delim\": True,\n        \"na_value\": \"\",\n        \"select_columns\": None,\n    }\n)\n\ndataset = loader.load()  # Returns tf.data.Dataset\n</code></pre> <p>Features: - Automatic type inference - Parallel reading for performance - Memory-efficient streaming - Glob pattern support for multiple files</p>"},{"location":"frameworks/tensorflow.html#dataset-optimization","title":"Dataset Optimization","text":"<pre><code>from mlpotion.frameworks.tensorflow import DatasetOptimizer\n\noptimizer = DatasetOptimizer(\n    batch_size=32,\n    shuffle_buffer_size=10000,      # Set to enable shuffling\n    cache=True,                     # Cache in memory\n    prefetch=True,                  # Prefetch for performance\n)\n\noptimized = optimizer.optimize(dataset)\n</code></pre> <p>Performance Tips: - Use <code>cache=True</code> for datasets that fit in memory - Enable <code>prefetch=True</code> to overlap data loading with training - Set <code>shuffle_buffer_size</code> to a large enough value for good randomization</p>"},{"location":"frameworks/tensorflow.html#model-training","title":"Model Training \ud83c\udf93","text":""},{"location":"frameworks/tensorflow.html#basic-training","title":"Basic Training","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelTrainer, ModelTrainingConfig\n\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    batch_size=32,\n    optimizer=\"adam\",  # Can be string or optimizer instance\n    loss=\"mse\",\n    metrics=[\"mae\"],\n    verbose=1,\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, train_dataset, config)\n\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"Training time: {result.training_time:.2f}s\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#advanced-training-configuration","title":"Advanced Training Configuration","text":"<pre><code>import keras\n\n# Using string optimizer name\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    batch_size=32,\n    optimizer=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\n# Using custom optimizer instance\ncustom_optimizer = keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    clipnorm=1.0,\n)\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    batch_size=32,\n    optimizer=custom_optimizer,  # Pass optimizer instance\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\n# Using custom loss and metrics\ncustom_loss = keras.losses.Huber(delta=1.0)\ncustom_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    batch_size=32,\n    optimizer=\"adam\",\n    loss=custom_loss,  # Custom loss instance\n    metrics=[custom_metric, \"mse\"],  # Mix of instances and strings\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#callbacks-and-tensorboard","title":"Callbacks and TensorBoard","text":"<pre><code>import keras\n\n# Method 1: Pass callback instances directly\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True,\n)\n\ncsv_logger = keras.callbacks.CSVLogger(\"training.log\")\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[early_stopping, csv_logger],  # Pass instances\n    use_tensorboard=True,  # TensorBoard enabled by default\n    tensorboard_log_dir=\"logs/my_experiment\",  # Optional custom path\n)\n\n# Method 2: Pass callback configs as dicts\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[\n        {\n            \"name\": \"EarlyStopping\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"patience\": 5,\n                \"restore_best_weights\": True,\n            }\n        },\n        {\n            \"name\": \"ReduceLROnPlateau\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"factor\": 0.5,\n                \"patience\": 3,\n            }\n        },\n    ],\n    use_tensorboard=True,\n)\n\n# Method 3: Mix of instances and dicts\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[\n        early_stopping,  # Instance\n        {\"name\": \"CSVLogger\", \"params\": {\"filename\": \"training.log\"}},  # Dict\n    ],\n)\n\n# Disable TensorBoard if needed\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    use_tensorboard=False,  # Disable TensorBoard\n    callbacks=[early_stopping],\n)\n\n# Custom TensorBoard configuration\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/experiment_1\",\n    tensorboard_params={\n        \"histogram_freq\": 1,\n        \"write_graph\": True,\n        \"write_images\": True,\n        \"update_freq\": \"epoch\",\n        \"profile_batch\": \"10,20\",\n    },\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#distributed-training","title":"Distributed Training","text":"<pre><code>import tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import ModelTrainer, ModelTrainingConfig\n\n# Create distribution strategy\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Create model within strategy scope\n    model = tf.keras.Sequential([...])\n\n    # Train as usual\n    trainer = ModelTrainer()\n    result = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-evaluation","title":"Model Evaluation \ud83d\udcca","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelEvaluator, ModelEvaluationConfig\n\nconfig = ModelEvaluationConfig(\n    batch_size=32,\n    verbose=1,\n    return_dict=True,\n)\n\nevaluator = ModelEvaluator()\nresult = evaluator.evaluate(model, test_dataset, config)\n\nprint(f\"Test loss: {result.metrics['loss']:.4f}\")\nprint(f\"Test MAE: {result.metrics['mae']:.4f}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-persistence","title":"Model Persistence \ud83d\udcbe","text":""},{"location":"frameworks/tensorflow.html#saving-models","title":"Saving Models","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelPersistence\n\n# Save in TensorFlow format (recommended)\npersistence = ModelPersistence(\n    path=\"models/my_model\",\n    model=model,\n)\npersistence.save(save_format=\".keras\")\n\n# Save in H5 format\npersistence_h5 = ModelPersistence(\n    path=\"models/my_model.h5\",\n    model=model,\n)\npersistence_h5.save(save_format=\"h5\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#loading-models","title":"Loading Models","text":"<pre><code># Load full model\npersistence = ModelPersistence(\n    path=\"models/my_model\",\n    model=None,  # Will be loaded\n)\nloaded_model, metadata = persistence.load()\n\n# Load with custom objects\npersistence_custom = ModelPersistence(\n    path=\"models/my_model\",\n    model=None,\n)\nloaded_model, metadata = persistence_custom.load(\n    custom_objects={\"CustomLayer\": CustomLayer}\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-export","title":"Model Export \ud83d\udce4","text":""},{"location":"frameworks/tensorflow.html#savedmodel-format-recommended-for-serving","title":"SavedModel Format (Recommended for Serving)","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelExporter, ModelExportConfig\n\nexporter = ModelExporter()\n\nconfig = ModelExportConfig(\n    format=\"saved_model\",\n    include_optimizer=False,  # Smaller size for inference\n    signatures=None,  # Auto-detect\n)\n\nresult = exporter.export(model, \"exports/saved_model\", config)\nprint(f\"Exported to: {result.export_path}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#tflite-mobileedge-devices","title":"TFLite (Mobile/Edge Devices)","text":"<pre><code>config = ModelExportConfig(\n    format=\"tflite\",\n    optimization=\"default\",  # or \"float16\", \"int8\"\n    representative_dataset=None,  # For int8 quantization\n)\n\nresult = exporter.export(model, \"exports/model.tflite\", config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#tensorflowjs-web-deployment","title":"TensorFlow.js (Web Deployment)","text":"<pre><code>config = ModelExportConfig(\n    format=\"tfjs\",\n    quantization_dtype=\"float16\",  # Smaller model size\n)\n\nresult = exporter.export(model, \"exports/tfjs_model\", config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-inspection","title":"Model Inspection \ud83d\udd0d","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelInspector\n\ninspector = ModelInspector()\ninfo = inspector.inspect(model)\n\nprint(f\"Model name: {info.name}\")\nprint(f\"Backend: {info.backend}\")\nprint(f\"Trainable: {info.trainable}\")\nprint(f\"Total params: {info.parameters['total']}\")\nprint(f\"Trainable params: {info.parameters['trainable']}\")\n\n# Inspect inputs\nfor inp in info.inputs:\n    print(f\"Input: {inp['name']}, shape: {inp['shape']}, dtype: {inp['dtype']}\")\n\n# Inspect outputs\nfor out in info.outputs:\n    print(f\"Output: {out['name']}, shape: {out['shape']}, dtype: {out['dtype']}\")\n\n# Inspect layers\nfor layer in info.layers:\n    print(f\"Layer: {layer['name']}, type: {layer['type']}, params: {layer['params']}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#common-patterns","title":"Common Patterns \ud83c\udfaf","text":""},{"location":"frameworks/tensorflow.html#pattern-train-val-test-pipeline","title":"Pattern: Train-Val-Test Pipeline","text":"<pre><code>from mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelTrainingConfig,\n)\n\n# Load splits\ntrain_loader = CSVDataLoader(\"train.csv\", label_name=\"target\")\nval_loader = CSVDataLoader(\"val.csv\", label_name=\"target\")\ntest_loader = CSVDataLoader(\"test.csv\", label_name=\"target\")\n\ntrain_data = train_loader.load()\nval_data = val_loader.load()\ntest_data = test_loader.load()\n\n# Optimize\noptimizer = DatasetOptimizer(batch_size=32, cache=True, prefetch=True)\ntrain_data = optimizer.optimize(train_data)\nval_data = optimizer.optimize(val_data)\ntest_data = optimizer.optimize(test_data)\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n)\n\nresult = trainer.train(model, train_data, config, validation_dataset=val_data)\n\n# Evaluate\nevaluator = ModelEvaluator()\ntest_metrics = evaluator.evaluate(result.model, test_data, config)\n\nprint(f\"Best epoch: {result.best_epoch}\")\nprint(f\"Test accuracy: {test_metrics.metrics['accuracy']:.2%}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#pattern-hyperparameter-tuning","title":"Pattern: Hyperparameter Tuning","text":"<pre><code>from itertools import product\n\n# Define hyperparameters to try\nlearning_rates = [0.0001, 0.001, 0.01]\nbatch_sizes = [16, 32, 64]\noptimizers = [\"adam\", \"rmsprop\"]\n\nbest_val_loss = float('inf')\nbest_params = {}\n\nfor lr, bs, opt in product(learning_rates, batch_sizes, optimizers):\n    print(f\"\\nTrying: lr={lr}, batch_size={bs}, optimizer={opt}\")\n\n    # Create fresh model\n    model = create_model()\n\n    # Configure training\n    config = ModelTrainingConfig(\n        epochs=30,\n        learning_rate=lr,\n        batch_size=bs,\n        optimizer=opt,\n        verbose=0,\n        callbacks=[\n            {\"name\": \"EarlyStopping\", \"params\": {\"patience\": 5}},\n        ],\n    )\n\n    # Optimize dataset with new batch size\n    optimizer = DatasetOptimizer(batch_size=bs)\n    train_opt = optimizer.optimize(train_data)\n    val_opt = optimizer.optimize(val_data)\n\n    # Train\n    result = trainer.train(model, train_opt, config, validation_dataset=val_opt)\n\n    # Check if best\n    val_loss = result.history['val_loss'][-1]\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        best_params = {'lr': lr, 'batch_size': bs, 'optimizer': opt}\n        # Save best model\n        persistence.save(result.model, \"models/best_model\")\n        print(f\"\u2728 New best! Val loss: {val_loss:.4f}\")\n\nprint(f\"\\n\ud83c\udfc6 Best params: {best_params}\")\nprint(f\"\ud83c\udfc6 Best val loss: {best_val_loss:.4f}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#pattern-mixed-precision-training","title":"Pattern: Mixed Precision Training","text":"<pre><code>import tensorflow as tf\n\n# Enable mixed precision\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\n\n# Create model (automatically uses mixed precision)\nmodel = create_model()\n\n# Train as usual\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer=\"sgd\",\n)\n\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#pattern-custom-training-loop","title":"Pattern: Custom Training Loop","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelTrainer\n\nclass CustomTrainer(ModelTrainer):\n    def train_step(self, model, batch):\n        \"\"\"Custom training step logic.\"\"\"\n        features, labels = batch\n\n        with tf.GradientTape() as tape:\n            predictions = model(features, training=True)\n            loss = self.loss_fn(labels, predictions)\n\n            # Add custom regularization\n            regularization_loss = sum(model.losses)\n            total_loss = loss + regularization_loss\n\n        # Compute gradients\n        gradients = tape.gradient(total_loss, model.trainable_variables)\n\n        # Custom gradient clipping\n        gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n\n        # Apply gradients\n        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        return {'loss': loss, 'total_loss': total_loss}\n\n# Use custom trainer\ncustom_trainer = CustomTrainer()\nresult = custom_trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#integration-with-tensorflow-ecosystem","title":"Integration with TensorFlow Ecosystem \ud83d\udd27","text":""},{"location":"frameworks/tensorflow.html#tensorboard-integration","title":"TensorBoard Integration","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelTrainer, ModelTrainingConfig\n\n# TensorBoard is enabled by default\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    use_tensorboard=True,  # Default is True\n    tensorboard_log_dir=\"logs/my_experiment\",  # Optional\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\n# View results in terminal:\n# tensorboard --logdir=logs\n\n# Advanced TensorBoard configuration\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/advanced\",\n    tensorboard_params={\n        \"histogram_freq\": 1,\n        \"write_graph\": True,\n        \"write_images\": True,\n        \"update_freq\": \"epoch\",\n        \"profile_batch\": \"10,20\",\n        \"embeddings_freq\": 1,\n    },\n)\n\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#tensorflow-datasets-integration","title":"TensorFlow Datasets Integration","text":"<pre><code>import tensorflow_datasets as tfds\nfrom mlpotion.frameworks.tensorflow import DatasetOptimizer\n\n# Load from TFDS\n(train_ds, val_ds, test_ds), info = tfds.load(\n    'mnist',\n    split=['train[:80%]', 'train[80%:]', 'test'],\n    with_info=True,\n    as_supervised=True,\n)\n\n# Optimize with MLPotion\noptimizer = DatasetOptimizer(batch_size=32, cache=True, prefetch=True)\ntrain_ds = optimizer.optimize(train_ds)\n\n# Train as usual\nresult = trainer.train(model, train_ds, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#best-practices","title":"Best Practices \ud83d\udca1","text":"<ol> <li>Use Dataset Optimization: Always use <code>DatasetOptimizer</code> for better performance</li> <li>Enable Prefetching: Set <code>prefetch=True</code> to overlap data loading with training</li> <li>Cache Small Datasets: Use <code>cache=True</code> if dataset fits in memory</li> <li>Early Stopping: Enable to prevent overfitting</li> <li>SavedModel Format: Use for production deployment</li> <li>Mixed Precision: Enable for faster training on modern GPUs</li> </ol>"},{"location":"frameworks/tensorflow.html#troubleshooting","title":"Troubleshooting \ud83d\udd27","text":""},{"location":"frameworks/tensorflow.html#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Solution: Reduce batch size or use gradient accumulation</p> <pre><code>config = ModelTrainingConfig(\n    batch_size=16,  # Smaller batches\n    # Or use gradient accumulation\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#issue-slow-data-loading","title":"Issue: Slow Data Loading","text":"<p>Solution: Enable optimization and parallel reading</p> <pre><code>loader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    num_parallel_reads=8,  # More parallel readers\n)\n\noptimizer = DatasetOptimizer(\n    prefetch=True,\n    num_parallel_calls=tf.data.AUTOTUNE,\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>PyTorch Guide \u2192 - Compare with PyTorch</li> <li>ZenML Integration \u2192 - Add MLOps</li> <li>API Reference \u2192 - Detailed API docs</li> </ul> <p> TensorFlow + MLPotion = Production ML Made Easy! \ud83d\udd36 </p>"},{"location":"getting-started/concepts.html","title":"Core Concepts \ud83e\udde0","text":"<p>Understanding MLPotion's architecture will make you a more effective potion brewer. Let's dive into the key concepts!</p>"},{"location":"getting-started/concepts.html#design-philosophy","title":"Design Philosophy \ud83c\udfaf","text":"<p>MLPotion is built on a foundation of modularity, reusability, and extensibility. Our design philosophy centers on providing composable building blocks that work seamlessly across frameworks and MLOps platforms.</p>"},{"location":"getting-started/concepts.html#core-pillars","title":"Core Pillars","text":"<ol> <li>Modularity First: Small, composable, reusable building blocks</li> <li>Each component has a single, well-defined responsibility</li> <li>Components can be mixed and matched freely</li> <li>No monolithic frameworks or rigid hierarchies</li> <li> <p>Build complex workflows from simple pieces</p> </li> <li> <p>Framework Agnostic: Works with TensorFlow, PyTorch, Keras with identical APIs</p> </li> <li>Write code once, use it with any framework</li> <li>Same patterns, same interfaces, different implementations</li> <li>Framework-specific optimizations under the hood</li> <li> <p>Easy migration between frameworks</p> </li> <li> <p>Community Extensible: Easy to add new frameworks, new integrations</p> </li> <li>ZenML is just one integration - extend to Prefect, Airflow, Kubeflow, or any orchestrator</li> <li>Protocol-based design makes adding new components straightforward</li> <li>No vendor lock-in - use what works for your team</li> <li> <p>Community contributions welcome - see Contributing Guide</p> </li> <li> <p>Protocol-Based: Type-safe, flexible design using Python protocols</p> </li> <li>Interfaces over inheritance</li> <li>Duck typing with type safety</li> <li>Framework-specific types preserved</li> <li>IDE autocomplete and static type checking</li> </ol>"},{"location":"getting-started/concepts.html#the-big-picture","title":"The Big Picture \ud83d\uddbc\ufe0f","text":"<p>MLPotion provides a consistent interface across different ML frameworks:</p> <pre><code>graph TB\n    A[Your Code] --&gt; B[MLPotion Protocols]\n    B --&gt; C[TensorFlow Implementation]\n    B --&gt; D[PyTorch Implementation]\n    B --&gt; E[Keras Implementation]\n\n    C --&gt; F[tf.data.Dataset]\n    D --&gt; G[torch.DataLoader]\n    E --&gt; H[keras Dataset]\n\n    style B fill:#4a86e8,color:#fff\n    style C fill:#ff6f00,color:#fff\n    style D fill:#ee4c2c,color:#fff\n    style E fill:#d00000,color:#fff</code></pre>"},{"location":"getting-started/concepts.html#the-protocol-pattern","title":"The Protocol Pattern \ud83c\udfaf","text":"<p>At MLPotion's heart are Protocols - Python's way of saying \"I don't care what you are, just what you can do.\"</p>"},{"location":"getting-started/concepts.html#whats-a-protocol","title":"What's a Protocol?","text":"<pre><code>from typing import Protocol\n\nclass DataLoader(Protocol):\n    \"\"\"I don't care HOW you load data, just that you CAN.\"\"\"\n\n    def load(self) -&gt; Dataset:\n        \"\"\"Load and return a dataset.\"\"\"\n        ...\n</code></pre> <p>Any class with a <code>load()</code> method satisfies this protocol!</p>"},{"location":"getting-started/concepts.html#why-protocols","title":"Why Protocols?","text":"<p>Without protocols (the old way):</p> <pre><code># Rigid inheritance - eww!\nclass BaseDataLoader(ABC):\n    @abstractmethod\n    def load(self) -&gt; Dataset:\n        pass\n\nclass TFDataLoader(BaseDataLoader):  # Must inherit\n    def load(self) -&gt; tf.data.Dataset:\n        # Implementation\n        pass\n</code></pre> <p>With protocols (the MLPotion way):</p> <pre><code># Duck typing with type safety!\nclass TFDataLoader:  # No inheritance needed!\n    def load(self) -&gt; tf.data.Dataset:\n        # Implementation\n        pass\n\n# Type checker knows this works!\nloader: DataLoader = TFDataLoader()  # \u2705 Type safe!\n</code></pre> <p>Benefits:</p> <ul> <li>No forced inheritance</li> <li>Framework-specific types preserved</li> <li>IDE autocomplete works perfectly</li> <li>Mix and match components easily</li> </ul>"},{"location":"getting-started/concepts.html#cross-framework-example","title":"Cross-Framework Example \ud83d\udd04","text":"<p>See how the same pattern works across all frameworks:</p> <pre><code># TensorFlow\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\nloader = TFCSVDataLoader(file_pattern=\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()  # Returns tf.data.Dataset\ntrainer = TFModelTrainer()\nresult = trainer.train(model, dataset, config)\n\n# PyTorch\nfrom mlpotion.frameworks.pytorch import PyTorchCSVDataset, PyTorchModelTrainer\n\ndataset = PyTorchCSVDataset(file_pattern=\"data.csv\", label_name=\"target\")\nloader = DataLoader(dataset, batch_size=32)  # Returns torch.utils.data.DataLoader\ntrainer = PyTorchModelTrainer()\nresult = trainer.train(model, loader, config)\n\n# Keras\nfrom mlpotion.frameworks.keras import KerasCSVDataLoader, KerasModelTrainer\n\nloader = KerasCSVDataLoader(file_pattern=\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()  # Returns keras-compatible dataset\ntrainer = KerasModelTrainer()\nresult = trainer.train(model, dataset, config)\n</code></pre> <p>Same concepts, same API, different frameworks!</p>"},{"location":"getting-started/concepts.html#extending-mlpotion","title":"Extending MLPotion \ud83d\udd27","text":"<p>MLPotion is designed for community extensibility:</p>"},{"location":"getting-started/concepts.html#adding-new-integrations","title":"Adding New Integrations","text":"<p>Want to integrate with Prefect, Airflow, or Kubeflow? Just wrap MLPotion components:</p> <pre><code># Example: Creating Prefect tasks\nfrom prefect import task\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\n@task\ndef load_data_task(file_pattern: str):\n    loader = TFCSVDataLoader(file_pattern=file_pattern, ...)\n    return loader.load()\n\n@task\ndef train_model_task(model, dataset, config):\n    trainer = TFModelTrainer()\n    return trainer.train(model, dataset, config)\n\n# Use in Prefect flow\nfrom prefect import flow\n\n@flow\ndef ml_flow():\n    dataset = load_data_task(\"data.csv\")\n    result = train_model_task(model, dataset, config)\n    return result\n</code></pre>"},{"location":"getting-started/concepts.html#adding-new-frameworks","title":"Adding New Frameworks","text":"<p>Want to add support for JAX, MXNet, or another framework? Implement the protocols:</p> <pre><code>from mlpotion.core.protocols import DataLoader, ModelTrainer\n\nclass JAXDataLoader:\n    \"\"\"Implements DataLoader protocol for JAX.\"\"\"\n    def load(self) -&gt; jax.Array:\n        # Your JAX-specific implementation\n        return jax_array\n\nclass JAXModelTrainer:\n    \"\"\"Implements ModelTrainer protocol for JAX.\"\"\"\n    def train(self, model, dataset, config, validation_dataset=None):\n        # Your JAX-specific training logic\n        return TrainingResult(...)\n</code></pre> <p>See Contributing Guide to add your integration!</p>"},{"location":"getting-started/concepts.html#core-components","title":"Core Components \ud83e\udde9","text":"<p>MLPotion has 6 main component types:</p>"},{"location":"getting-started/concepts.html#1-data-loaders","title":"1. Data Loaders \ud83d\udce5","text":"<p>Purpose: Load data from various sources into framework-specific formats.</p> <p>Protocol:</p> <pre><code>class DataLoader(Protocol[DatasetT]):\n    def load(self) -&gt; DatasetT:\n        \"\"\"Load data and return a dataset.\"\"\"\n        ...\n</code></pre> <p>Implementations:</p> <pre><code># TensorFlow\nTFCSVDataLoader \u2192 tf.data.Dataset\nTFParquetLoader \u2192 tf.data.Dataset\n\n# PyTorch\nPyTorchCSVDataset \u2192 torch.utils.data.Dataset\nPyTorchDataLoaderFactory \u2192 torch.utils.data.DataLoader\n\n# Keras\nKerasCSVDataLoader \u2192 keras dataset\n</code></pre> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFCSVDataLoader\n\nloader = TFCSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n)\ndataset = loader.load()  # Returns tf.data.Dataset\n</code></pre>"},{"location":"getting-started/concepts.html#2-dataset-optimizers","title":"2. Dataset Optimizers \u26a1","text":"<p>Purpose: Optimize datasets for training/inference performance.</p> <p>Protocol:</p> <pre><code>class DatasetOptimizer(Protocol[DatasetT]):\n    def optimize(self, dataset: DatasetT) -&gt; DatasetT:\n        \"\"\"Optimize dataset for performance.\"\"\"\n        ...\n</code></pre> <p>What they do:</p> <ul> <li>Batching</li> <li>Caching</li> <li>Prefetching</li> <li>Shuffling</li> <li>Parallelization</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFDatasetOptimizer\n\noptimizer = TFDatasetOptimizer(\n    batch_size=32,\n    shuffle_buffer_size=1000,\n    cache=True,\n    prefetch=True,\n)\n\noptimized_dataset = optimizer.optimize(dataset)\n</code></pre>"},{"location":"getting-started/concepts.html#3-model-trainers","title":"3. Model Trainers \ud83c\udf93","text":"<p>Purpose: Train models with a consistent interface.</p> <p>Protocol:</p> <pre><code>class ModelTrainer(Protocol[ModelT, DatasetT]):\n    def train(\n        self,\n        model: ModelT,\n        dataset: DatasetT,\n        config: TrainingConfig,\n        validation_dataset: DatasetT | None = None,\n    ) -&gt; TrainingResult[ModelT]:\n        \"\"\"Train a model and return results.\"\"\"\n        ...\n</code></pre> <p>What they handle:</p> <ul> <li>Model compilation</li> <li>Training loop</li> <li>Validation</li> <li>Callbacks</li> <li>History tracking</li> <li>Early stopping</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelTrainer, TFTrainingConfig\n\ntrainer = TFModelTrainer()\nconfig = TFTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    early_stopping=True,\n)\n\nresult = trainer.train(model, dataset, config)\nprint(f\"Final loss: {result.metrics['loss']}\")\n</code></pre>"},{"location":"getting-started/concepts.html#4-model-evaluators","title":"4. Model Evaluators \ud83d\udcca","text":"<p>Purpose: Evaluate trained models consistently.</p> <p>Protocol:</p> <pre><code>class ModelEvaluator(Protocol[ModelT, DatasetT]):\n    def evaluate(\n        self,\n        model: ModelT,\n        dataset: DatasetT,\n        config: EvaluationConfig,\n    ) -&gt; EvaluationResult:\n        \"\"\"Evaluate model and return metrics.\"\"\"\n        ...\n</code></pre> <p>What they provide:</p> <ul> <li>Metric computation</li> <li>Performance analysis</li> <li>Result objects</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelEvaluator\n\nevaluator = TFModelEvaluator()\nresult = evaluator.evaluate(model, test_dataset, config)\nprint(f\"Test accuracy: {result.metrics['accuracy']:.2%}\")\n</code></pre>"},{"location":"getting-started/concepts.html#5-model-persistence","title":"5. Model Persistence \ud83d\udcbe","text":"<p>Purpose: Save and load models reliably.</p> <p>Protocol:</p> <pre><code>class ModelPersistence(Protocol[ModelT]):\n    def save(\n        self,\n        model: ModelT,\n        path: str,\n        **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Save model to disk.\"\"\"\n        ...\n\n    def load(self, path: str, **kwargs: Any) -&gt; tuple[ModelT, dict[str, Any]]:\n        \"\"\"Load model from disk.\n\n        Returns:\n            Tuple of (loaded model, inspection metadata)\n        \"\"\"\n        ...\n</code></pre> <p>What they handle:</p> <ul> <li>Model serialization</li> <li>Metadata preservation</li> <li>Model inspection on load</li> <li>Version compatibility</li> <li>Path management</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelPersistence\n\npersistence = TFModelPersistence(path=\"models/my_model\", model=model)\n# Save\npersistence.save()\n\n# Load - returns tuple of (model, metadata)\nloader = TFModelPersistence(path=\"models/my_model\")\nloaded_model, metadata = loader.load()\nprint(f\"Model inputs: {metadata['inputs']}\")\nprint(f\"Model outputs: {metadata['outputs']}\")\n</code></pre>"},{"location":"getting-started/concepts.html#6-model-exporters","title":"6. Model Exporters \ud83d\udce4","text":"<p>Purpose: Export models for production deployment.</p> <p>Protocol:</p> <pre><code>class ModelExporter(Protocol[ModelT]):\n    def export(\n        self,\n        model: ModelT,\n        export_path: str,\n        config: ExportConfig,\n    ) -&gt; ExportResult:\n        \"\"\"Export model for serving.\"\"\"\n        ...\n</code></pre> <p>Export formats:</p> <ul> <li>TensorFlow: SavedModel, TFLite, TF.js</li> <li>PyTorch: TorchScript, ONNX</li> <li>Keras: Keras format, ONNX</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelExporter, TFExportConfig\n\nexporter = TFModelExporter()\nconfig = TFExportConfig(format=\"saved_model\")\n\nresult = exporter.export(model, \"exports/model\", config)\nprint(f\"Exported to: {result.export_path}\")\n</code></pre>"},{"location":"getting-started/concepts.html#configuration-objects","title":"Configuration Objects \u2699\ufe0f","text":"<p>MLPotion uses Pydantic-based config objects for type safety and validation.</p>"},{"location":"getting-started/concepts.html#training-configuration","title":"Training Configuration","text":"<pre><code>from mlpotion.frameworks.tensorflow import TFTrainingConfig\n\nconfig = TFTrainingConfig(\n    # Required\n    epochs=10,\n    learning_rate=0.001,\n\n    # Optimizer\n    optimizer_type=\"adam\",  # or \"sgd\", \"rmsprop\", etc.\n    optimizer_kwargs={\"beta_1\": 0.9, \"beta_2\": 0.999\"},\n\n    # Loss and metrics\n    loss=\"mse\",  # or custom loss\n    metrics=[\"mae\", \"mse\"],\n\n    # Training behavior\n    batch_size=32,\n    validation_split=0.2,\n    shuffle=True,\n    verbose=1,\n\n    # Callbacks\n    early_stopping=True,\n    early_stopping_patience=10,\n    early_stopping_monitor=\"val_loss\",\n\n    # Checkpointing\n    save_best_only=True,\n    checkpoint_monitor=\"val_loss\",\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Type checking at creation time</li> <li>Validation before training</li> <li>IDE autocomplete for all options</li> <li>Serializable for reproducibility</li> </ul>"},{"location":"getting-started/concepts.html#result-objects","title":"Result Objects \ud83d\udccb","text":"<p>All operations return rich result objects:</p>"},{"location":"getting-started/concepts.html#trainingresult","title":"TrainingResult","text":"<pre><code>@dataclass\nclass TrainingResult(Generic[ModelT]):\n    model: ModelT                           # Trained model\n    history: dict[str, list[float]]         # Training history\n    metrics: dict[str, float]               # Final metrics\n    config: TrainingConfig                  # Configuration used\n    training_time: float | None             # Training duration\n    best_epoch: int | None                  # Best epoch (if early stopping)\n\n# Usage\nresult = trainer.train(...)\nprint(f\"Loss: {result.metrics['loss']}\")\nprint(f\"Training time: {result.training_time}s\")\nprint(f\"Best epoch: {result.best_epoch}\")\n\n# Access history\nlosses = result.history['loss']\nval_losses = result.history['val_loss']\n</code></pre>"},{"location":"getting-started/concepts.html#evaluationresult","title":"EvaluationResult","text":"<pre><code>@dataclass\nclass EvaluationResult:\n    metrics: dict[str, float]              # Evaluation metrics\n    config: EvaluationConfig               # Configuration used\n    evaluation_time: float | None          # Evaluation duration\n\n# Usage\nresult = evaluator.evaluate(...)\naccuracy = result.get_metric('accuracy')\n</code></pre>"},{"location":"getting-started/concepts.html#exportresult","title":"ExportResult","text":"<pre><code>@dataclass\nclass ExportResult:\n    export_path: str                       # Where model was saved\n    format: str                            # Export format used\n    config: ExportConfig                   # Configuration used\n    metadata: dict[str, Any]               # Additional info\n\n# Usage\nresult = exporter.export(...)\nprint(f\"Model saved to: {result.export_path}\")\nprint(f\"Format: {result.format}\")\n</code></pre>"},{"location":"getting-started/concepts.html#type-safety","title":"Type Safety \ud83d\udee1\ufe0f","text":"<p>MLPotion uses Python 3.10+ type hints for maximum safety:</p>"},{"location":"getting-started/concepts.html#generic-types","title":"Generic Types","text":"<pre><code>from typing import TypeVar, Generic\n\nModelT = TypeVar(\"ModelT\")\nDatasetT = TypeVar(\"DatasetT\")\n\nclass Trainer(Generic[ModelT, DatasetT]):\n    def train(\n        self,\n        model: ModelT,\n        dataset: DatasetT,\n    ) -&gt; TrainingResult[ModelT]:\n        ...\n</code></pre> <p>What this means:</p> <pre><code># TensorFlow\ntf_trainer: Trainer[tf.keras.Model, tf.data.Dataset]\ntf_result: TrainingResult[tf.keras.Model]\n\n# PyTorch\ntorch_trainer: Trainer[nn.Module, DataLoader]\ntorch_result: TrainingResult[nn.Module]\n</code></pre> <p>Your IDE knows the exact types!</p>"},{"location":"getting-started/concepts.html#runtime-checking","title":"Runtime Checking","text":"<pre><code>from mlpotion.core.protocols import DataLoader\n\n# This is checked at runtime!\nif isinstance(my_loader, DataLoader):\n    dataset = my_loader.load()\n</code></pre>"},{"location":"getting-started/concepts.html#error-handling","title":"Error Handling \ud83d\udea8","text":"<p>MLPotion has a consistent exception hierarchy:</p> <pre><code>MLPotionError                    # Base exception\n\u251c\u2500\u2500 DataLoadingError            # Data loading issues\n\u251c\u2500\u2500 TrainingError               # Training failures\n\u251c\u2500\u2500 EvaluationError             # Evaluation problems\n\u251c\u2500\u2500 ExportError                 # Export issues\n\u2514\u2500\u2500 ConfigurationError          # Invalid configuration\n</code></pre> <p>Usage:</p> <pre><code>from mlpotion.core import DataLoadingError, TrainingError\n\ntry:\n    dataset = loader.load()\n    result = trainer.train(model, dataset, config)\nexcept DataLoadingError as e:\n    print(f\"Failed to load data: {e}\")\nexcept TrainingError as e:\n    print(f\"Training failed: {e}\")\n</code></pre>"},{"location":"getting-started/concepts.html#framework-detection","title":"Framework Detection \ud83d\udd0d","text":"<p>MLPotion auto-detects available frameworks:</p> <pre><code>from mlpotion.utils import is_framework_available, get_available_frameworks\n\n# Check if a framework is available\nif is_framework_available(\"tensorflow\"):\n    from mlpotion.frameworks.tensorflow import TFCSVDataLoader\n    # Use TensorFlow components\n\n# Get all available frameworks\nframeworks = get_available_frameworks()\nprint(f\"Available: {frameworks}\")  # ['tensorflow', 'torch']\n</code></pre>"},{"location":"getting-started/concepts.html#composition-pattern","title":"Composition Pattern \ud83d\udd17","text":"<p>MLPotion components compose naturally:</p> <pre><code># Load data\nloader = TFCSVDataLoader(...)\ndataset = loader.load()\n\n# Optimize\noptimizer = TFDatasetOptimizer(...)\ndataset = optimizer.optimize(dataset)\n\n# Train\ntrainer = TFModelTrainer()\nresult = trainer.train(model, dataset, config)\n\n# Evaluate\nevaluator = TFModelEvaluator()\nmetrics = evaluator.evaluate(result.model, dataset, config)\n\n# Export\nexporter = TFModelExporter()\nexport_result = exporter.export(result.model, \"path/\", export_config)\n</code></pre> <p>Each component does one thing well, and they work together seamlessly!</p>"},{"location":"getting-started/concepts.html#zenml-integration","title":"ZenML Integration \ud83d\udd04","text":"<p>MLPotion provides ready-to-use ZenML steps for seamless MLOps integration:</p> <pre><code>from zenml import pipeline\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,\n    optimize_data,\n    train_model,\n    evaluate_model,\n    export_model,\n)\n\n@pipeline\ndef ml_pipeline():\n    # Load data into tf.data.Dataset\n    dataset = load_data(\n        file_path=\"data.csv\",\n        batch_size=32,\n        label_name=\"target\",\n    )\n\n    # Optimize dataset for training\n    dataset = optimize_data(\n        dataset=dataset,\n        prefetch=True,\n        cache=True,\n    )\n\n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        dataset=dataset,\n        epochs=10,\n        learning_rate=0.001,\n    )\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=trained_model,\n        dataset=dataset,\n    )\n\n    # Export for serving\n    export_path = export_model(\n        model=trained_model,\n        export_path=\"models/my_model\",\n        export_format=\"keras\",\n    )\n\n    return metrics, export_path\n</code></pre> <p>What you get:</p> <ul> <li>Automatic artifact tracking</li> <li>Pipeline reproducibility</li> <li>Version control for models and datasets</li> <li>Caching of unchanged steps</li> <li>Full lineage tracking</li> </ul> <p>Important: ZenML is just one integration! You can extend MLPotion to work with Prefect, Airflow, Kubeflow, or any other orchestrator. See Extending MLPotion above.</p>"},{"location":"getting-started/concepts.html#design-patterns","title":"Design Patterns \ud83c\udfa8","text":""},{"location":"getting-started/concepts.html#strategy-pattern","title":"Strategy Pattern","text":"<p>Different implementations of the same protocol:</p> <pre><code># All implement DataLoader protocol\nloaders = [\n    TFCSVDataLoader(...),\n    TFParquetLoader(...),\n    TFBigQueryLoader(...),\n]\n\n# Use any of them interchangeably\nfor loader in loaders:\n    dataset = loader.load()  # Works with all!\n</code></pre>"},{"location":"getting-started/concepts.html#factory-pattern","title":"Factory Pattern","text":"<p>Create complex objects easily:</p> <pre><code>from mlpotion.frameworks.pytorch import PyTorchDataLoaderFactory\n\nfactory = PyTorchDataLoaderFactory(\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n)\n\n# Create dataloaders for different datasets\ntrain_loader = factory.load(train_dataset)\nval_loader = factory.load(val_dataset)\ntest_loader = factory.load(test_dataset)\n</code></pre>"},{"location":"getting-started/concepts.html#builder-pattern","title":"Builder Pattern","text":"<p>Complex configuration:</p> <pre><code>config = (TFTrainingConfig()\n    .with_epochs(10)\n    .with_learning_rate(0.001)\n    .with_early_stopping(patience=5)\n    .with_checkpointing(save_best_only=True)\n    .build())\n</code></pre> <p>(Note: This is conceptual; actual API uses direct construction)</p>"},{"location":"getting-started/concepts.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<p>Now that you understand the concepts:</p> <ol> <li>TensorFlow Guide \u2192 - TensorFlow-specific details</li> <li>PyTorch Guide \u2192 - PyTorch-specific details</li> <li>Keras Guide \u2192 - Keras-specific details</li> <li>ZenML Integration \u2192 - Add MLOps powers</li> </ol>"},{"location":"getting-started/concepts.html#key-takeaways","title":"Key Takeaways \ud83d\udcdd","text":"<ul> <li>Protocols &gt; Inheritance: Flexible, type-safe interfaces</li> <li>Composition &gt; Configuration: Build by combining components</li> <li>Type Safety: Catch errors early with mypy</li> <li>Consistency: Same patterns across all frameworks</li> <li>Simplicity: Each component does one thing well</li> </ul> <p> Concepts mastered! Time to dive deeper! \ud83e\uddd9\u200d\u2642\ufe0f </p>"},{"location":"getting-started/installation.html","title":"Installation Guide \ud83d\udce5","text":"<p>Getting MLPotion installed is easier than pronouncing \"scikit-learn\" correctly! Let's get you set up.</p>"},{"location":"getting-started/installation.html#tldr-just-tell-me-what-to-run","title":"TL;DR - Just Tell Me What to Run! \ud83c\udfc3\u200d\u2642\ufe0f","text":"<pre><code># Most common setup (using Poetry)\npoetry add mlpotion -E tensorflow\n\n# ... or using pip\npip install \"mlpotion[tensorflow]\"\n\n# Or for PyTorch lovers\npoetry add mlpotion -E pytorch\n\n# Or if you can't decide (we don't judge)\npoetry add mlpotion -E all\n</code></pre> <p>Done! Jump to Quick Start to start brewing.</p>"},{"location":"getting-started/installation.html#requirements","title":"Requirements \ud83d\udccb","text":"<p>Before you start, make sure you have:</p> <ul> <li>Python &gt;3.10 (we live on the edge, but not too close to it)</li> <li>pip (you probably have this already)</li> <li>A sense of adventure (optional, but recommended)</li> </ul> <p>Python Version</p> <p>MLPotion requires Python 3.10 or 3.11. Why? Because we use modern type hints that make your IDE actually helpful! Python 3.12 support is coming soon.</p>"},{"location":"getting-started/installation.html#installation-options","title":"Installation Options \ud83c\udfaf","text":"<p>MLPotion follows a \"bring your own framework\" philosophy. You only install what you need!</p>"},{"location":"getting-started/installation.html#option-1-core-only-framework-agnostic","title":"Option 1: Core Only (Framework Agnostic) \ud83c\udf1f","text":"<p>Install just the core without any ML frameworks:</p> <pre><code>poetry add mlpotion\n</code></pre> <p>Use this when:</p> <ul> <li>You want to explore the package structure</li> <li>You're installing on systems without ML frameworks</li> <li>You're writing framework-agnostic code</li> <li>You're a minimalist at heart</li> </ul> <p>What you get:</p> <ul> <li>Core protocols and interfaces</li> <li>Result types and configurations</li> <li>Utility functions</li> <li>Type stubs for IDE support</li> </ul> <p>What you don't get:</p> <ul> <li>Actual ML framework implementations (obviously!)</li> <li>Data loaders (they need frameworks)</li> <li>Training components (ditto)</li> </ul>"},{"location":"getting-started/installation.html#option-2-with-tensorflow","title":"Option 2: With TensorFlow \ud83d\udd36","text":"<p>The production workhorse setup:</p> <pre><code>poetry add mlpotion -E tensorflow\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>TensorFlow 2.15+</li> <li>Keras 3.0+ (automatically included with TensorFlow)</li> <li>All TensorFlow-specific components</li> </ul> <p>Perfect for:</p> <ul> <li>Production deployments</li> <li>TensorFlow ecosystem users</li> <li>Google Cloud Platform projects</li> <li>When you need tf.data.Dataset optimization</li> </ul>"},{"location":"getting-started/installation.html#option-3-with-pytorch","title":"Option 3: With PyTorch \ud83d\udd25","text":"<p>The researcher's choice:</p> <pre><code>poetry add mlpotion -E pytorch\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>PyTorch 2.0+</li> <li>TorchVision (for image processing)</li> <li>All PyTorch-specific components</li> </ul> <p>Perfect for:</p> <ul> <li>Research projects</li> <li>Academic work</li> <li>When you love <code>nn.Module</code></li> <li>Dynamic computation graphs</li> </ul>"},{"location":"getting-started/installation.html#option-4-with-keras","title":"Option 4: With Keras \ud83c\udfa8","text":"<p>The friendly, backend-agnostic option:</p> <pre><code>poetry add mlpotion -E keras\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>Keras 3.0+ (standalone)</li> <li>Keras-specific components</li> </ul> <p>Perfect for:</p> <ul> <li>Quick prototyping</li> <li>When you want to switch backends later</li> <li>Teaching and learning</li> <li>Keras fans (obviously!)</li> </ul>"},{"location":"getting-started/installation.html#option-5-everything","title":"Option 5: Everything! \ud83c\udf89","text":"<p>When you can't choose or need it all:</p> <pre><code>poetry add mlpotion -E all\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>TensorFlow 2.15+</li> <li>PyTorch 2.0+ with TorchVision</li> <li>Keras 3.0+</li> <li>All framework-specific components</li> </ul> <p>Warning:</p> <p>This will install a lot of dependencies (~3GB). Your disk space might need therapy afterward.</p>"},{"location":"getting-started/installation.html#option-6-with-zenml-integration","title":"Option 6: With ZenML Integration \ud83d\udd04","text":"<p>For the MLOps enthusiasts:</p> <pre><code># TensorFlow + ZenML\npoetry add mlpotion -E tensorflow -E zenml\n\n# PyTorch + ZenML\npoetry add mlpotion -E pytorch -E zenml\n\n# Everything + ZenML (bold choice!)\npoetry add mlpotion -E all -E zenml\n</code></pre> <p>What you get extra:</p> <ul> <li>ZenML integration components</li> <li>Pre-built pipeline steps (\u267b\ufe0f REUSABLE)</li> <li>Custom materializers</li> <li>ZenML-specific utilities</li> </ul>"},{"location":"getting-started/installation.html#installing-from-source","title":"Installing from Source \ud83d\udee0\ufe0f","text":"<p>Want the bleeding edge or contributing? Clone and install:</p> <pre><code># Clone the repository\ngit clone https://github.com/UnicoLab/MLPotion.git\ncd MLPotion\n\n# Install in development mode\npip install -e .\n\n# Or with extras\npip install -e \".[tensorflow]\"\npip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation.html#using-poetry","title":"Using Poetry \ud83d\udce6","text":"<p>We use Poetry for dependency management. If you prefer Poetry:</p> <pre><code># Clone the repo\ngit clone https://github.com/UnicoLab/MLPotion.git\ncd MLPotion\n\n# Install with Poetry\npoetry install\n\n# Or with extras\npoetry install -E tensorflow\npoetry install -E pytorch\npoetry install -E all\n</code></pre>"},{"location":"getting-started/installation.html#virtual-environments-highly-recommended","title":"Virtual Environments (Highly Recommended!) \ud83e\uddca","text":"<p>Don't pollute your global Python! Use virtual environments:</p>"},{"location":"getting-started/installation.html#using-venv","title":"Using venv","text":"<pre><code># Create virtual environment\npython -m venv mlpotion-env\n\n# Activate (macOS/Linux)\nsource mlpotion-env/bin/activate\n\n# Activate (Windows)\nmlpotion-env\\Scripts\\activate\n\n# Install MLPotion\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#using-conda","title":"Using conda","text":"<pre><code># Create conda environment\nconda create -n mlpotion python=3.10\n\n# Activate\nconda activate mlpotion\n\n# Install MLPotion\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#verifying-installation","title":"Verifying Installation \u2705","text":"<p>Let's make sure everything works:</p> <pre><code># Test core installation\nimport mlpotion\nprint(f\"MLPotion version: {mlpotion.__version__}\")\n\n# Check available frameworks\nfrom mlpotion.utils import get_available_frameworks\nprint(f\"Available frameworks: {get_available_frameworks()}\")\n</code></pre> <p>Expected output:</p> <pre><code>MLPotion version: 0.1.0\nAvailable frameworks: ['tensorflow', 'torch']  # Depends on what you installed\n</code></pre>"},{"location":"getting-started/installation.html#framework-specific-tests","title":"Framework-Specific Tests","text":""},{"location":"getting-started/installation.html#tensorflow","title":"TensorFlow","text":"<pre><code>from mlpotion.frameworks.tensorflow import TFCSVDataLoader\nprint(\"\u2705 TensorFlow support is working!\")\n</code></pre>"},{"location":"getting-started/installation.html#pytorch","title":"PyTorch","text":"<pre><code>from mlpotion.frameworks.pytorch import PyTorchCSVDataset\nprint(\"\u2705 PyTorch support is working!\")\n</code></pre>"},{"location":"getting-started/installation.html#keras","title":"Keras","text":"<pre><code>from mlpotion.frameworks.keras import KerasCSVDataLoader\nprint(\"\u2705 Keras support is working!\")\n</code></pre>"},{"location":"getting-started/installation.html#common-installation-issues","title":"Common Installation Issues \ud83d\udd27","text":""},{"location":"getting-started/installation.html#issue-tensorflow-not-installing-on-m1m2-macs","title":"Issue: TensorFlow not installing on M1/M2 Macs","text":"<p>Problem: Apple Silicon can be picky about TensorFlow.</p> <p>Solution:</p> <pre><code># Use conda for M1/M2 Macs\nconda create -n mlpotion python=3.10\nconda activate mlpotion\nconda install -c apple tensorflow-deps\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#issue-pytorch-cuda-version-mismatch","title":"Issue: PyTorch CUDA version mismatch","text":"<p>Problem: PyTorch CUDA version doesn't match your GPU.</p> <p>Solution: Install PyTorch first with the correct CUDA version:</p> <pre><code># Check your CUDA version first\nnvidia-smi\n\n# Install PyTorch with specific CUDA version\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# Then install MLPotion core only\npip install mlpotion\n</code></pre>"},{"location":"getting-started/installation.html#issue-conflicting-dependencies","title":"Issue: Conflicting dependencies","text":"<p>Problem: Package version conflicts.</p> <p>Solution: Use a fresh virtual environment:</p> <pre><code># Remove old environment\nrm -rf venv\n\n# Create fresh one\npython -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#issue-import-errors-after-installation","title":"Issue: Import errors after installation","text":"<p>Problem: Python can't find the package.</p> <p>Solution:</p> <pre><code># Check where Python looks for packages\nimport sys\nprint(sys.path)\n\n# Verify installation location\npip show mlpotion\n</code></pre> <p>If they don't match, you might have multiple Python installations. Use <code>python -m pip</code> instead of <code>pip</code>.</p>"},{"location":"getting-started/installation.html#upgrading-mlpotion","title":"Upgrading MLPotion \ud83d\udd04","text":"<p>Keep your potion fresh:</p> <pre><code># Upgrade to latest version\npip install --upgrade mlpotion[tensorflow]\n\n# Or force reinstall everything\npip install --force-reinstall mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#uninstalling","title":"Uninstalling \ud83d\uddd1\ufe0f","text":"<p>Sad to see you go, but here's how:</p> <pre><code># Uninstall MLPotion\npip uninstall mlpotion\n\n# Remove the frameworks too (if you want)\npip uninstall tensorflow torch keras\n</code></pre>"},{"location":"getting-started/installation.html#docker-setup","title":"Docker Setup \ud83d\udc33","text":"<p>Prefer containers? We got you:</p> <pre><code>FROM python:3.10-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install MLPotion with TensorFlow\nRUN pip install --no-cache-dir mlpotion[tensorflow]\n\n# Verify installation\nRUN python -c \"import mlpotion; print(mlpotion.__version__)\"\n\nWORKDIR /app\nCMD [\"python\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t mlpotion:latest .\ndocker run -it mlpotion:latest python\n</code></pre>"},{"location":"getting-started/installation.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<p>Installation complete! Now what?</p> <ol> <li>Quick Start \u2192 - Build your first pipeline in 5 minutes</li> <li>Core Concepts \u2192 - Understand the architecture</li> <li>Framework Guides \u2192 - Deep dive into your framework</li> </ol>"},{"location":"getting-started/installation.html#need-help","title":"Need Help? \ud83c\udd98","text":"<ul> <li>Check the FAQ for common questions</li> <li>Open an issue on GitHub</li> <li>Join our community discussions</li> </ul> <p> Installation successful? Time to brew some magic! \ud83e\uddea\u2728 </p>"},{"location":"getting-started/quickstart.html","title":"Quick Start \u26a1","text":"<p>Welcome to the fastest way to get productive with MLPotion! In 5 minutes, you'll have your first ML pipeline running.</p>"},{"location":"getting-started/quickstart.html#your-first-pipeline-in-5-minutes","title":"Your First Pipeline in 5 Minutes \u23f1\ufe0f","text":"<p>Let's build a simple regression pipeline with TensorFlow. Same principles apply to PyTorch and Keras!</p>"},{"location":"getting-started/quickstart.html#step-1-install-mlpotion-30-seconds","title":"Step 1: Install MLPotion (30 seconds)","text":"<pre><code>poetry add mlpotion -E tensorflow\n# OR\npip install \"mlpotion[tensorflow]\"\n</code></pre>"},{"location":"getting-started/quickstart.html#step-2-prepare-your-data-1-minute","title":"Step 2: Prepare Your Data (1 minute)","text":"<p>Create a simple CSV file or use your own:</p> <pre><code># create_data.py\nimport pandas as pd\nimport numpy as np\n\n# Generate synthetic data\nnp.random.seed(42)\nn_samples = 1000\n\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(n_samples),\n    'feature_2': np.random.randn(n_samples),\n    'feature_3': np.random.randn(n_samples),\n    'target': np.random.randn(n_samples)\n})\n\ndata.to_csv('data.csv', index=False)\nprint(\"\u2705 Data created!\")\n</code></pre> <p>Run it:</p> <pre><code>python create_data.py\n</code></pre>"},{"location":"getting-started/quickstart.html#step-3-load-and-explore-data-1-minute","title":"Step 3: Load and Explore Data (1 minute)","text":"<pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\n# Create a data loader\nloader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    shuffle=True,\n)\n\n# Load the dataset\ndataset = loader.load()\nprint(\"\u2705 Data loaded!\")\n\n# Peek at the data\nfor batch in dataset.take(1):\n    features, labels = batch\n    print(f\"Features shape: {features.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n</code></pre> <p>Output:</p> <pre><code>\u2705 Data loaded!\nFeatures shape: (32, 3)\nLabels shape: (32,)\n</code></pre>"},{"location":"getting-started/quickstart.html#step-4-create-and-train-a-model-2-minutes","title":"Step 4: Create and Train a Model (2 minutes)","text":"<pre><code>import tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import (\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n# Create a simple model\ndef create_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n    return model\n\nmodel = create_model()\n\n# Configure training\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n    verbose=1,\n)\n\n# Train the model\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"\u2705 Training complete!\")\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"Final MAE: {result.metrics['mae']:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart.html#step-5-evaluate-and-save-1-minute","title":"Step 5: Evaluate and Save (1 minute)","text":"<pre><code>from mlpotion.frameworks.tensorflow import (\n    ModelEvaluator,\n    ModelPersistence,\n)\n\n# Evaluate the model\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(result.model, dataset, config)\n\nprint(f\"\u2705 Evaluation complete!\")\nprint(f\"Test MAE: {eval_result.metrics['mae']:.4f}\")\n\n# Save the model\npersistence = ModelPersistence(\n    path=\"my_model\",\n    model=result.model,\n)\npersistence.save()\n\nprint(\"\u2705 Model saved to 'my_model' directory!\")\n</code></pre>"},{"location":"getting-started/quickstart.html#complete-script","title":"Complete Script","text":"<p>Here's everything together:</p> <pre><code>\"\"\"quickstart_example.py - Your first MLPotion pipeline!\"\"\"\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainingConfig,\n)\n\n# 1. Create sample data\nprint(\"\ud83d\udcca Creating sample data...\")\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'target': np.random.randn(1000)\n})\ndata.to_csv('data.csv', index=False)\n\n# 2. Load data\nprint(\"\ud83d\udce5 Loading data...\")\nloader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    shuffle=True,\n)\ndataset = loader.load()\n\n# 3. Create model\nprint(\"\ud83c\udfd7\ufe0f Creating model...\")\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\n# 4. Train model\nprint(\"\ud83c\udf93 Training model...\")\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"\\n\u2705 Training complete!\")\nprint(f\"   Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"   Final MAE: {result.metrics['mae']:.4f}\")\n\n# 5. Evaluate\nprint(\"\\n\ud83d\udcca Evaluating model...\")\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(result.model, dataset, config)\n\nprint(f\"\u2705 Evaluation complete!\")\nprint(f\"   Test MAE: {eval_result.metrics['mae']:.4f}\")\n\n# 6. Save model\nprint(\"\\n\ud83d\udcbe Saving model...\")\npersistence = ModelPersistence(\n    path=\"my_model\",\n    model=result.model,\n)\npersistence.save()\n\nprint(\"\\n\ud83c\udf89 All done! Your first MLPotion pipeline is complete!\")\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_example.py\n</code></pre>"},{"location":"getting-started/quickstart.html#quick-start-with-pytorch","title":"Quick Start with PyTorch \ud83d\udd25","text":"<p>Prefer PyTorch? Here's the same pipeline:</p> <pre><code>\"\"\"quickstart_pytorch.py - PyTorch version\"\"\"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom mlpotion.frameworks.pytorch import (\n    CSVDataset,\n    CSVDataLoader,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelTrainingConfig,\n)\n\n# 1. Create sample data\nprint(\"\ud83d\udcca Creating sample data...\")\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'target': np.random.randn(1000)\n})\ndata.to_csv('data.csv', index=False)\n\n# 2. Load data\nprint(\"\ud83d\udce5 Loading data...\")\ndataset = CSVDataset(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n)\n\nfactory = CSVDataLoader(batch_size=32, shuffle=True)\ndataloader = factory.load(dataset)\n\n# 3. Create model\nprint(\"\ud83c\udfd7\ufe0f Creating model...\")\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(3, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nmodel = SimpleModel()\n\n# 4. Train model\nprint(\"\ud83c\udf93 Training model...\")\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataloader, config)\n\nprint(f\"\\n\u2705 Training complete!\")\nprint(f\"   Final loss: {result.metrics['loss']:.4f}\")\n\n# 5. Evaluate\nprint(\"\\n\ud83d\udcca Evaluating model...\")\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(result.model, dataloader, config)\n\nprint(f\"\u2705 Evaluation complete!\")\nprint(f\"   Test loss: {eval_result.metrics['loss']:.4f}\")\n\nprint(\"\\n\ud83c\udf89 PyTorch pipeline complete!\")\n</code></pre>"},{"location":"getting-started/quickstart.html#quick-start-with-keras","title":"Quick Start with Keras \ud83c\udfa8","text":"<p>And the Keras version:</p> <pre><code>\"\"\"quickstart_keras.py - Keras version\"\"\"\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelTrainingConfig,\n)\n\n# 1. Create sample data\nprint(\"\ud83d\udcca Creating sample data...\")\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'target': np.random.randn(1000)\n})\ndata.to_csv('data.csv', index=False)\n\n# 2. Load data\nprint(\"\ud83d\udce5 Loading data...\")\nloader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    shuffle=True,\n)\ndataset = loader.load()\n\n# 3. Create model\nprint(\"\ud83c\udfd7\ufe0f Creating model...\")\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\n# 4. Train model\nprint(\"\ud83c\udf93 Training model...\")\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"\\n\u2705 Training complete!\")\nprint(f\"   Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"   Final MAE: {result.metrics['mae']:.4f}\")\n\nprint(\"\\n\ud83c\udf89 Keras pipeline complete!\")\n</code></pre>"},{"location":"getting-started/quickstart.html#understanding-what-just-happened","title":"Understanding What Just Happened \ud83e\udd14","text":"<p>Let's break down what you just built:</p>"},{"location":"getting-started/quickstart.html#1-data-loading-the-easy-part","title":"1. Data Loading (The Easy Part)","text":"<pre><code>loader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n)\ndataset = loader.load()\n</code></pre> <p>What it does:</p> <ul> <li>Reads your CSV file</li> <li>Separates features from labels</li> <li>Batches data for training</li> <li>Handles shuffling and caching</li> </ul> <p>Why it's cool:</p> <ul> <li>Same API across all frameworks</li> <li>Optimized for performance</li> <li>Handles edge cases automatically</li> </ul>"},{"location":"getting-started/quickstart.html#2-training-configuration-the-type-safe-part","title":"2. Training Configuration (The Type-Safe Part)","text":"<pre><code>config = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n</code></pre> <p>What it does:</p> <ul> <li>Defines all training parameters</li> <li>Type-checked at runtime</li> <li>Validated before training</li> </ul> <p>Why it's cool:</p> <ul> <li>Your IDE autocompletes everything</li> <li>Catch errors before running</li> <li>Consistent across frameworks</li> </ul>"},{"location":"getting-started/quickstart.html#3-training-the-simple-part","title":"3. Training (The Simple Part)","text":"<pre><code>trainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n</code></pre> <p>What it does:</p> <ul> <li>Compiles your model</li> <li>Runs training loop</li> <li>Tracks metrics and history</li> <li>Returns a rich result object</li> </ul> <p>Why it's cool:</p> <ul> <li>One line to train</li> <li>Consistent interface</li> <li>Detailed results</li> </ul>"},{"location":"getting-started/quickstart.html#common-patterns","title":"Common Patterns \ud83c\udfaf","text":""},{"location":"getting-started/quickstart.html#pattern-1-train-val-test-split","title":"Pattern 1: Train-Val-Test Split","text":"<pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\n# Load different splits\ntrain_loader = CSVDataLoader(file_pattern=\"train.csv\", label_name=\"target\")\nval_loader = CSVDataLoader(file_pattern=\"val.csv\", label_name=\"target\")\ntest_loader = CSVDataLoader(file_pattern=\"test.csv\", label_name=\"target\")\n\ntrain_data = train_loader.load()\nval_data = val_loader.load()\ntest_data = test_loader.load()\n\n# Train with validation\nresult = trainer.train(\n    model,\n    train_data,\n    config,\n    validation_dataset=val_data\n)\n\n# Evaluate on test set\ntest_metrics = evaluator.evaluate(result.model, test_data, config)\n</code></pre>"},{"location":"getting-started/quickstart.html#pattern-2-early-stopping","title":"Pattern 2: Early Stopping","text":"<pre><code>config = ModelTrainingConfig(\n    epochs=100,  # Set high\n    learning_rate=0.001,\n    early_stopping=True,\n    early_stopping_patience=10,\n    early_stopping_monitor=\"val_loss\",\n)\n\nresult = trainer.train(model, train_data, config, validation_dataset=val_data)\nprint(f\"Best epoch: {result.best_epoch}\")\n</code></pre>"},{"location":"getting-started/quickstart.html#pattern-3-model-checkpointing","title":"Pattern 3: Model Checkpointing","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelPersistence\n\n# During training, save checkpoints\nfor epoch in range(10):\n    result = trainer.train(model, dataset, config)\n    if epoch % 2 == 0:  # Save every 2 epochs\n        persistence = ModelPersistence(\n            path=f\"checkpoint_epoch_{epoch}\",\n            model=result.model,\n        )\n        persistence.save()\n</code></pre>"},{"location":"getting-started/quickstart.html#pattern-4-hyperparameter-tuning","title":"Pattern 4: Hyperparameter Tuning","text":"<pre><code>learning_rates = [0.0001, 0.001, 0.01]\nbest_mae = float('inf')\nbest_model = None\n\nfor lr in learning_rates:\n    config = ModelTrainingConfig(\n        epochs=10,\n        learning_rate=lr,\n        verbose=0,  # Quiet mode\n    )\n\n    result = trainer.train(model, dataset, config)\n\n    if result.metrics['mae'] &lt; best_mae:\n        best_mae = result.metrics['mae']\n        best_model = result.model\n        print(f\"\u2728 New best! LR={lr}, MAE={best_mae:.4f}\")\n\npersistence = ModelPersistence(path=\"best_model\", model=best_model)\npersistence.save()\n</code></pre>"},{"location":"getting-started/quickstart.html#whats-next","title":"What's Next? \ud83d\uddfa\ufe0f","text":"<p>You've built your first pipeline! Here are your next steps:</p>"},{"location":"getting-started/quickstart.html#beginner-path","title":"Beginner Path \ud83c\udf31","text":"<ol> <li>Core Concepts \u2192 - Understand the architecture</li> <li>Framework Guide \u2192 - Deep dive into your framework</li> <li>Basic Tutorial \u2192 - Build a complete project</li> </ol>"},{"location":"getting-started/quickstart.html#intermediate-path","title":"Intermediate Path \ud83c\udf3f","text":"<ol> <li>ZenML Integration \u2192 - Add MLOps superpowers</li> <li>Advanced Training \u2192 - Custom callbacks, mixed precision</li> <li>Multi-Framework \u2192 - Switch between frameworks</li> </ol>"},{"location":"getting-started/quickstart.html#advanced-path","title":"Advanced Path \ud83c\udf33","text":"<ol> <li>Custom Components \u2192 - Build your own atoms</li> <li>Production Deployment \u2192 - Ship to production</li> <li>API Reference \u2192 - Deep technical docs</li> </ol>"},{"location":"getting-started/quickstart.html#need-help","title":"Need Help? \ud83c\udd98","text":"<ul> <li>Questions? Check the FAQ</li> <li>Issues? GitHub Issues</li> <li>Ideas? Discussions</li> </ul>"},{"location":"getting-started/quickstart.html#quick-reference-card","title":"Quick Reference Card \ud83d\udcc7","text":"<pre><code># Import patterns (UNIFIED API - same names across all frameworks!)\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,           # Load CSV data\n    DatasetOptimizer,        # Optimize datasets\n    ModelTrainer,            # Train models\n    ModelEvaluator,          # Evaluate models\n    ModelPersistence,        # Save/load models\n    ModelExporter,           # Export for serving\n    ModelTrainingConfig,     # Training configuration\n    ModelEvaluationConfig,   # Evaluation configuration\n    ModelExportConfig,       # Export configuration\n)\n\n# Basic workflow\nloader = CSVDataLoader(...)        # 1. Load data\ndataset = loader.load()\n\ntrainer = ModelTrainer()           # 2. Train\nresult = trainer.train(model, dataset, config)\n\nevaluator = ModelEvaluator()       # 3. Evaluate\nmetrics = evaluator.evaluate(result.model, dataset, config)\n\npersistence = ModelPersistence(    # 4. Save\n    path=\"path/\",\n    model=result.model,\n)\npersistence.save()\n</code></pre> <p> Congratulations! You're now brewing ML magic! \ud83e\uddea\u2728 Ready for more? Check out the tutorials! </p>"},{"location":"integrations/flowyml.html","title":"FlowyML Integration","text":"<p>MLPotion provides a first-class integration with FlowyML, an artifact-centric ML pipeline framework. Every MLPotion component (data loaders, trainers, evaluators, exporters) is exposed as a fully-wired FlowyML <code>@step</code> that returns typed artifacts \u2014 <code>Dataset</code>, <code>Model</code>, and <code>Metrics</code> \u2014 with automatic metadata extraction, lineage tracking, and DAG resolution.</p> <p>TL;DR \u2014 Install with <code>pip install mlpotion[flowyml,keras]</code>, pick a pipeline template, pass a <code>Context</code>, and call <code>.run()</code>. The integration handles artifact wrapping, metadata, caching, retry, and DAG wiring for you.</p>"},{"location":"integrations/flowyml.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start</li> <li>Installation</li> <li>Architecture</li> <li>Reusable Steps</li> <li>Step Design Principles</li> <li>Keras Steps</li> <li>PyTorch Steps</li> <li>TensorFlow Steps</li> <li>The <code>FlowyMLAdapter</code> (Generic Steps)</li> <li>Pipeline Templates</li> <li>Pipeline Design Principles</li> <li>1. Training Pipeline</li> <li>2. Full Pipeline</li> <li>3. Evaluation Pipeline</li> <li>4. Export Pipeline</li> <li>5. Experiment Pipeline (Conditional Deploy)</li> <li>6. Scheduled Pipeline</li> <li>Composing Custom Pipelines</li> <li>Artifact Types</li> <li>DAG Wiring</li> <li>Step Decorator Options</li> <li>Caching Strategies</li> <li>GPU Resources &amp; Execution Groups</li> <li>Context Injection</li> <li>Advanced Patterns</li> <li>Cross-Framework Reuse</li> <li>Combining Steps from Different Frameworks</li> <li>Building a Custom Step from MLPotion Components</li> <li>Conditional Flows</li> <li>Lineage Tracking</li> <li>Framework-Specific Notes</li> <li>API Reference</li> </ul>"},{"location":"integrations/flowyml.html#quick-start","title":"Quick Start","text":"<pre><code># Install MLPotion with FlowyML support\npip install mlpotion[flowyml,keras]\n</code></pre>"},{"location":"integrations/flowyml.html#train-a-keras-model-in-5-lines","title":"Train a Keras Model in 5 Lines","text":"<pre><code>from flowyml.core.context import Context\nfrom mlpotion.integrations.flowyml.keras import (\n    create_keras_training_pipeline,\n)\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=20,\n    learning_rate=0.001,\n    experiment_name=\"quick-start\",\n)\n\npipeline = create_keras_training_pipeline(context=ctx)\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#installation","title":"Installation","text":"<p>Install the integration alongside the framework you need:</p> <pre><code># Keras\npip install mlpotion[flowyml,keras]\n\n# PyTorch\npip install mlpotion[flowyml,pytorch]\n\n# TensorFlow\npip install mlpotion[flowyml,tensorflow]\n\n# All frameworks\npip install mlpotion[flowyml,keras,pytorch,tensorflow]\n</code></pre> <p>The <code>FLOWYML_AVAILABLE</code> flag is set at import time \u2014 if <code>flowyml</code> is not installed, the integration module skips all imports gracefully:</p> <pre><code>from mlpotion.integrations.flowyml import FLOWYML_AVAILABLE\n\nif FLOWYML_AVAILABLE:\n    from mlpotion.integrations.flowyml.keras import create_keras_training_pipeline\n</code></pre>"},{"location":"integrations/flowyml.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    FlowyML Runtime                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 DAG      \u2502  \u2502 Caching  \u2502  \u2502 Retry    \u2502  \u2502 Resource \u2502 \u2502\n\u2502  \u2502 Resolver \u2502  \u2502 Engine   \u2502  \u2502 Logic    \u2502  \u2502 Scheduler\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                        \u2502                                  \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502            \u25bc           \u25bc           \u25bc                      \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502      \u2502  Keras   \u2502 \u2502 PyTorch  \u2502 \u2502 TF/Keras \u2502  \u2190 Steps    \u2502\n\u2502      \u2502  Steps   \u2502 \u2502  Steps   \u2502 \u2502  Steps   \u2502             \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502            \u2502           \u2502           \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u25bc           \u25bc           \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502        MLPotion Components           \u2502\n      \u2502  CSVDataLoader \u00b7 ModelTrainer \u00b7      \u2502\n      \u2502  ModelEvaluator \u00b7 ModelExporter \u00b7    \u2502\n      \u2502  ModelPersistence \u00b7 ModelInspector   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/flowyml.html#flowyml-features-used","title":"FlowyML Features Used","text":"Feature How We Use It Artifact types Every step returns <code>Dataset</code>, <code>Model</code>, or <code>Metrics</code> with auto-metadata DAG wiring <code>inputs</code>/<code>outputs</code> on <code>@step</code> enable auto-resolution between steps Caching <code>cache=\"code_hash\"</code> or <code>cache=\"input_hash\"</code> skips unchanged steps Retry <code>retry=1</code> on training steps for transient failures Resource specs <code>ResourceRequirements(cpu, memory, gpu)</code> on GPU-intensive steps Execution groups <code>execution_group=\"training\"</code> groups training + eval on same node Tags <code>tags={\"framework\": \"keras\"}</code> for filtering and observability Context injection <code>Context(...)</code> provides hyperparameters to all steps Conditional flows <code>If(condition=..., then_steps=[...])</code> for conditional deployment Scheduling <code>PipelineScheduler</code> with cron syntax for periodic retraining Checkpointing <code>enable_checkpointing=True</code> for resumable long-running pipelines Experiment tracking <code>FlowymlKerasCallback</code> auto-captures all training metrics live Lineage <code>parent=</code> links transformed datasets back to their source"},{"location":"integrations/flowyml.html#reusable-steps","title":"Reusable Steps","text":"<p>Every step in the integration follows the same design contract:</p> <ol> <li>Accepts raw objects or FlowyML artifacts \u2014 steps auto-unwrap <code>Dataset.data</code>, <code>Model.data</code>, etc.</li> <li>Returns a typed artifact \u2014 <code>Dataset</code>, <code>Model</code>, or <code>Metrics</code> with auto-extracted metadata.</li> <li>Declares <code>inputs</code>/<code>outputs</code> \u2014 enables FlowyML DAG auto-wiring.</li> <li>Ships without hardcoded resource requirements \u2014 portable across CPU and GPU environments.</li> <li>Is independently usable \u2014 every step can be called standalone or composed into pipelines.</li> </ol>"},{"location":"integrations/flowyml.html#step-design-principles","title":"Step Design Principles","text":"<pre><code># Every step can be used in three ways:\n\n# 1. Standalone \u2014 call it directly as a function\ndataset = load_data(file_path=\"data/train.csv\", batch_size=32)\n\n# 2. In a pipeline \u2014 add it to a Pipeline object\npipeline = Pipeline(\"my_pipeline\")\npipeline.add_step(load_data)\n\n# 3. Via the adapter \u2014 wrap any MLPotion protocol component\nstep = FlowyMLAdapter.create_data_loader_step(my_custom_loader)\npipeline.add_step(step)\n</code></pre>"},{"location":"integrations/flowyml.html#keras-steps","title":"Keras Steps","text":"<p>Module: <code>mlpotion.integrations.flowyml.keras</code></p> <pre><code>from mlpotion.integrations.flowyml.keras import (\n    load_data,         # CSV \u2192 Dataset artifact\n    transform_data,    # Dataset + Model \u2192 transformed Dataset artifact\n    train_model,       # Model + Dataset \u2192 (Model, Metrics) artifacts\n    evaluate_model,    # Model + Dataset \u2192 Metrics artifact\n    export_model,      # Model \u2192 exported Model artifact\n    save_model,        # Model \u2192 saved Model artifact\n    load_model,        # path \u2192 Model artifact\n    inspect_model,     # Model \u2192 Metrics artifact (architecture details)\n)\n</code></pre>"},{"location":"integrations/flowyml.html#load_data","title":"<code>load_data</code>","text":"Parameter Type Default Description <code>file_path</code> <code>str</code> \u2014 Glob pattern for CSV files <code>batch_size</code> <code>int</code> <code>32</code> Batch size for the CSVSequence <code>label_name</code> <code>str \\| None</code> <code>None</code> Target column name <code>column_names</code> <code>list[str] \\| None</code> <code>None</code> Columns to load (None = all) <code>shuffle</code> <code>bool</code> <code>True</code> Whether to shuffle data <code>dtype</code> <code>str</code> <code>\"float32\"</code> Data type for numeric conversion <p>Returns: <code>Dataset</code> artifact with <code>source</code>, <code>batch_size</code>, <code>batches</code>, <code>label_name</code> metadata.</p> <p>Decorator config: <code>outputs=[\"dataset\"]</code>, <code>cache=\"code_hash\"</code>, <code>tags={\"framework\": \"keras\"}</code></p> <pre><code>dataset = load_data(file_path=\"data/train.csv\", batch_size=32, label_name=\"target\")\n\n# Access raw data\nraw_sequence = dataset.data          # CSVSequence\nprint(dataset.metadata.properties)   # {'source': '...', 'batch_size': 32, ...}\n</code></pre>"},{"location":"integrations/flowyml.html#transform_data","title":"<code>transform_data</code>","text":"Parameter Type Default Description <code>dataset</code> <code>Dataset</code> \u2014 Input Dataset artifact <code>model</code> <code>keras.Model</code> \u2014 Model for generating predictions <code>data_output_path</code> <code>str</code> \u2014 Output path for transformed CSV <code>data_output_per_batch</code> <code>bool</code> <code>False</code> One file per batch <code>batch_size</code> <code>int \\| None</code> <code>None</code> Batch size override <code>feature_names</code> <code>list[str] \\| None</code> <code>None</code> Feature names for output <code>input_columns</code> <code>list[str] \\| None</code> <code>None</code> Input columns to pass to model <p>Returns: <code>Dataset</code> artifact with parent lineage linked to the input dataset.</p> <p>Decorator config: <code>inputs=[\"dataset\"]</code>, <code>outputs=[\"transformed\"]</code>, <code>cache=\"code_hash\"</code></p>"},{"location":"integrations/flowyml.html#train_model","title":"<code>train_model</code>","text":"Parameter Type Default Description <code>model</code> <code>keras.Model</code> \u2014 Compiled Keras model <code>data</code> <code>CSVSequence \\| Dataset</code> \u2014 Training data <code>epochs</code> <code>int</code> <code>10</code> Number of epochs <code>learning_rate</code> <code>float</code> <code>0.001</code> Learning rate <code>verbose</code> <code>int</code> <code>1</code> Keras verbosity level <code>validation_data</code> <code>CSVSequence \\| Dataset \\| None</code> <code>None</code> Validation data <code>callbacks</code> <code>list[Callback] \\| None</code> <code>None</code> Extra Keras callbacks <code>experiment_name</code> <code>str \\| None</code> <code>None</code> FlowyML experiment name <code>project</code> <code>str \\| None</code> <code>None</code> FlowyML project name <code>log_model</code> <code>bool</code> <code>True</code> Log model artifact after training <p>Returns: <code>tuple[Model, Metrics]</code> \u2014 Model via <code>Model.from_keras()</code> with auto-extracted architecture metadata; Metrics with training history.</p> <p>Decorator config: <code>inputs=[\"dataset\"]</code>, <code>outputs=[\"model\", \"training_metrics\"]</code>, <code>cache=False</code>, <code>retry=1</code></p> <p>\ud83d\udd11 Key feature: A <code>FlowymlKerasCallback</code> is automatically attached to capture all training metrics live to the FlowyML dashboard.</p> <pre><code>model_asset, metrics_asset = train_model(\n    model=my_keras_model,\n    data=dataset,\n    epochs=20,\n    learning_rate=0.001,\n    experiment_name=\"v1\",\n)\n\nprint(metrics_asset.get_metric(\"loss\"))\nprint(metrics_asset.values)           # All metrics as dict\nprint(model_asset.metadata.properties)  # Auto-extracted: layers, params, etc.\n</code></pre>"},{"location":"integrations/flowyml.html#evaluate_model","title":"<code>evaluate_model</code>","text":"Parameter Type Default Description <code>model</code> <code>keras.Model \\| Model</code> \u2014 Trained model or Model artifact <code>data</code> <code>CSVSequence \\| Dataset</code> \u2014 Evaluation data <code>verbose</code> <code>int</code> <code>0</code> Keras verbosity level <p>Returns: <code>Metrics</code> artifact with evaluation results.</p> <p>Decorator config: <code>inputs=[\"model\", \"dataset\"]</code>, <code>outputs=[\"metrics\"]</code>, <code>cache=\"input_hash\"</code></p>"},{"location":"integrations/flowyml.html#export_model","title":"<code>export_model</code>","text":"Parameter Type Default Description <code>model</code> <code>keras.Model \\| Model</code> \u2014 Model to export <code>export_path</code> <code>str</code> \u2014 Destination path <code>export_format</code> <code>str \\| None</code> <code>None</code> Format: <code>\"keras\"</code>, <code>\"saved_model\"</code>, <code>\"tflite\"</code> <p>Returns: <code>Model</code> artifact with export metadata.</p> <p>Decorator config: <code>inputs=[\"model\"]</code>, <code>outputs=[\"exported_model\"]</code>, <code>cache=\"code_hash\"</code></p>"},{"location":"integrations/flowyml.html#save_model","title":"<code>save_model</code>","text":"Parameter Type Default Description <code>model</code> <code>keras.Model \\| Model</code> \u2014 Model to save <code>save_path</code> <code>str</code> \u2014 Destination file path <p>Returns: <code>Model</code> artifact with save location metadata.</p> <p>Decorator config: <code>inputs=[\"model\"]</code>, <code>outputs=[\"saved_model\"]</code></p>"},{"location":"integrations/flowyml.html#load_model","title":"<code>load_model</code>","text":"Parameter Type Default Description <code>model_path</code> <code>str</code> \u2014 Path to saved model <code>inspect</code> <code>bool</code> <code>False</code> Log inspection info <p>Returns: <code>Model</code> artifact wrapping the loaded Keras model.</p> <p>Decorator config: <code>outputs=[\"model\"]</code>, <code>cache=\"code_hash\"</code></p>"},{"location":"integrations/flowyml.html#inspect_model","title":"<code>inspect_model</code>","text":"Parameter Type Default Description <code>model</code> <code>keras.Model \\| Model</code> \u2014 Model to inspect <code>include_layers</code> <code>bool</code> <code>True</code> Include per-layer info <code>include_signatures</code> <code>bool</code> <code>True</code> Include I/O signatures <p>Returns: <code>Metrics</code> artifact with model architecture details (name, parameter counts, layer info).</p> <p>Decorator config: <code>inputs=[\"model\"]</code>, <code>outputs=[\"inspection\"]</code></p>"},{"location":"integrations/flowyml.html#pytorch-steps","title":"PyTorch Steps","text":"<p>Module: <code>mlpotion.integrations.flowyml.pytorch</code></p> <pre><code>from mlpotion.integrations.flowyml.pytorch import (\n    load_csv_data,           # CSV \u2192 Dataset artifact (standard)\n    load_streaming_csv_data, # CSV \u2192 Dataset artifact (chunked streaming)\n    train_model,             # Model + Dataset \u2192 (Model, Metrics) artifacts\n    evaluate_model,          # Model + Dataset \u2192 Metrics artifact\n    export_model,            # Model \u2192 exported Model artifact\n    save_model,              # Model \u2192 saved Model artifact\n    load_model,              # path \u2192 Model artifact\n)\n</code></pre>"},{"location":"integrations/flowyml.html#load_csv_data","title":"<code>load_csv_data</code>","text":"Parameter Type Default Description <code>file_path</code> <code>str</code> \u2014 Glob pattern for CSV files <code>batch_size</code> <code>int</code> <code>32</code> Batch size <code>label_name</code> <code>str \\| None</code> <code>None</code> Target column <code>column_names</code> <code>list[str] \\| None</code> <code>None</code> Columns to load <code>shuffle</code> <code>bool</code> <code>True</code> Shuffle data <code>num_workers</code> <code>int</code> <code>0</code> DataLoader workers <code>pin_memory</code> <code>bool</code> <code>False</code> Pin memory for faster GPU transfer <code>drop_last</code> <code>bool</code> <code>False</code> Drop last incomplete batch <code>dtype</code> <code>str</code> <code>\"float32\"</code> Tensor data type <p>Returns: <code>Dataset</code> artifact wrapping a PyTorch <code>DataLoader</code>.</p> <p>Decorator config: <code>outputs=[\"dataset\"]</code>, <code>cache=\"code_hash\"</code></p>"},{"location":"integrations/flowyml.html#load_streaming_csv_data","title":"<code>load_streaming_csv_data</code>","text":"<p>For datasets that don't fit in memory \u2014 uses chunked reading:</p> Parameter Type Default Description <code>file_path</code> <code>str</code> \u2014 Glob pattern for CSV files <code>batch_size</code> <code>int</code> <code>32</code> Batch size <code>label_name</code> <code>str \\| None</code> <code>None</code> Target column <code>column_names</code> <code>list[str] \\| None</code> <code>None</code> Columns to load <code>num_workers</code> <code>int</code> <code>0</code> DataLoader workers <code>pin_memory</code> <code>bool</code> <code>False</code> Pin memory for GPU <code>chunksize</code> <code>int</code> <code>10000</code> Rows per chunk <code>dtype</code> <code>str</code> <code>\"float32\"</code> Tensor data type <p>Returns: <code>Dataset</code> artifact with <code>mode: \"streaming\"</code> metadata.</p> <p>Decorator config: <code>outputs=[\"dataset\"]</code>, <code>cache=False</code> (streaming data changes)</p> <p>\ud83d\udca1 When to use: Choose <code>load_streaming_csv_data</code> when your dataset is too large for memory. Choose <code>load_csv_data</code> for standard in-memory workflows.</p>"},{"location":"integrations/flowyml.html#train_model_1","title":"<code>train_model</code>","text":"Parameter Type Default Description <code>model</code> <code>nn.Module</code> \u2014 PyTorch model <code>data</code> <code>DataLoader \\| Dataset</code> \u2014 Training data <code>epochs</code> <code>int</code> <code>10</code> Number of epochs <code>learning_rate</code> <code>float</code> <code>0.001</code> Learning rate <code>optimizer</code> <code>str</code> <code>\"adam\"</code> Optimizer: <code>\"adam\"</code>, <code>\"sgd\"</code>, <code>\"adamw\"</code> <code>loss_fn</code> <code>str</code> <code>\"mse\"</code> Loss: <code>\"mse\"</code>, <code>\"cross_entropy\"</code> <code>device</code> <code>str</code> <code>\"cpu\"</code> Device: <code>\"cpu\"</code> or <code>\"cuda\"</code> <code>validation_data</code> <code>DataLoader \\| Dataset \\| None</code> <code>None</code> Validation data <code>verbose</code> <code>bool</code> <code>True</code> Log per-epoch metrics <code>max_batches_per_epoch</code> <code>int \\| None</code> <code>None</code> Limit batches (debugging) <p>Returns: <code>tuple[Model, Metrics]</code> \u2014 Model via <code>Model.from_pytorch()</code> with auto-extracted module architecture.</p> <p>Decorator config: <code>inputs=[\"dataset\"]</code>, <code>outputs=[\"model\", \"training_metrics\"]</code>, <code>cache=False</code>, <code>retry=1</code></p>"},{"location":"integrations/flowyml.html#evaluate_model_1","title":"<code>evaluate_model</code>","text":"Parameter Type Default Description <code>model</code> <code>nn.Module \\| Model</code> \u2014 Trained model or Model artifact <code>data</code> <code>DataLoader \\| Dataset</code> \u2014 Evaluation data <code>loss_fn</code> <code>str</code> <code>\"mse\"</code> Loss function <code>device</code> <code>str</code> <code>\"cpu\"</code> Device <code>verbose</code> <code>bool</code> <code>True</code> Log metrics <code>max_batches</code> <code>int \\| None</code> <code>None</code> Limit batches <p>Returns: <code>Metrics</code> artifact.</p> <p>Decorator config: <code>inputs=[\"model\", \"dataset\"]</code>, <code>outputs=[\"metrics\"]</code>, <code>cache=\"input_hash\"</code></p>"},{"location":"integrations/flowyml.html#export_model_1","title":"<code>export_model</code>","text":"Parameter Type Default Description <code>model</code> <code>nn.Module \\| Model</code> \u2014 Model to export <code>export_path</code> <code>str</code> \u2014 Destination path <code>export_format</code> <code>str</code> <code>\"torchscript\"</code> Format: <code>\"torchscript\"</code>, <code>\"onnx\"</code> <code>sample_input</code> <code>torch.Tensor \\| None</code> <code>None</code> Required for ONNX tracing <p>Returns: <code>Model</code> artifact with export metadata.</p> <p>Decorator config: <code>inputs=[\"model\"]</code>, <code>outputs=[\"exported_model\"]</code>, <code>cache=\"code_hash\"</code></p>"},{"location":"integrations/flowyml.html#save_model-load_model","title":"<code>save_model</code> / <code>load_model</code>","text":"<p>Same contract as Keras \u2014 accept raw or artifact objects, return <code>Model</code> artifacts.</p>"},{"location":"integrations/flowyml.html#tensorflow-steps","title":"TensorFlow Steps","text":"<p>Module: <code>mlpotion.integrations.flowyml.tensorflow</code></p> <pre><code>from mlpotion.integrations.flowyml.tensorflow import (\n    load_data,         # CSV \u2192 Dataset artifact\n    optimize_data,     # Dataset \u2192 optimized Dataset artifact (TF-specific)\n    transform_data,    # Dataset + Model \u2192 transformed Dataset artifact\n    train_model,       # Model + Dataset \u2192 (Model, Metrics) artifacts\n    evaluate_model,    # Model + Dataset \u2192 Metrics artifact\n    export_model,      # Model \u2192 exported Model artifact\n    save_model,        # Model \u2192 saved Model artifact\n    load_model,        # path \u2192 Model artifact\n    inspect_model,     # Model \u2192 Metrics artifact\n)\n</code></pre>"},{"location":"integrations/flowyml.html#optimize_data-tf-exclusive","title":"<code>optimize_data</code> (TF-exclusive)","text":"<p>Applies <code>tf.data.Dataset</code> optimization (prefetch, cache, shuffle) and returns a new <code>Dataset</code> artifact with parent lineage linked to the input:</p> Parameter Type Default Description <code>dataset</code> <code>tf.data.Dataset \\| Dataset</code> \u2014 Input dataset <code>batch_size</code> <code>int</code> <code>32</code> Batch size <code>shuffle_buffer_size</code> <code>int \\| None</code> <code>None</code> Shuffle buffer size <code>prefetch</code> <code>bool</code> <code>True</code> Enable prefetching <code>cache</code> <code>bool</code> <code>False</code> Enable dataset caching <p>Returns: <code>Dataset</code> artifact with <code>parent=</code> lineage.</p> <p>Decorator config: <code>inputs=[\"dataset\"]</code>, <code>outputs=[\"optimized_dataset\"]</code>, <code>cache=\"code_hash\"</code></p> <pre><code>from mlpotion.integrations.flowyml.tensorflow import load_data, optimize_data\n\ndataset = load_data(file_path=\"data/train.csv\", batch_size=32)\noptimized = optimize_data(dataset=dataset, prefetch=True, cache=True)\n\n# Lineage is preserved\nprint(optimized.parent)  # Points back to `dataset`\n</code></pre> <p>The remaining TF steps (<code>load_data</code>, <code>train_model</code>, <code>evaluate_model</code>, etc.) follow the same contract as Keras. The <code>train_model</code> step also auto-attaches <code>FlowymlKerasCallback</code> when <code>experiment_name</code> is provided.</p>"},{"location":"integrations/flowyml.html#the-flowymladapter-generic-steps","title":"The <code>FlowyMLAdapter</code> (Generic Steps)","text":"<p>For framework-agnostic or custom component scenarios, the <code>FlowyMLAdapter</code> wraps any MLPotion protocol-compliant component into a FlowyML step:</p> <pre><code>from mlpotion.integrations.flowyml import FlowyMLAdapter\n</code></pre>"},{"location":"integrations/flowyml.html#flowymladaptercreate_data_loader_step","title":"<code>FlowyMLAdapter.create_data_loader_step()</code>","text":"<pre><code>step = FlowyMLAdapter.create_data_loader_step(\n    loader,                     # Any DataLoader protocol implementation\n    name=\"custom_load\",         # Step name (default: \"load_data\")\n    cache=\"code_hash\",          # Caching strategy\n    retry=0,                    # Retry count\n    resources=None,             # ResourceRequirements\n    tags={\"env\": \"staging\"},    # Metadata tags\n)\n</code></pre> <p>Returns: A <code>Step</code> that calls <code>loader.load()</code> and wraps the result as a <code>Dataset</code> artifact.</p>"},{"location":"integrations/flowyml.html#flowymladaptercreate_training_step","title":"<code>FlowyMLAdapter.create_training_step()</code>","text":"<pre><code>step = FlowyMLAdapter.create_training_step(\n    trainer,                    # Any ModelTrainer protocol implementation\n    name=\"custom_train\",        # Step name (default: \"train_model\")\n    cache=False,                # Default: no caching for training\n    retry=1,                    # Retry once on failure\n    resources=gpu_resources,    # GPU ResourceRequirements\n    tags={\"stage\": \"train\"},\n)\n</code></pre> <p>Returns: A <code>Step</code> that calls <code>trainer.train()</code> and wraps result as a <code>Model</code> artifact.</p>"},{"location":"integrations/flowyml.html#flowymladaptercreate_evaluation_step","title":"<code>FlowyMLAdapter.create_evaluation_step()</code>","text":"<pre><code>step = FlowyMLAdapter.create_evaluation_step(\n    evaluator,                  # Any ModelEvaluator protocol implementation\n    name=\"custom_eval\",         # Step name (default: \"evaluate_model\")\n    cache=\"input_hash\",         # Default: cache by input hash\n    retry=0,\n    tags={\"stage\": \"eval\"},\n)\n</code></pre> <p>Returns: A <code>Step</code> that calls <code>evaluator.evaluate()</code> and wraps result as a <code>Metrics</code> artifact.</p>"},{"location":"integrations/flowyml.html#example-custom-component-pipeline","title":"Example: Custom Component \u2192 Pipeline","text":"<pre><code>from mlpotion.frameworks.keras.data.loaders import CSVDataLoader\nfrom mlpotion.frameworks.keras.training.trainers import ModelTrainer\nfrom mlpotion.frameworks.keras.evaluation.evaluators import ModelEvaluator\nfrom mlpotion.integrations.flowyml import FlowyMLAdapter\nfrom flowyml.core.pipeline import Pipeline\n\n# Wrap MLPotion components\nload_step = FlowyMLAdapter.create_data_loader_step(\n    CSVDataLoader(file_pattern=\"data/*.csv\", batch_size=64)\n)\ntrain_step = FlowyMLAdapter.create_training_step(ModelTrainer())\neval_step = FlowyMLAdapter.create_evaluation_step(ModelEvaluator())\n\n# Build a pipeline\npipeline = Pipeline(\"custom_pipeline\")\npipeline.add_step(load_step)\npipeline.add_step(train_step)\npipeline.add_step(eval_step)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#pipeline-templates","title":"Pipeline Templates","text":"<p>Each framework provides 6 ready-to-use pipeline factories. All accept a <code>Context</code> object for injecting hyperparameters and return a configured <code>Pipeline</code> (or <code>dict</code> with pipeline + scheduler for scheduled pipelines).</p>"},{"location":"integrations/flowyml.html#pipeline-design-principles","title":"Pipeline Design Principles","text":"<ol> <li>Context-driven \u2014 all hyperparameters flow through a single <code>Context</code> object.</li> <li>DAG-resolved \u2014 steps are wired by their <code>inputs</code>/<code>outputs</code> declarations.</li> <li>Configurable \u2014 toggle caching, checkpointing, versioning via factory args.</li> <li>Composable \u2014 use them as-is or as templates for your own pipelines.</li> </ol>"},{"location":"integrations/flowyml.html#pipeline-availability-matrix","title":"Pipeline Availability Matrix","text":"Pipeline Keras PyTorch TensorFlow Training <code>create_keras_training_pipeline</code> <code>create_pytorch_training_pipeline</code> <code>create_tf_training_pipeline</code> Full <code>create_keras_full_pipeline</code> <code>create_pytorch_full_pipeline</code> <code>create_tf_full_pipeline</code> Evaluation <code>create_keras_evaluation_pipeline</code> <code>create_pytorch_evaluation_pipeline</code> <code>create_tf_evaluation_pipeline</code> Export <code>create_keras_export_pipeline</code> <code>create_pytorch_export_pipeline</code> <code>create_tf_export_pipeline</code> Experiment <code>create_keras_experiment_pipeline</code> <code>create_pytorch_experiment_pipeline</code> <code>create_tf_experiment_pipeline</code> Scheduled <code>create_keras_scheduled_pipeline</code> <code>create_pytorch_scheduled_pipeline</code> <code>create_tf_scheduled_pipeline</code>"},{"location":"integrations/flowyml.html#1-training-pipeline","title":"1. Training Pipeline","text":"<p>Basic load \u2192 train \u2192 evaluate workflow.</p> <p>DAG: <pre><code>load_data \u2192 train_model \u2192 evaluate_model\n</code></pre></p> <p>Factory arguments:</p> Argument Type Default Description <code>name</code> <code>str</code> <code>\"&lt;fw&gt;_training\"</code> Pipeline name <code>context</code> <code>Context \\| None</code> <code>None</code> Hyperparameters <code>enable_cache</code> <code>bool</code> <code>True</code> Enable step caching <code>project_name</code> <code>str \\| None</code> <code>None</code> FlowyML project <code>version</code> <code>str \\| None</code> <code>None</code> Pipeline version <p>Context parameters:</p> Parameter Required Description <code>file_path</code> \u2705 Path/glob to training CSV <code>label_name</code> \u2705 Target column name <code>batch_size</code> \u2014 Batch size (default: 32) <code>epochs</code> \u2014 Training epochs (default: 10) <code>learning_rate</code> \u2014 Learning rate (default: 0.001) <code>experiment_name</code> \u2014 FlowyML experiment tracking name <pre><code>from flowyml.core.context import Context\nfrom mlpotion.integrations.flowyml.keras import create_keras_training_pipeline\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=20,\n    learning_rate=0.001,\n)\n\npipeline = create_keras_training_pipeline(\n    name=\"classification_v1\",\n    context=ctx,\n    project_name=\"my_project\",\n)\nresult = pipeline.run()\n</code></pre> <p>PyTorch equivalent includes additional context params: <code>optimizer</code>, <code>loss_fn</code>, <code>device</code>:</p> <pre><code>from mlpotion.integrations.flowyml.pytorch import create_pytorch_training_pipeline\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    epochs=20,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=\"cross_entropy\",\n    device=\"cuda\",\n)\n\npipeline = create_pytorch_training_pipeline(context=ctx)\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#2-full-pipeline","title":"2. Full Pipeline","text":"<p>Complete lifecycle with data transformation/optimization and export.</p> <p>DAG (Keras): <pre><code>load_data \u2192 transform_data \u2192 train_model \u2192 evaluate_model \u2192 export_model\n</code></pre></p> <p>DAG (TensorFlow): <pre><code>load_data \u2192 optimize_data \u2192 train_model \u2192 evaluate_model \u2192 export_model\n</code></pre></p> <p>DAG (PyTorch): <pre><code>load_csv_data \u2192 train_model \u2192 evaluate_model \u2192 export_model \u2192 save_model\n</code></pre></p> <p>Additional factory arguments:</p> Argument Type Default Description <code>enable_checkpointing</code> <code>bool</code> <code>True</code> Resume on failure <p>Context parameters (superset):</p> Parameter Required Description <code>file_path</code> \u2705 Training data path <code>label_name</code> \u2705 Target column <code>batch_size</code> \u2014 Batch size <code>data_output_path</code> \u2014 Keras: transformed data output path <code>shuffle_buffer_size</code> \u2014 TF: shuffle buffer size <code>prefetch</code> \u2014 TF: enable prefetching <code>epochs</code> \u2014 Training epochs <code>learning_rate</code> \u2014 Learning rate <code>experiment_name</code> \u2014 Experiment tracking name <code>export_path</code> \u2014 Export destination <code>export_format</code> \u2014 Export format <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_full_pipeline\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    data_output_path=\"data/transformed/\",\n    epochs=50,\n    learning_rate=0.001,\n    experiment_name=\"full-run\",\n    export_path=\"models/production/\",\n    export_format=\"keras\",\n)\n\npipeline = create_keras_full_pipeline(\n    context=ctx,\n    enable_checkpointing=True,\n)\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#3-evaluation-pipeline","title":"3. Evaluation Pipeline","text":"<p>Evaluate an existing model against new data.</p> <p>DAG: <pre><code>load_model \u2192 load_data \u2192 evaluate_model \u2192 inspect_model\n</code></pre></p> <p>Note: PyTorch evaluation pipeline does not include <code>inspect_model</code>.</p> <p>Context parameters:</p> Parameter Required Description <code>model_path</code> \u2705 Path to saved model <code>file_path</code> \u2705 Evaluation data path <code>label_name</code> \u2705 Target column <code>batch_size</code> \u2014 Batch size <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_evaluation_pipeline\n\nctx = Context(\n    model_path=\"models/production/model.keras\",\n    file_path=\"data/test.csv\",\n    label_name=\"target\",\n    batch_size=64,\n)\n\npipeline = create_keras_evaluation_pipeline(context=ctx)\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#4-export-pipeline","title":"4. Export Pipeline","text":"<p>Convert and persist a model to a specified format.</p> <p>DAG: <pre><code>load_model \u2192 export_model, save_model\n</code></pre></p> <p>Context parameters:</p> Parameter Required Description <code>model_path</code> \u2705 Path to trained model <code>export_path</code> \u2705 Export destination <code>export_format</code> \u2014 Export format <code>save_path</code> \u2014 Backup save path <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_export_pipeline\n\nctx = Context(\n    model_path=\"models/trained/model.keras\",\n    export_path=\"models/exported/\",\n    export_format=\"saved_model\",\n    save_path=\"models/backup/model.keras\",\n)\n\npipeline = create_keras_export_pipeline(context=ctx)\nresult = pipeline.run()\n</code></pre> <p>PyTorch supports <code>export_format=\"torchscript\"</code> or <code>\"onnx\"</code>.</p>"},{"location":"integrations/flowyml.html#5-experiment-pipeline-conditional-deploy","title":"5. Experiment Pipeline (Conditional Deploy)","text":"<p>Train and auto-deploy only if metrics exceed a threshold.</p> <p>DAG: <pre><code>load_data \u2192 train_model \u2192 evaluate_model\n                                \u2193\n                       [if metric \u2265 threshold]\n                                \u2193\n                       export_model \u2192 save_model\n</code></pre></p> <p>Additional factory arguments:</p> Argument Type Default Description <code>deploy_threshold</code> <code>float</code> <code>0.8</code> Minimum metric value <code>threshold_metric</code> <code>str</code> <code>\"accuracy\"</code> Metric to check <p>Pipeline features enabled: - <code>enable_experiment_tracking=True</code> - <code>enable_checkpointing=True</code> - <code>enable_cache=False</code></p> <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_experiment_pipeline\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    epochs=30,\n    experiment_name=\"experiment-v1\",\n    export_path=\"models/production/\",\n    save_path=\"models/checkpoints/model.keras\",\n)\n\npipeline = create_keras_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\nresult = pipeline.run()\n</code></pre> <p>Under the hood, this uses FlowyML's <code>If</code> conditional flow:</p> <pre><code>from flowyml.core.conditional import If\n\ndeploy_condition = If(\n    condition=lambda metrics: metrics.get_metric(\"accuracy\", 0) &gt;= 0.85,\n    then_steps=[export_model, save_model],\n    name=\"deploy_if_accuracy_above_0.85\",\n)\npipeline.control_flows.append(deploy_condition)\n</code></pre>"},{"location":"integrations/flowyml.html#6-scheduled-pipeline","title":"6. Scheduled Pipeline","text":"<p>Periodic retraining with cron scheduling. Returns both the pipeline and a configured scheduler:</p> <p>DAG: <pre><code>load_data \u2192 train_model \u2192 evaluate_model \u2192 export_model\n</code></pre></p> <p>Additional factory arguments:</p> Argument Type Default Description <code>schedule</code> <code>str</code> <code>\"0 2 * * 0\"</code> Cron expression <code>timezone</code> <code>str</code> <code>\"UTC\"</code> Timezone <p>Returns: <code>dict[str, Any]</code> with <code>\"pipeline\"</code> and <code>\"scheduler\"</code> keys.</p> <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_scheduled_pipeline\n\ninfo = create_keras_scheduled_pipeline(\n    context=ctx,\n    schedule=\"0 2 * * 0\",   # Every Sunday at 2 AM\n    timezone=\"UTC\",\n)\n\npipeline = info[\"pipeline\"]\nscheduler = info[\"scheduler\"]\n\n# Run once now\nresult = pipeline.run()\n\n# Or start the scheduler for automatic retraining\nscheduler.start()\n</code></pre> <p>Common cron patterns:</p> Pattern Description <code>0 2 * * 0</code> Every Sunday at 2 AM <code>0 0 * * *</code> Every day at midnight <code>0 */6 * * *</code> Every 6 hours <code>0 0 1 * *</code> First day of every month"},{"location":"integrations/flowyml.html#composing-custom-pipelines","title":"Composing Custom Pipelines","text":"<p>You can mix and match any steps from the integration to build your own pipeline. Steps are not locked to their pipeline templates:</p>"},{"location":"integrations/flowyml.html#example-custom-train-inspect-export-pipeline","title":"Example: Custom Train \u2192 Inspect \u2192 Export Pipeline","text":"<pre><code>from flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    train_model,\n    inspect_model,\n    export_model,\n)\n\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    epochs=50,\n    export_path=\"models/prod/\",\n)\n\npipeline = Pipeline(\n    name=\"train_inspect_export\",\n    context=ctx,\n    enable_cache=True,\n    enable_checkpointing=True,\n)\n\n# Add only the steps you need\npipeline.add_step(load_data)\npipeline.add_step(train_model)\npipeline.add_step(inspect_model)   # Inspect architecture after training\npipeline.add_step(export_model)\n\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#example-adding-a-custom-step","title":"Example: Adding a Custom Step","text":"<pre><code>from flowyml.core.step import step\nfrom flowyml import Metrics\n\n@step(\n    name=\"compute_custom_metrics\",\n    inputs=[\"model\", \"dataset\"],\n    outputs=[\"custom_metrics\"],\n    tags={\"stage\": \"custom\"},\n)\ndef compute_custom_metrics(model, data):\n    \"\"\"Your custom metric computation logic.\"\"\"\n    # ... your code here ...\n    return Metrics.create(\n        metrics={\"f1_score\": 0.92, \"precision\": 0.95},\n        name=\"custom_metrics\",\n    )\n\n# Add it to any pipeline\npipeline.add_step(compute_custom_metrics)\n</code></pre>"},{"location":"integrations/flowyml.html#example-mixing-adapter-and-pre-built-steps","title":"Example: Mixing Adapter and Pre-built Steps","text":"<pre><code>from mlpotion.integrations.flowyml import FlowyMLAdapter\nfrom mlpotion.integrations.flowyml.keras import train_model, evaluate_model\n\n# Use MyCustomLoader with the adapter\nmy_loader = MyCustomLoader(source=\"s3://bucket/data.csv\")\nload_step = FlowyMLAdapter.create_data_loader_step(my_loader, name=\"s3_load\")\n\n# Combine with pre-built steps\npipeline = Pipeline(\"hybrid_pipeline\")\npipeline.add_step(load_step)          # Custom adapter step\npipeline.add_step(train_model)         # Pre-built Keras step\npipeline.add_step(evaluate_model)      # Pre-built Keras step\n\nresult = pipeline.run()\n</code></pre>"},{"location":"integrations/flowyml.html#artifact-types","title":"Artifact Types","text":""},{"location":"integrations/flowyml.html#dataset","title":"Dataset","text":"<p>Wraps raw data (CSVSequence, DataLoader, tf.data.Dataset) with metadata:</p> <pre><code>from flowyml import Dataset\n\n# Created automatically by load_data steps\ndataset = load_data(file_path=\"data/train.csv\", batch_size=32)\n\nassert isinstance(dataset, Dataset)\nprint(dataset.data)                    # Raw CSVSequence/DataLoader\nprint(dataset.metadata.properties)     # {'source': '...', 'batch_size': 32, ...}\n</code></pre>"},{"location":"integrations/flowyml.html#model","title":"Model","text":"<p>Wraps trained models with auto-extracted architecture metadata:</p> <pre><code>from flowyml import Model\n\n# Created automatically by train_model, from_keras / from_pytorch\nmodel_asset, _ = train_model(model=my_model, data=dataset, epochs=10)\n\nassert isinstance(model_asset, Model)\nprint(model_asset.data)                # Raw keras.Model / nn.Module\nprint(model_asset.metadata.properties) # Auto-extracted: layers, params, etc.\n</code></pre> <p>Auto-extraction methods: - <code>Model.from_keras(model, ...)</code> \u2014 extracts layers, parameters, optimizer info - <code>Model.from_pytorch(model, ...)</code> \u2014 extracts module architecture</p>"},{"location":"integrations/flowyml.html#metrics","title":"Metrics","text":"<p>Wraps numeric metrics and training history:</p> <pre><code>from flowyml import Metrics\n\n_, metrics = train_model(model=my_model, data=dataset, epochs=10)\n\nassert isinstance(metrics, Metrics)\nprint(metrics.get_metric(\"loss\"))          # Access single metric\nprint(metrics.values)                      # All metrics as dict\nprint(metrics.metadata.properties)         # Also stored in properties\n</code></pre>"},{"location":"integrations/flowyml.html#dag-wiring","title":"DAG Wiring","text":"<p>Steps declare <code>inputs</code> and <code>outputs</code> so FlowyML can auto-wire them:</p> <pre><code>load_data            \u2192  outputs: [\"dataset\"]\ntransform_data       \u2192  inputs: [\"dataset\"],  outputs: [\"transformed\"]\noptimize_data (TF)   \u2192  inputs: [\"dataset\"],  outputs: [\"optimized_dataset\"]\ntrain_model          \u2192  inputs: [\"dataset\"],   outputs: [\"model\", \"training_metrics\"]\nevaluate_model       \u2192  inputs: [\"model\", \"dataset\"],  outputs: [\"metrics\"]\nexport_model         \u2192  inputs: [\"model\"],     outputs: [\"exported_model\"]\nsave_model           \u2192  inputs: [\"model\"],     outputs: [\"saved_model\"]\nload_model           \u2192  outputs: [\"model\"]\ninspect_model        \u2192  inputs: [\"model\"],     outputs: [\"inspection\"]\n</code></pre> <p>When steps are added to a pipeline, FlowyML's DAG resolver automatically connects matching output\u2192input names, creating a dependency graph.</p>"},{"location":"integrations/flowyml.html#artifact-unwrapping","title":"Artifact Unwrapping","text":"<p>Steps also gracefully accept both raw objects and FlowyML artifacts as input \u2014 they unwrap artifacts internally before passing data to the underlying MLPotion components:</p> <pre><code># Both of these work identically:\nevaluate_model(model=keras_model, data=csv_sequence)           # raw objects\nevaluate_model(model=model_artifact, data=dataset_artifact)     # FlowyML artifacts\n</code></pre> <p>The unwrapping logic is:</p> <pre><code>raw_model = model.data if isinstance(model, Model) else model\nraw_data  = data.data  if isinstance(data, Dataset) else data\n</code></pre>"},{"location":"integrations/flowyml.html#step-decorator-options","title":"Step Decorator Options","text":"<p>Every step in the integration uses FlowyML's <code>@step</code> decorator. Here is the full set of options available to you when creating custom steps:</p> <pre><code>from flowyml.core.step import step\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig\n\n@step(\n    name=\"my_step\",                          # Unique step name\n    inputs=[\"dataset\"],                      # DAG input names\n    outputs=[\"model\", \"metrics\"],            # DAG output names\n    cache=\"code_hash\",                       # \"code_hash\", \"input_hash\", False\n    retry=1,                                 # Retry count on failure\n    resources=ResourceRequirements(          # Resource requirements\n        cpu=\"4\",\n        memory=\"16Gi\",\n        gpu=GPUConfig(gpu_type=\"nvidia-a100\", count=2),\n    ),\n    execution_group=\"training\",              # Group steps on same node\n    tags={\"framework\": \"keras\"},             # Metadata tags\n)\ndef my_step(data: Dataset) -&gt; tuple[Model, Metrics]:\n    ...\n</code></pre> Option Type Description <code>name</code> <code>str</code> Unique step name within the pipeline <code>inputs</code> <code>list[str]</code> DAG input artifact names <code>outputs</code> <code>list[str]</code> DAG output artifact names <code>cache</code> <code>bool \\| str \\| Callable</code> Caching strategy <code>retry</code> <code>int</code> Number of retries on failure <code>resources</code> <code>ResourceRequirements</code> CPU/memory/GPU requirements <code>execution_group</code> <code>str</code> Group steps on the same compute node <code>tags</code> <code>dict[str, str]</code> Metadata tags for filtering/observability"},{"location":"integrations/flowyml.html#caching-strategies","title":"Caching Strategies","text":"Strategy Value Use Case Used By Code hash <code>\"code_hash\"</code> Skip if step code hasn't changed <code>load_data</code>, <code>export_model</code>, <code>load_model</code> Input hash <code>\"input_hash\"</code> Skip if inputs haven't changed <code>evaluate_model</code> Disabled <code>False</code> Always re-execute <code>train_model</code>, <code>streaming_load</code> <p>Rule of thumb: Data loading and export steps use <code>code_hash</code> since they're deterministic. Evaluation uses <code>input_hash</code> since results depend on the model. Training always re-runs because model weights are non-deterministic.</p>"},{"location":"integrations/flowyml.html#gpu-resources-execution-groups","title":"GPU Resources &amp; Execution Groups","text":"<p>Steps ship without hardcoded resource requirements so they work out-of-the-box on any hardware. When you need GPU scheduling, simply pass <code>resources</code> and <code>execution_group</code> to the <code>@step</code> decorator \u2014 FlowyML natively supports these:</p> <pre><code>from flowyml.core.step import step\nfrom flowyml.core.resources import ResourceRequirements, GPUConfig\n\n# Option 1: Override when defining your own step\n@step(\n    name=\"my_train\",\n    resources=ResourceRequirements(\n        cpu=\"4\",\n        memory=\"16Gi\",\n        gpu=GPUConfig(gpu_type=\"nvidia-a100\", count=2),\n    ),\n    execution_group=\"training\",\n)\ndef my_train_model(...):\n    ...\n\n# Option 2: Use the predefined step and configure resources at the pipeline level\npipeline.add_step(train_model, resources=ResourceRequirements(...))\n</code></pre> <p>This design gives you full flexibility \u2014 the predefined steps remain portable across CPU-only and GPU environments.</p>"},{"location":"integrations/flowyml.html#context-injection","title":"Context Injection","text":"<p>The <code>Context</code> object is the single entry-point for passing hyperparameters to all steps in a pipeline. FlowyML resolves context values to matching step parameters automatically:</p> <pre><code>from flowyml.core.context import Context\n\nctx = Context(\n    # Data loading params \u2192 resolved to load_data(file_path=, batch_size=, ...)\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    batch_size=32,\n\n    # Training params \u2192 resolved to train_model(epochs=, learning_rate=, ...)\n    epochs=20,\n    learning_rate=0.001,\n    experiment_name=\"v1\",\n\n    # Export params \u2192 resolved to export_model(export_path=, export_format=, ...)\n    export_path=\"models/prod/\",\n    export_format=\"keras\",\n)\n</code></pre> <p>How it works: when a pipeline runs, FlowyML inspects each step's function signature and matches context keys to parameter names. Any matching key is injected as a keyword argument.</p>"},{"location":"integrations/flowyml.html#complete-context-parameter-reference","title":"Complete Context Parameter Reference","text":"Parameter Steps That Use It Framework <code>file_path</code> <code>load_data</code>, <code>load_csv_data</code> All <code>label_name</code> <code>load_data</code>, <code>load_csv_data</code> All <code>batch_size</code> <code>load_data</code>, <code>load_csv_data</code>, <code>optimize_data</code> All <code>column_names</code> <code>load_data</code>, <code>load_csv_data</code> All <code>shuffle</code> <code>load_data</code>, <code>load_csv_data</code> All <code>dtype</code> <code>load_data</code>, <code>load_csv_data</code> All <code>num_workers</code> <code>load_csv_data</code> PyTorch <code>pin_memory</code> <code>load_csv_data</code> PyTorch <code>chunksize</code> <code>load_streaming_csv_data</code> PyTorch <code>shuffle_buffer_size</code> <code>optimize_data</code> TensorFlow <code>prefetch</code> <code>optimize_data</code> TensorFlow <code>data_output_path</code> <code>transform_data</code> Keras, TF <code>epochs</code> <code>train_model</code> All <code>learning_rate</code> <code>train_model</code> All <code>verbose</code> <code>train_model</code>, <code>evaluate_model</code> All <code>optimizer</code> <code>train_model</code> PyTorch <code>loss_fn</code> <code>train_model</code>, <code>evaluate_model</code> PyTorch <code>device</code> <code>train_model</code>, <code>evaluate_model</code>, <code>load_model</code> PyTorch <code>experiment_name</code> <code>train_model</code> Keras, TF <code>project</code> <code>train_model</code> Keras, TF <code>log_model</code> <code>train_model</code> Keras, TF <code>model_path</code> <code>load_model</code> All <code>export_path</code> <code>export_model</code> All <code>export_format</code> <code>export_model</code> All <code>save_path</code> <code>save_model</code> All <code>sample_input</code> <code>export_model</code> PyTorch"},{"location":"integrations/flowyml.html#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"integrations/flowyml.html#cross-framework-reuse","title":"Cross-Framework Reuse","text":"<p>All pipeline templates follow the same factory signature, making it trivial to swap frameworks:</p> <pre><code># The same Context works across frameworks\nctx = Context(\n    file_path=\"data/train.csv\",\n    label_name=\"target\",\n    epochs=20,\n)\n\n# Swap just the import\nfrom mlpotion.integrations.flowyml.keras import create_keras_training_pipeline\nfrom mlpotion.integrations.flowyml.pytorch import create_pytorch_training_pipeline\nfrom mlpotion.integrations.flowyml.tensorflow import create_tf_training_pipeline\n\n# All three work with the same context\nkeras_pipe = create_keras_training_pipeline(context=ctx)\npytorch_pipe = create_pytorch_training_pipeline(context=ctx)\ntf_pipe = create_tf_training_pipeline(context=ctx)\n</code></pre>"},{"location":"integrations/flowyml.html#combining-steps-from-different-frameworks","title":"Combining Steps from Different Frameworks","text":"<p>While not common, you can mix steps from different framework modules in a single pipeline when the artifact types are compatible:</p> <pre><code>from flowyml.core.pipeline import Pipeline\nfrom mlpotion.integrations.flowyml.keras import load_data, train_model\nfrom mlpotion.integrations.flowyml.keras import inspect_model\n\npipeline = Pipeline(\"multi_step\")\npipeline.add_step(load_data)         # Keras data loading\npipeline.add_step(train_model)       # Keras training\npipeline.add_step(inspect_model)     # Keras inspection\n# All compatible because they share the same artifact types\n</code></pre>"},{"location":"integrations/flowyml.html#building-a-custom-step-from-mlpotion-components","title":"Building a Custom Step from MLPotion Components","text":"<p>Create a new FlowyML step from scratch using any MLPotion component:</p> <pre><code>from flowyml.core.step import step\nfrom flowyml import Dataset, Model, Metrics\nfrom mlpotion.frameworks.keras.data.loaders import CSVDataLoader\nfrom mlpotion.frameworks.keras.training.trainers import ModelTrainer\n\n@step(\n    name=\"custom_train_with_augmentation\",\n    inputs=[\"dataset\"],\n    outputs=[\"model\", \"metrics\"],\n    retry=2,\n    tags={\"stage\": \"training\", \"augmentation\": \"enabled\"},\n)\ndef custom_train_with_augmentation(\n    model,\n    data: Dataset,\n    epochs: int = 10,\n    augment: bool = True,\n) -&gt; tuple[Model, Metrics]:\n    \"\"\"Custom training step with data augmentation.\"\"\"\n    raw_data = data.data if isinstance(data, Dataset) else data\n\n    if augment:\n        # Your custom augmentation logic\n        raw_data = apply_augmentation(raw_data)\n\n    trainer = ModelTrainer()\n    result = trainer.train(model=model, dataset=raw_data, config=...)\n\n    return (\n        Model.from_keras(result.model, name=\"augmented_model\"),\n        Metrics.create(metrics=result.metrics, name=\"aug_metrics\"),\n    )\n</code></pre>"},{"location":"integrations/flowyml.html#conditional-flows","title":"Conditional Flows","text":"<p>Use FlowyML's <code>If</code> to add conditional logic to any pipeline:</p> <pre><code>from flowyml.core.conditional import If\nfrom mlpotion.integrations.flowyml.keras import export_model, save_model\n\n# Deploy only if loss is below 0.1\ndeploy_condition = If(\n    condition=lambda metrics: metrics.get_metric(\"loss\", 1.0) &lt; 0.1,\n    then_steps=[export_model, save_model],\n    name=\"deploy_if_loss_low\",\n)\npipeline.control_flows.append(deploy_condition)\n</code></pre>"},{"location":"integrations/flowyml.html#lineage-tracking","title":"Lineage Tracking","text":"<p>Steps that transform data automatically link to their parent via FlowyML's <code>parent=</code> parameter:</p> <pre><code># transform_data and optimize_data both preserve lineage\ntransformed = Dataset.create(\n    data=output_data,\n    name=\"transformed\",\n    parent=input_dataset,    # \u2190 Lineage link\n)\n\n# Query lineage\nprint(transformed.parent)   # \u2192 original Dataset reference\n</code></pre>"},{"location":"integrations/flowyml.html#framework-specific-notes","title":"Framework-Specific Notes","text":""},{"location":"integrations/flowyml.html#keras","title":"Keras","text":"<ul> <li><code>FlowymlKerasCallback</code> auto-captures all training metrics live to the FlowyML dashboard</li> <li>Supports <code>experiment_name</code> and <code>project</code> parameters for experiment tracking</li> <li><code>Model.from_keras()</code> auto-extracts layer counts, parameter counts, and optimizer info</li> <li>Includes <code>transform_data</code> step for model-based data transformation with CSV output</li> <li>Full pipeline includes: <code>load_data \u2192 transform_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></li> </ul>"},{"location":"integrations/flowyml.html#pytorch","title":"PyTorch","text":"<ul> <li>Supports both <code>CSVDataset</code> and <code>StreamingCSVDataset</code> for memory-efficient loading</li> <li><code>Model.from_pytorch()</code> auto-extracts module architecture</li> <li>Export supports <code>torchscript</code> and <code>onnx</code> formats with <code>sample_input</code> for tracing</li> <li>Configurable <code>device</code> parameter (<code>\"cpu\"</code> or <code>\"cuda\"</code>) on train and eval steps</li> <li>Optimizer selection: <code>\"adam\"</code>, <code>\"sgd\"</code>, <code>\"adamw\"</code></li> <li>Full pipeline: <code>load_csv_data \u2192 train_model \u2192 evaluate_model \u2192 export_model \u2192 save_model</code></li> </ul>"},{"location":"integrations/flowyml.html#tensorflow","title":"TensorFlow","text":"<ul> <li>Includes <code>optimize_data</code> step for <code>tf.data.Dataset</code> optimization (prefetch, cache, shuffle)</li> <li>Includes <code>transform_data</code> step for model-based data transformation with CSV output</li> <li>Uses <code>Model.from_keras()</code> since TF models are Keras models under the hood</li> <li>Full pipeline: <code>load_data \u2192 optimize_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></li> </ul>"},{"location":"integrations/flowyml.html#api-reference","title":"API Reference","text":""},{"location":"integrations/flowyml.html#steps-by-framework","title":"Steps by Framework","text":""},{"location":"integrations/flowyml.html#keras-mlpotionintegrationsflowymlkeras","title":"Keras (<code>mlpotion.integrations.flowyml.keras</code>)","text":"Step Inputs Outputs Cache Retry <code>load_data</code> \u2014 <code>dataset</code> <code>code_hash</code> 0 <code>transform_data</code> <code>dataset</code> <code>transformed</code> <code>code_hash</code> 0 <code>train_model</code> <code>dataset</code> <code>model</code>, <code>training_metrics</code> <code>False</code> 1 <code>evaluate_model</code> <code>model</code>, <code>dataset</code> <code>metrics</code> <code>input_hash</code> 0 <code>export_model</code> <code>model</code> <code>exported_model</code> <code>code_hash</code> 0 <code>save_model</code> <code>model</code> <code>saved_model</code> \u2014 0 <code>load_model</code> \u2014 <code>model</code> <code>code_hash</code> 0 <code>inspect_model</code> <code>model</code> <code>inspection</code> \u2014 0"},{"location":"integrations/flowyml.html#pytorch-mlpotionintegrationsflowymlpytorch","title":"PyTorch (<code>mlpotion.integrations.flowyml.pytorch</code>)","text":"Step Inputs Outputs Cache Retry <code>load_csv_data</code> \u2014 <code>dataset</code> <code>code_hash</code> 0 <code>load_streaming_csv_data</code> \u2014 <code>dataset</code> <code>False</code> 0 <code>train_model</code> <code>dataset</code> <code>model</code>, <code>training_metrics</code> <code>False</code> 1 <code>evaluate_model</code> <code>model</code>, <code>dataset</code> <code>metrics</code> <code>input_hash</code> 0 <code>export_model</code> <code>model</code> <code>exported_model</code> <code>code_hash</code> 0 <code>save_model</code> <code>model</code> <code>saved_model</code> \u2014 0 <code>load_model</code> \u2014 <code>model</code> <code>code_hash</code> 0"},{"location":"integrations/flowyml.html#tensorflow-mlpotionintegrationsflowymltensorflow","title":"TensorFlow (<code>mlpotion.integrations.flowyml.tensorflow</code>)","text":"Step Inputs Outputs Cache Retry <code>load_data</code> \u2014 <code>dataset</code> <code>code_hash</code> 0 <code>optimize_data</code> <code>dataset</code> <code>optimized_dataset</code> <code>code_hash</code> 0 <code>transform_data</code> <code>dataset</code> <code>transformed</code> \u2014 0 <code>train_model</code> <code>dataset</code> <code>model</code>, <code>training_metrics</code> <code>False</code> 1 <code>evaluate_model</code> <code>model</code>, <code>dataset</code> <code>metrics</code> <code>input_hash</code> 0 <code>export_model</code> <code>model</code> <code>exported_model</code> <code>code_hash</code> 0 <code>save_model</code> <code>model</code> <code>saved_model</code> \u2014 0 <code>load_model</code> \u2014 <code>model</code> <code>code_hash</code> 0 <code>inspect_model</code> <code>model</code> <code>inspection</code> \u2014 0"},{"location":"integrations/flowyml.html#generic-adapter-mlpotionintegrationsflowymlflowymladapter","title":"Generic Adapter (<code>mlpotion.integrations.flowyml.FlowyMLAdapter</code>)","text":"Factory Method Wraps Returns <code>create_data_loader_step(loader)</code> <code>DataLoader</code> protocol <code>Dataset</code> artifact <code>create_training_step(trainer)</code> <code>ModelTrainer</code> protocol <code>Model</code> artifact <code>create_evaluation_step(evaluator)</code> <code>ModelEvaluator</code> protocol <code>Metrics</code> artifact"},{"location":"integrations/flowyml.html#pipelines-by-framework","title":"Pipelines by Framework","text":"Pipeline Factory DAG Returns <code>create_&lt;fw&gt;_training_pipeline</code> load \u2192 train \u2192 eval <code>Pipeline</code> <code>create_&lt;fw&gt;_full_pipeline</code> load \u2192 [transform|optimize] \u2192 train \u2192 eval \u2192 export <code>Pipeline</code> <code>create_&lt;fw&gt;_evaluation_pipeline</code> load_model \u2192 load_data \u2192 eval [\u2192 inspect] <code>Pipeline</code> <code>create_&lt;fw&gt;_export_pipeline</code> load_model \u2192 export + save <code>Pipeline</code> <code>create_&lt;fw&gt;_experiment_pipeline</code> load \u2192 train \u2192 eval \u2192 [if metric \u2265 threshold] \u2192 export + save <code>Pipeline</code> <code>create_&lt;fw&gt;_scheduled_pipeline</code> load \u2192 train \u2192 eval \u2192 export (+ scheduler) <code>dict</code>"},{"location":"integrations/zenml.html","title":"ZenML Integration \ud83d\udd04","text":"<p>Transform your MLPotion pipelines into production-ready MLOps workflows with ZenML!</p> <p>Important: ZenML is just one integration example. MLPotion is designed to be framework and orchestrator agnostic. You can easily extend MLPotion to work with Prefect, Airflow, Kubeflow, or any other orchestration platform. Community contributions welcome - see Contributing Guide!</p>"},{"location":"integrations/zenml.html#why-zenml","title":"Why ZenML? \ud83e\udd14","text":"<ul> <li>Reproducibility: Track every pipeline run with full lineage</li> <li>Versioning: Automatic artifact and model versioning</li> <li>Collaboration: Share pipelines with your team</li> <li>Scalability: Run on different compute backends</li> <li>Observability: Monitor pipeline health and performance</li> </ul>"},{"location":"integrations/zenml.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code># TensorFlow + ZenML\npoetry add mlpotion -E tensorflow -E zenml\n\n# PyTorch + ZenML\npoetry add mlpotion -E pytorch -E zenml\n\n# Initialize ZenML (first time only)\nzenml init\n</code></pre>"},{"location":"integrations/zenml.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from zenml import pipeline, step\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,\n    train_model,\n    evaluate_model,\n)\nimport keras\n\n# custom model init step\n@step\ndef init_model() -&gt; keras.Model:\n    \"\"\"Initialize the model.\n\n    Note: When using label_name with load_data, the dataset returns\n    (features_dict, labels) where features_dict is a dictionary.\n    The model must accept dict inputs matching the CSV column names.\n    \"\"\"\n    # Create model that accepts dict inputs (matching CSV columns)\n    inputs = {\n        \"feature_1\": keras.Input(shape=(1,), name=\"feature_1\"),\n        \"feature_2\": keras.Input(shape=(1,), name=\"feature_2\"),\n        \"feature_3\": keras.Input(shape=(1,), name=\"feature_3\"),\n    }\n    concatenated = keras.layers.Concatenate()(list(inputs.values()))\n    x = keras.layers.Dense(10, activation=\"relu\")(concatenated)\n    outputs = keras.layers.Dense(1)(x)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n@pipeline\ndef ml_pipeline():\n    \"\"\"Simple ML pipeline with MLPotion + ZenML.\"\"\"\n    # Load data into tf.data.Dataset\n    dataset = load_data(\n        file_path=\"data.csv\",\n        batch_size=32,\n        label_name=\"target\",\n    )\n    # load model\n    model = init_model()\n\n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        dataset=dataset,\n        epochs=10,\n        learning_rate=0.001,\n    )\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=trained_model,\n        dataset=dataset,\n    )\n\n    return metrics\n\n# Run the pipeline\n# Note: ZenML pipelines return a PipelineRunResponse, not the actual return values\nrun = ml_pipeline()\n\n# Access step outputs from the pipeline run\n# Get the step run by step name and load the artifact\nevaluate_step = run.steps[\"evaluate_model\"]\nmetrics = evaluate_step.output.load()  # Use .load() to get the actual value\n\n# Now you can access the metrics dictionary\nprint(f\"Loss: {metrics['loss']:.4f}\")\nprint(f\"MAE: {metrics['mae']:.4f}\")\n</code></pre>"},{"location":"integrations/zenml.html#available-zenml-steps","title":"Available ZenML Steps \ud83d\udce6","text":""},{"location":"integrations/zenml.html#tensorflow-steps","title":"TensorFlow Steps","text":"<pre><code>from mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,          # Load CSV data into tf.data.Dataset\n    optimize_data,      # Optimize dataset for training (caching, prefetch, etc.)\n    transform_data,     # Transform data using model and save predictions\n    train_model,        # Train TensorFlow/Keras model\n    evaluate_model,     # Evaluate model performance\n    save_model,         # Save model to disk\n    load_model,         # Load model from disk\n    export_model,       # Export model for serving (SavedModel, TFLite, etc.)\n    inspect_model,      # Inspect model architecture\n)\n</code></pre>"},{"location":"integrations/zenml.html#pytorch-steps","title":"PyTorch Steps","text":"<pre><code>from mlpotion.integrations.zenml.pytorch.steps import (\n    load_csv_data,              # Load CSV into PyTorch DataLoader\n    load_streaming_csv_data,    # Load large CSV as streaming DataLoader\n    train_model,                # Train PyTorch model\n    evaluate_model,             # Evaluate PyTorch model\n    save_model,                 # Save PyTorch model (state_dict or full)\n    load_model,                 # Load PyTorch model\n    export_model,               # Export model (TorchScript, ONNX, state_dict)\n)\n</code></pre>"},{"location":"integrations/zenml.html#keras-steps","title":"Keras Steps","text":"<pre><code>from mlpotion.integrations.zenml.keras.steps import (\n    load_data,          # Load CSV into CSVSequence\n    transform_data,     # Transform data with predictions\n    train_model,        # Train Keras model\n    evaluate_model,     # Evaluate Keras model\n    save_model,         # Save Keras model\n    load_model,         # Load Keras model\n    export_model,       # Export Keras model\n    inspect_model,      # Inspect Keras model\n)\n</code></pre>"},{"location":"integrations/zenml.html#complete-production-pipeline","title":"Complete Production Pipeline \ud83d\ude80","text":""},{"location":"integrations/zenml.html#tensorflow-example","title":"TensorFlow Example","text":"<pre><code>from zenml import pipeline\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,\n    optimize_data,\n    train_model,\n    evaluate_model,\n    save_model,\n    export_model,\n)\nimport keras\n\n# custom model init step\n@step\ndef init_model() -&gt; keras.Model:\n    \"\"\"Initialize the model.\n\n    Note: When using label_name with load_data, the dataset returns\n    (features_dict, labels) where features_dict is a dictionary.\n    The model must accept dict inputs matching the CSV column names.\n    \"\"\"\n    # Create model that accepts dict inputs (matching CSV columns)\n    inputs = {\n        \"feature_1\": keras.Input(shape=(1,), name=\"feature_1\"),\n        \"feature_2\": keras.Input(shape=(1,), name=\"feature_2\"),\n        \"feature_3\": keras.Input(shape=(1,), name=\"feature_3\"),\n    }\n    concatenated = keras.layers.Concatenate()(list(inputs.values()))\n    x = keras.layers.Dense(10, activation=\"relu\")(concatenated)\n    outputs = keras.layers.Dense(1)(x)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n@pipeline\ndef production_ml_pipeline(\n    train_data: str,\n    test_data: str,\n    model_name: str,\n    epochs: int = 50,\n):\n    \"\"\"Full production ML pipeline for TensorFlow.\"\"\"\n    # Load data\n    train_dataset = load_data(\n        file_path=train_data,\n        batch_size=32,\n        label_name=\"target\",\n    )\n    test_dataset = load_data(\n        file_path=test_data,\n        batch_size=32,\n        label_name=\"target\",\n    )\n\n    # Optimize datasets\n    train_dataset = optimize_data(\n        dataset=train_dataset,\n        batch_size=32,\n        cache=True,\n        prefetch=True,\n        shuffle_buffer_size=1000,\n    )\n    # init model\n    model = init_model()\n\n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        dataset=train_dataset,\n        epochs=epochs,\n        learning_rate=0.001,\n        validation_dataset=test_dataset,\n    )\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=trained_model,\n        dataset=test_dataset,\n    )\n\n    # Save model\n    save_path = save_model(\n        model=trained_model,\n        save_path=f\"models/{model_name}\",\n    )\n\n    # Export for serving\n    export_path = export_model(\n        model=trained_model,\n        export_path=f\"exports/{model_name}\",\n        export_format=\"keras\",\n    )\n\n    return metrics, export_path\n\n\n# Run the pipeline\n# Note: ZenML pipelines return a PipelineRunResponse, not the actual return values\nrun = production_ml_pipeline(\n    train_data=\"s3://bucket/train.csv\",\n    test_data=\"s3://bucket/test.csv\",\n    model_name=\"my-model-v1\",\n    epochs=50,\n)\n\n# Access step outputs from the pipeline run\n# Use .load() to get the actual artifact values\nevaluate_step = run.steps[\"evaluate_model\"]\nmetrics = evaluate_step.output.load()\n\nexport_step = run.steps[\"export_model\"]\nexport_path = export_step.output.load()\n\nprint(f\"Metrics: {metrics}\")\nprint(f\"Export path: {export_path}\")\n</code></pre>"},{"location":"integrations/zenml.html#pytorch-example","title":"PyTorch Example","text":"<pre><code>from zenml import pipeline\nfrom mlpotion.integrations.zenml.pytorch.steps import (\n    load_csv_data,\n    train_model,\n    evaluate_model,\n    export_model,\n)\nimport torch.nn as nn\n\n@step\ndef init_model() -&gt; nn.Module:\n    \"\"\"Initialize the model.\"\"\"\n    model = nn.Sequential(\n        nn.Linear(10, 64),\n        nn.ReLU(),\n        nn.Linear(64, 1),\n    )\n    return model\n\n@pipeline\ndef pytorch_ml_pipeline(\n    train_data: str,\n    test_data: str,\n    model_name: str,\n    epochs: int = 50,\n):\n    \"\"\"Full production ML pipeline for PyTorch.\"\"\"\n    # Load data\n    train_loader = load_csv_data(\n        file_path=train_data,\n        batch_size=32,\n        label_name=\"target\",\n        shuffle=True,\n        num_workers=4,\n    )\n    test_loader = load_csv_data(\n        file_path=test_data,\n        batch_size=32,\n        label_name=\"target\",\n        shuffle=False,\n    )\n    # init model\n    model = init_model()\n\n    # Train model\n    trained_model, train_metrics = train_model(\n        model=model,\n        dataloader=train_loader,\n        epochs=epochs,\n        learning_rate=0.001,\n        device=\"cuda\",\n        validation_dataloader=test_loader,\n    )\n\n    # Evaluate\n    eval_metrics = evaluate_model(\n        model=trained_model,\n        dataloader=test_loader,\n        device=\"cuda\",\n    )\n\n    # Export for serving\n    export_path = export_model(\n        model=trained_model,\n        export_path=f\"exports/{model_name}\",\n        export_format=\"torchscript\",\n        jit_mode=\"script\",\n    )\n\n    return eval_metrics, export_path\n\n# Run the pipeline\n# Note: ZenML pipelines return a PipelineRunResponse, not the actual return values\nrun = pytorch_ml_pipeline(\n    train_data=\"data/train.csv\",\n    test_data=\"data/test.csv\",\n    model_name=\"pytorch-model-v1\",\n    epochs=50,\n)\n\n# Access step outputs from the pipeline run\n# Use .load() to get the actual artifact values\nevaluate_step = run.steps[\"evaluate_model\"]\neval_metrics = evaluate_step.output.load()\n\nexport_step = run.steps[\"export_model\"]\nexport_path = export_step.output.load()\n\nprint(f\"Evaluation metrics: {eval_metrics}\")\nprint(f\"Export path: {export_path}\")\n</code></pre>"},{"location":"integrations/zenml.html#benefits-of-zenml-integration","title":"Benefits of ZenML Integration \ud83c\udf1f","text":""},{"location":"integrations/zenml.html#1-automatic-tracking","title":"1. Automatic Tracking","text":"<p>Every pipeline run is tracked automatically: - Input data versions - Model versions - Hyperparameters - Training metrics - Output artifacts</p>"},{"location":"integrations/zenml.html#2-artifact-caching","title":"2. Artifact Caching","text":"<p>ZenML caches artifacts, so unchanged steps are skipped:</p> <pre><code># First run: All steps execute\nresult1 = ml_pipeline()\n\n# Second run: Only changed steps execute\nresult2 = ml_pipeline()  # Much faster!\n</code></pre>"},{"location":"integrations/zenml.html#3-experiment-comparison","title":"3. Experiment Comparison","text":"<p>Compare different runs:</p> <pre><code># View all pipeline runs\nzenml pipeline runs list\n\n# Compare specific runs\nzenml pipeline runs compare RUN_ID_1 RUN_ID_2\n</code></pre>"},{"location":"integrations/zenml.html#4-model-registry","title":"4. Model Registry","text":"<p>Automatically version and register models:</p> <pre><code>from zenml.integrations.mlflow.model_deployers import MLFlowModelDeployer\n\n# Models are automatically registered\n# Access them via ZenML's model registry\n</code></pre>"},{"location":"integrations/zenml.html#custom-zenml-steps","title":"Custom ZenML Steps \ud83d\udd27","text":"<p>Create your own MLPotion-based ZenML steps:</p> <pre><code>from zenml import step\nfrom mlpotion.frameworks.tensorflow import TFModelTrainer\nimport keras\n\n@step\ndef custom_train_step(\n    model: keras.Model,\n    dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n):\n    \"\"\"Custom training step with MLPotion.\"\"\"\n    trainer = TFModelTrainer()\n\n    compile_params = {\n        \"optimizer\": keras.optimizers.Adam(learning_rate=learning_rate),\n        \"loss\": \"mse\",\n        \"metrics\": [\"mae\"],\n    }\n\n    fit_params = {\n        \"epochs\": epochs,\n        \"verbose\": 1,\n    }\n\n    history = trainer.train(\n        model=model,\n        data=dataset,\n        compile_params=compile_params,\n        fit_params=fit_params,\n    )\n    return model, history\n</code></pre>"},{"location":"integrations/zenml.html#extending-beyond-zenml","title":"Extending Beyond ZenML \ud83d\ude80","text":"<p>MLPotion is not limited to ZenML! The same modular components work with any orchestrator:</p>"},{"location":"integrations/zenml.html#prefect-integration-example","title":"Prefect Integration Example","text":"<pre><code>from prefect import task, flow\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\n@task\ndef load_data_task(file_path: str):\n    loader = TFCSVDataLoader(file_pattern=file_path, label_name=\"target\", batch_size=32)\n    return loader.load()\n\n@task\ndef train_model_task(model, dataset, epochs: int = 10):\n    trainer = TFModelTrainer()\n    compile_params = {\"optimizer\": \"adam\", \"loss\": \"mse\", \"metrics\": [\"mae\"]}\n    fit_params = {\"epochs\": epochs}\n    return trainer.train(model, dataset, compile_params, fit_params)\n\n@flow\ndef ml_flow():\n    dataset = load_data_task(\"data.csv\")\n    result = train_model_task(model, dataset, epochs=10)\n    return result\n</code></pre>"},{"location":"integrations/zenml.html#airflow-integration-example","title":"Airflow Integration Example","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\ndef load_data(**context):\n    loader = TFCSVDataLoader(file_pattern=\"data.csv\", label_name=\"target\", batch_size=32)\n    dataset = loader.load()\n    # Store dataset reference or materialize\n    context['ti'].xcom_push(key='dataset', value=dataset)\n\ndef train_model(**context):\n    dataset = context['ti'].xcom_pull(key='dataset')\n    trainer = TFModelTrainer()\n    # Training logic...\n\nwith DAG('ml_pipeline', ...) as dag:\n    load_task = PythonOperator(task_id='load_data', python_callable=load_data)\n    train_task = PythonOperator(task_id='train_model', python_callable=train_model)\n    load_task &gt;&gt; train_task\n</code></pre> <p>Community contributions welcome! Want to add official support for your favorite orchestrator? See Contributing Guide!</p>"},{"location":"integrations/zenml.html#next-steps","title":"Next Steps \ud83d\uddfa\ufe0f","text":"<ul> <li>ZenML Documentation - Learn more about ZenML</li> <li>API Reference \u2192 - Detailed API docs</li> <li>Contributing Guide \u2192 - Add new integrations</li> </ul> <p> MLPotion: Built for extensibility, works with any orchestrator! \ud83d\udd04 </p>"},{"location":"tutorials/basic-pipeline.html","title":"Your First Pipeline Tutorial \ud83c\udf93","text":"<p>Let's build a complete end-to-end ML pipeline from scratch! This tutorial covers data loading, training, evaluation, and deployment.</p>"},{"location":"tutorials/basic-pipeline.html#what-well-build","title":"What We'll Build \ud83c\udfaf","text":"<p>A regression pipeline that: 1. Loads data from CSV 2. Trains a neural network 3. Evaluates performance 4. Saves the model 5. Exports for serving</p> <p>Time: ~15 minutes Level: Beginner</p>"},{"location":"tutorials/basic-pipeline.html#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<pre><code>poetry add mlpotion -E tensorflow\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-1-prepare-your-data","title":"Step 1: Prepare Your Data \ud83d\udcca","text":"<pre><code># create_data.py\nimport pandas as pd\nimport numpy as np\n\n# Generate synthetic house price data\nnp.random.seed(42)\nn_samples = 10000\n\ndata = pd.DataFrame({\n    'square_feet': np.random.randint(500, 5000, n_samples),\n    'bedrooms': np.random.randint(1, 6, n_samples),\n    'bathrooms': np.random.randint(1, 4, n_samples),\n    'age_years': np.random.randint(0, 100, n_samples),\n    'price': np.random.randint(100000, 1000000, n_samples)\n})\n\n# Add some correlation\ndata['price'] = (\n    data['square_feet'] * 200 +\n    data['bedrooms'] * 10000 +\n    data['bathrooms'] * 15000 -\n    data['age_years'] * 500 +\n    np.random.randn(n_samples) * 50000\n)\n\n# Split into train/test\ntrain_size = int(0.8 * len(data))\ntrain_data = data[:train_size]\ntest_data = data[train_size:]\n\ntrain_data.to_csv('train.csv', index=False)\ntest_data.to_csv('test.csv', index=False)\n\nprint(f\"\u2705 Created {len(train_data)} training samples\")\nprint(f\"\u2705 Created {len(test_data)} test samples\")\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-2-build-the-pipeline","title":"Step 2: Build the Pipeline \ud83c\udfd7\ufe0f","text":"<pre><code># pipeline.py\nimport tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelExporter,\n    ModelTrainingConfig,\n    ModelExportConfig,\n)\n\ndef create_model(input_dim: int) -&gt; tf.keras.Model:\n    \"\"\"Create a simple neural network.\"\"\"\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n\ndef main():\n    print(\"\ud83d\ude80 Starting ML Pipeline...\")\n\n    # 1. Load data\n    print(\"\\n\ud83d\udce5 Loading data...\")\n    train_loader = CSVDataLoader(\n        file_pattern=\"train.csv\",\n        label_name=\"price\",\n        batch_size=32,\n    )\n    test_loader = CSVDataLoader(\n        file_pattern=\"test.csv\",\n        label_name=\"price\",\n        batch_size=32,\n    )\n\n    train_dataset = train_loader.load()\n    test_dataset = test_loader.load()\n\n    # 2. Optimize datasets\n    print(\"\u26a1 Optimizing datasets...\")\n    optimizer = DatasetOptimizer(\n        batch_size=32,\n        cache=True,\n        prefetch=True,\n    )\n\n    train_dataset = optimizer.optimize(train_dataset)\n    test_dataset = optimizer.optimize(test_dataset)\n\n    # 3. Create model\n    print(\"\\n\ud83c\udfd7\ufe0f Creating model...\")\n    model = create_model(input_dim=4)  # 4 features\n\n    # 4. Configure training\n    print(\"\u2699\ufe0f Configuring training...\")\n    config = ModelTrainingConfig(\n        epochs=50,\n        learning_rate=0.001,\n        optimizer_type=\"adam\",\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n        early_stopping=True,\n        early_stopping_patience=10,\n        verbose=1,\n    )\n\n    # 5. Train model\n    print(\"\\n\ud83c\udf93 Training model...\")\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model,\n        train_dataset,\n        config,\n        validation_dataset=test_dataset,\n    )\n\n    print(f\"\\n\u2705 Training complete!\")\n    print(f\"   Final loss: {result.metrics['loss']:.2f}\")\n    print(f\"   Final MAE: {result.metrics['mae']:.2f}\")\n    print(f\"   Best epoch: {result.best_epoch}\")\n    print(f\"   Training time: {result.training_time:.2f}s\")\n\n    # 6. Evaluate\n    print(\"\\n\ud83d\udcca Evaluating model...\")\n    evaluator = ModelEvaluator()\n    eval_result = evaluator.evaluate(result.model, test_dataset, config)\n\n    print(f\"\u2705 Evaluation complete!\")\n    print(f\"   Test MAE: ${eval_result.metrics['mae']:,.2f}\")\n    print(f\"   Test MSE: {eval_result.metrics['mse']:,.2f}\")\n\n    # 7. Save model\n    print(\"\\n\ud83d\udcbe Saving model...\")\n    persistence = ModelPersistence(\n        path=\"models/house_price_model\",\n        model=result.model,\n    )\n    persistence.save(save_format=\".keras\")\n\n    print(\"\u2705 Model saved!\")\n\n    # 8. Export for serving\n    print(\"\\n\ud83d\udce4 Exporting model...\")\n    exporter = ModelExporter()\n    export_config = ModelExportConfig(format=\"saved_model\")\n\n    export_result = exporter.export(\n        result.model,\n        \"exports/house_price_model\",\n        export_config,\n    )\n\n    print(f\"\u2705 Model exported to: {export_result.export_path}\")\n\n    print(\"\\n\ud83c\udf89 Pipeline complete!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline \ud83c\udfc3","text":"<pre><code># Create data\npython create_data.py\n\n# Run pipeline\npython pipeline.py\n</code></pre> <p>You'll see output like:</p> <pre><code>\ud83d\ude80 Starting ML Pipeline...\n\n\ud83d\udce5 Loading data...\n\u26a1 Optimizing datasets...\n\n\ud83c\udfd7\ufe0f Creating model...\n\u2699\ufe0f Configuring training...\n\n\ud83c\udf93 Training model...\nEpoch 1/50\n250/250 [==============================] - 2s 8ms/step - loss: 52345.67 - mae: 178.32 - val_loss: 48234.56 - val_mae: 165.43\n...\nEpoch 25/50\n250/250 [==============================] - 1s 4ms/step - loss: 15234.56 - mae: 98.21 - val_loss: 16543.21 - val_mae: 102.34\n\n\u2705 Training complete!\n   Final loss: 15234.56\n   Final MAE: 98.21\n   Best epoch: 25\n   Training time: 45.23s\n\n\ud83d\udcca Evaluating model...\n\u2705 Evaluation complete!\n   Test MAE: $102.34\n   Test MSE: 16543.21\n\n\ud83d\udcbe Saving model...\n\u2705 Model saved!\n\n\ud83d\udce4 Exporting model...\n\u2705 Model exported to: exports/house_price_model\n\n\ud83c\udf89 Pipeline complete!\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-4-use-the-model","title":"Step 4: Use the Model \ud83d\udd2e","text":"<pre><code># predict.py\nimport tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import ModelPersistence\n\n# Load model\npersistence = ModelPersistence(\n    path=\"models/house_price_model\",\n    model=None,  # Will be loaded\n)\nmodel, metadata = persistence.load()\n\n# Make predictions\nnew_house = [[2500, 3, 2, 10]]  # sq_ft, beds, baths, age\nprediction = model.predict(new_house)\n\nprint(f\"Predicted price: ${prediction[0][0]:,.2f}\")\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#what-you-learned","title":"What You Learned \ud83c\udf93","text":"<ol> <li>\u2705 How to load data from CSV</li> <li>\u2705 How to optimize datasets for performance</li> <li>\u2705 How to configure and train models</li> <li>\u2705 How to evaluate model performance</li> <li>\u2705 How to save and export models</li> </ol>"},{"location":"tutorials/basic-pipeline.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>ZenML Pipeline Tutorial \u2192 - Add MLOps superpowers</li> <li>Advanced Training \u2192 - Custom callbacks and more</li> <li>Production Deployment \u2192 - Ship to production</li> </ul> <p> Congratulations! You've built your first complete ML pipeline! \ud83c\udf89 </p>"},{"location":"tutorials/flowyml-custom-pipeline.html","title":"Custom Pipelines with FlowyML \ud83e\udde9","text":"<p>Learn how to compose custom pipelines by mixing and matching individual MLPotion steps. Go beyond the pre-built templates \u2014 build exactly the workflow your project needs.</p> <p>Time: ~15 minutes Level: Intermediate Prerequisites: Completed the FlowyML Quick Start</p>"},{"location":"tutorials/flowyml-custom-pipeline.html#what-well-build","title":"What We'll Build \ud83c\udfaf","text":"<p>Three custom pipelines that are not available as pre-built templates:</p> <ol> <li>\ud83d\udd0d Train \u2192 Inspect \u2192 Export \u2014 inspect architecture before deploying</li> <li>\ud83d\udd04 Multi-Dataset Evaluation \u2014 evaluate one model against multiple datasets</li> <li>\ud83e\uddea Custom Step Pipeline \u2014 add your own business logic as a FlowyML step</li> </ol>"},{"location":"tutorials/flowyml-custom-pipeline.html#pipeline-1-train-inspect-export","title":"Pipeline 1: Train \u2192 Inspect \u2192 Export \ud83d\udd0d","text":"<p>Sometimes you want to inspect the model architecture before exporting. No template for this? No problem \u2014 build it from individual steps.</p> <pre><code># custom_inspect_pipeline.py\nimport keras\nfrom flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    train_model,\n    inspect_model,\n    export_model,\n)\n\n\ndef create_model() -&gt; keras.Model:\n    model = keras.Sequential([\n        keras.layers.Dense(256, activation=\"relu\", input_shape=(4,)),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(128, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(64, activation=\"relu\"),\n        keras.layers.Dense(1),\n    ])\n    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\n\ndef main():\n    ctx = Context(\n        file_path=\"train.csv\",\n        label_name=\"price\",\n        batch_size=32,\n        epochs=30,\n        experiment_name=\"inspect-before-export\",\n        export_path=\"models/inspected/\",\n        export_format=\"keras\",\n    )\n\n    # Build the custom pipeline from individual steps\n    pipeline = Pipeline(\n        name=\"train_inspect_export\",\n        context=ctx,\n        enable_cache=True,\n        enable_checkpointing=True,\n    )\n\n    pipeline.add_step(load_data)        # \u2192 Dataset artifact\n    pipeline.add_step(train_model)      # \u2192 (Model, Metrics) artifacts\n    pipeline.add_step(inspect_model)    # \u2192 Metrics artifact (architecture)\n    pipeline.add_step(export_model)     # \u2192 Model artifact (exported)\n\n    result = pipeline.run()\n    print(\"\u2705 Custom pipeline complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>DAG: <pre><code>load_data \u2192 train_model \u2192 inspect_model \u2192 export_model\n</code></pre></p> <p>FlowyML auto-wires this because: - <code>train_model</code> outputs <code>\"model\"</code> \u2192 <code>inspect_model</code> takes input <code>\"model\"</code> - <code>inspect_model</code> does NOT consume the model \u2014 it passes through - <code>export_model</code> takes input <code>\"model\"</code> from <code>train_model</code></p>"},{"location":"tutorials/flowyml-custom-pipeline.html#pipeline-2-multi-step-evaluation","title":"Pipeline 2: Multi-Step Evaluation \ud83d\udcca","text":"<p>Evaluate a single model against multiple data splits using separate <code>load_data</code> calls and a shared <code>evaluate_model</code> step:</p> <pre><code># multi_eval_pipeline.py\nfrom flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    load_model,\n    evaluate_model,\n)\n\n\ndef main():\n    # Evaluate an existing model against test data\n    ctx = Context(\n        model_path=\"models/production/model.keras\",\n        file_path=\"test.csv\",\n        label_name=\"price\",\n        batch_size=64,\n    )\n\n    pipeline = Pipeline(\n        name=\"comprehensive_evaluation\",\n        context=ctx,\n        enable_cache=True,\n    )\n\n    pipeline.add_step(load_model)       # \u2192 Model artifact\n    pipeline.add_step(load_data)        # \u2192 Dataset artifact\n    pipeline.add_step(evaluate_model)   # \u2192 Metrics artifact\n\n    result = pipeline.run()\n    print(\"\u2705 Multi-evaluation complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/flowyml-custom-pipeline.html#pipeline-3-custom-step-pipeline","title":"Pipeline 3: Custom Step Pipeline \ud83e\uddea","text":"<p>Add your own business logic as a proper FlowyML step, then compose it with pre-built MLPotion steps:</p> <pre><code># custom_step_pipeline.py\nimport keras\nfrom flowyml.core.step import step\nfrom flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom flowyml import Dataset, Model, Metrics\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    train_model,\n    export_model,\n)\n\n\n# \u2500\u2500\u2500 Your Custom Step \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n@step(\n    name=\"validate_metrics\",\n    inputs=[\"metrics\"],\n    outputs=[\"validation_report\"],\n    tags={\"stage\": \"validation\", \"custom\": \"true\"},\n)\ndef validate_metrics(\n    metrics: Metrics,\n    max_acceptable_loss: float = 1000.0,\n    min_acceptable_mae: float = 200.0,\n) -&gt; Metrics:\n    \"\"\"Custom business logic: validate that metrics meet your criteria.\"\"\"\n    loss = metrics.get_metric(\"loss\", float(\"inf\"))\n    mae = metrics.get_metric(\"mae\", float(\"inf\"))\n\n    report = {\n        \"loss\": loss,\n        \"mae\": mae,\n        \"loss_acceptable\": loss &lt;= max_acceptable_loss,\n        \"mae_acceptable\": mae &lt;= min_acceptable_mae,\n        \"overall_pass\": loss &lt;= max_acceptable_loss and mae &lt;= min_acceptable_mae,\n    }\n\n    return Metrics.create(\n        metrics=report,\n        name=\"validation_report\",\n        tags={\"stage\": \"validation\"},\n        properties=report,\n    )\n\n\n# \u2500\u2500\u2500 Another Custom Step \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n@step(\n    name=\"log_to_slack\",\n    inputs=[\"validation_report\"],\n    outputs=[\"notification_status\"],\n    tags={\"stage\": \"notification\"},\n)\ndef log_to_slack(validation_report: Metrics) -&gt; Metrics:\n    \"\"\"Send validation results to Slack (mock for demo).\"\"\"\n    passed = validation_report.get_metric(\"overall_pass\", False)\n    status = \"\u2705 PASSED\" if passed else \"\u274c FAILED\"\n\n    print(f\"\\n\ud83d\udce2 Slack notification: Model validation {status}\")\n    print(f\"   Loss: {validation_report.get_metric('loss'):.2f}\")\n    print(f\"   MAE:  {validation_report.get_metric('mae'):.2f}\")\n\n    return Metrics.create(\n        metrics={\"notified\": True, \"status\": status},\n        name=\"notification_status\",\n    )\n\n\ndef main():\n    ctx = Context(\n        file_path=\"train.csv\",\n        label_name=\"price\",\n        batch_size=32,\n        epochs=20,\n        learning_rate=0.001,\n        export_path=\"models/validated/\",\n        export_format=\"keras\",\n    )\n\n    pipeline = Pipeline(\n        name=\"custom_validated_pipeline\",\n        context=ctx,\n        enable_cache=True,\n    )\n\n    # Mix pre-built + custom steps\n    pipeline.add_step(load_data)            # MLPotion step\n    pipeline.add_step(train_model)          # MLPotion step\n    pipeline.add_step(validate_metrics)     # Your custom step\n    pipeline.add_step(log_to_slack)         # Your custom step\n    pipeline.add_step(export_model)         # MLPotion step\n\n    result = pipeline.run()\n    print(\"\\n\u2705 Custom validated pipeline complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>DAG: <pre><code>load_data \u2192 train_model \u2192 validate_metrics \u2192 log_to_slack\n                      \u2198 export_model\n</code></pre></p>"},{"location":"tutorials/flowyml-custom-pipeline.html#using-the-flowymladapter-for-custom-components","title":"Using the FlowyMLAdapter for Custom Components \ud83d\udd0c","text":"<p>If you have a custom data loader, trainer, or evaluator that implements MLPotion's protocol interface, wrap it with <code>FlowyMLAdapter</code>:</p> <pre><code>from mlpotion.integrations.flowyml import FlowyMLAdapter\nfrom mlpotion.core.protocols import DataLoader\nfrom flowyml.core.pipeline import Pipeline\n\n\nclass S3DataLoader:\n    \"\"\"Custom loader that reads from S3.\"\"\"\n    def load(self):\n        # Your S3 loading logic\n        import pandas as pd\n        return pd.read_csv(\"s3://my-bucket/data.csv\")\n\n\n# Wrap it as a FlowyML step\ns3_load_step = FlowyMLAdapter.create_data_loader_step(\n    S3DataLoader(),\n    name=\"s3_data_load\",\n    cache=\"code_hash\",\n    tags={\"source\": \"s3\", \"env\": \"production\"},\n)\n\n# Use in a pipeline alongside pre-built steps\nfrom mlpotion.integrations.flowyml.keras import train_model, evaluate_model\n\npipeline = Pipeline(\"s3_pipeline\")\npipeline.add_step(s3_load_step)        # Your custom adapter step\npipeline.add_step(train_model)         # Pre-built MLPotion step\npipeline.add_step(evaluate_model)      # Pre-built MLPotion step\n\nresult = pipeline.run()\n</code></pre>"},{"location":"tutorials/flowyml-custom-pipeline.html#tips-for-custom-pipelines","title":"Tips for Custom Pipelines \ud83d\udca1","text":""},{"location":"tutorials/flowyml-custom-pipeline.html#1-always-declare-inputs-and-outputs","title":"1. Always Declare <code>inputs</code> and <code>outputs</code>","text":"<p>FlowyML needs these to resolve the DAG. If you omit them, steps won't wire automatically:</p> <pre><code># \u2705 Good \u2014 FlowyML can wire this\n@step(name=\"my_step\", inputs=[\"model\"], outputs=[\"processed_model\"])\ndef my_step(model: Model) -&gt; Model: ...\n\n# \u274c Bad \u2014 FlowyML can't auto-wire this\n@step(name=\"my_step\")\ndef my_step(model: Model) -&gt; Model: ...\n</code></pre>"},{"location":"tutorials/flowyml-custom-pipeline.html#2-accept-artifacts-or-raw-objects","title":"2. Accept Artifacts OR Raw Objects","text":"<p>Follow the pattern used by all MLPotion steps \u2014 unwrap artifacts if present:</p> <pre><code>@step(name=\"my_step\", inputs=[\"model\", \"dataset\"], outputs=[\"result\"])\ndef my_step(model, data):\n    # Unwrap if needed\n    raw_model = model.data if isinstance(model, Model) else model\n    raw_data = data.data if isinstance(data, Dataset) else data\n    # ...\n</code></pre>"},{"location":"tutorials/flowyml-custom-pipeline.html#3-use-tags-for-observability","title":"3. Use Tags for Observability","text":"<p>Tags make it easy to filter and search steps in the FlowyML dashboard:</p> <pre><code>@step(\n    name=\"my_step\",\n    tags={\n        \"stage\": \"preprocessing\",\n        \"framework\": \"keras\",\n        \"team\": \"ml-platform\",\n        \"priority\": \"high\",\n    },\n)\n</code></pre>"},{"location":"tutorials/flowyml-custom-pipeline.html#4-use-metricscreate-for-any-custom-metrics","title":"4. Use <code>Metrics.create()</code> for Any Custom Metrics","text":"<p>Any dict of values can be wrapped as a <code>Metrics</code> artifact:</p> <pre><code>from flowyml import Metrics\n\nreport = Metrics.create(\n    metrics={\"f1\": 0.92, \"precision\": 0.95, \"recall\": 0.89},\n    name=\"custom_classification_metrics\",\n    tags={\"model_version\": \"v2\"},\n    properties={\"threshold\": 0.5},\n)\n</code></pre>"},{"location":"tutorials/flowyml-custom-pipeline.html#what-you-learned","title":"What You Learned \ud83c\udf93","text":"<ol> <li>\u2705 How to compose custom pipelines from individual steps</li> <li>\u2705 How to create your own custom FlowyML steps</li> <li>\u2705 How to mix pre-built and custom steps in the same pipeline</li> <li>\u2705 How to use <code>FlowyMLAdapter</code> for custom protocol components</li> <li>\u2705 Best practices for DAG wiring and artifact handling</li> </ol>"},{"location":"tutorials/flowyml-custom-pipeline.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>Experiment Tracking \u2192 \u2014 Conditional deploy + metrics thresholds</li> <li>Scheduled Retraining \u2192 \u2014 Cron pipelines</li> <li>FlowyML Integration Guide \u2192 \u2014 Full API reference</li> </ul> <p> You're now a FlowyML pipeline architect! \ud83c\udfd7\ufe0f </p>"},{"location":"tutorials/flowyml-experiment-tracking.html","title":"Experiment Tracking &amp; Conditional Deployment \ud83e\uddea","text":"<p>Build a production-grade experiment pipeline that trains a model, tracks all metrics live, and only deploys if the model exceeds a quality threshold. This is the pattern used in real MLOps workflows to prevent bad models from reaching production.</p> <p>Time: ~15 minutes Level: Intermediate Prerequisites: Completed the FlowyML Quick Start</p>"},{"location":"tutorials/flowyml-experiment-tracking.html#what-well-build","title":"What We'll Build \ud83c\udfaf","text":"<p>An experiment pipeline that:</p> <ol> <li>\ud83d\udce5 Loads training data</li> <li>\ud83c\udf93 Trains a model with live metric capture via <code>FlowymlKerasCallback</code></li> <li>\ud83d\udcca Evaluates on test data</li> <li>\ud83d\udea6 Conditionally deploys only if accuracy \u2265 85%</li> <li>\ud83d\udcbe Saves + exports the model (only if threshold met)</li> </ol> <pre><code>load_data \u2192 train_model \u2192 evaluate_model\n                                \u2193\n                       [if accuracy \u2265 0.85]\n                                \u2193\n                       export_model \u2192 save_model\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#option-1-using-the-pre-built-template","title":"Option 1: Using the Pre-Built Template \ud83c\udfed","text":"<p>MLPotion provides <code>create_keras_experiment_pipeline</code> that does exactly this:</p> <pre><code># experiment_template.py\nimport keras\nfrom flowyml.core.context import Context\nfrom mlpotion.integrations.flowyml.keras import create_keras_experiment_pipeline\n\n\ndef create_model() -&gt; keras.Model:\n    model = keras.Sequential([\n        keras.layers.Dense(128, activation=\"relu\", input_shape=(4,)),\n        keras.layers.Dense(64, activation=\"relu\"),\n        keras.layers.Dense(1, activation=\"sigmoid\"),  # Binary classification\n    ])\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ndef main():\n    ctx = Context(\n        # Data\n        file_path=\"data/train.csv\",\n        label_name=\"is_fraud\",\n        batch_size=64,\n        # Training\n        epochs=50,\n        learning_rate=0.001,\n        experiment_name=\"fraud-detection-v3\",\n        project=\"fraud-detection\",\n        # Export (only used if threshold met)\n        export_path=\"models/production/fraud_model/\",\n        save_path=\"models/checkpoints/fraud_model.keras\",\n    )\n\n    pipeline = create_keras_experiment_pipeline(\n        name=\"fraud_experiment\",\n        context=ctx,\n        project_name=\"fraud-detection\",\n        deploy_threshold=0.85,        # Only deploy if accuracy \u2265 85%\n        threshold_metric=\"accuracy\",  # Which metric to check\n    )\n\n    result = pipeline.run()\n    print(\"\u2705 Experiment complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#what-happens-under-the-hood","title":"What Happens Under the Hood","text":"<p>The template creates a <code>Pipeline</code> with:</p> <ul> <li><code>enable_experiment_tracking=True</code> \u2014 all metrics are tracked</li> <li><code>enable_checkpointing=True</code> \u2014 pipeline can resume on failure</li> <li>A <code>FlowymlKerasCallback</code> auto-attached to the training step</li> <li>An <code>If</code> conditional flow that gates the export steps</li> </ul>"},{"location":"tutorials/flowyml-experiment-tracking.html#option-2-build-it-yourself","title":"Option 2: Build It Yourself \ud83c\udfd7\ufe0f","text":"<p>For full control, build the conditional pipeline manually:</p> <pre><code># experiment_manual.py\nimport keras\nfrom flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom flowyml.core.conditional import If\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    train_model,\n    evaluate_model,\n    export_model,\n    save_model,\n)\n\n\ndef main():\n    ctx = Context(\n        file_path=\"data/train.csv\",\n        label_name=\"is_fraud\",\n        batch_size=64,\n        epochs=50,\n        experiment_name=\"fraud-detection-manual\",\n        export_path=\"models/production/\",\n        save_path=\"models/checkpoints/model.keras\",\n    )\n\n    pipeline = Pipeline(\n        name=\"fraud_experiment_manual\",\n        context=ctx,\n        enable_cache=False,               # Don't cache \u2014 we want fresh runs\n        enable_experiment_tracking=True,   # Track all metrics\n        enable_checkpointing=True,        # Resume on failure\n    )\n\n    # Core training DAG\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    # Conditional deployment gate\n    deploy_gate = If(\n        condition=lambda metrics: (\n            metrics.get_metric(\"accuracy\", 0) &gt;= 0.85\n            if hasattr(metrics, \"get_metric\")\n            else metrics.get(\"accuracy\", 0) &gt;= 0.85\n        ),\n        then_steps=[export_model, save_model],\n        name=\"deploy_if_accuracy_above_0.85\",\n    )\n    pipeline.control_flows.append(deploy_gate)\n\n    result = pipeline.run()\n\n    # Check what happened\n    print(\"\u2705 Experiment complete!\")\n    # The export/save steps only ran if accuracy \u2265 85%\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#understanding-flowymlkerascallback","title":"Understanding <code>FlowymlKerasCallback</code> \ud83d\udcc8","text":"<p>When you provide <code>experiment_name</code> to <code>train_model</code>, a <code>FlowymlKerasCallback</code> is automatically attached to your training loop. It captures:</p> What How All epoch metrics <code>loss</code>, <code>accuracy</code>, <code>val_loss</code>, <code>val_accuracy</code>, etc. Per-batch metrics If enabled, captures granular training progress Model artifact Optionally logs the model itself after training Training metadata Epochs completed, learning rate, batch size <pre><code># The callback is auto-created inside train_model:\nfrom flowyml.integrations.keras import FlowymlKerasCallback\n\ncallback = FlowymlKerasCallback(\n    experiment_name=\"fraud-detection-v3\",\n    project=\"fraud-detection\",\n    log_model=True,\n)\n# This is attached to the Keras model.fit() call automatically\n</code></pre> <p>You can also add your own callbacks alongside the auto-attached one:</p> <pre><code>from mlpotion.integrations.flowyml.keras import train_model\n\nmodel_asset, metrics_asset = train_model(\n    model=my_model,\n    data=dataset,\n    epochs=50,\n    experiment_name=\"v3\",\n    callbacks=[\n        keras.callbacks.EarlyStopping(patience=10),\n        keras.callbacks.ReduceLROnPlateau(factor=0.5),\n    ],\n)\n# FlowymlKerasCallback is ADDED to your list, not replaced\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#multi-metric-conditional-gates","title":"Multi-Metric Conditional Gates \ud83d\udea6","text":"<p>You can create more complex conditions that check multiple metrics:</p> <pre><code>from flowyml.core.conditional import If\n\n# Gate on multiple metrics\ndeploy_gate = If(\n    condition=lambda metrics: (\n        metrics.get_metric(\"accuracy\", 0) &gt;= 0.85\n        and metrics.get_metric(\"loss\", float(\"inf\")) &lt; 0.3\n        and metrics.get_metric(\"precision\", 0) &gt;= 0.80\n    ),\n    then_steps=[export_model, save_model],\n    name=\"deploy_if_quality_sufficient\",\n)\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#cross-framework-experiments","title":"Cross-Framework Experiments \ud83d\udd00","text":"<p>The same experiment pattern works across all three frameworks. Just swap the imports:</p> KerasPyTorchTensorFlow <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_experiment_pipeline\n\npipeline = create_keras_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\n</code></pre> <pre><code>from mlpotion.integrations.flowyml.pytorch import create_pytorch_experiment_pipeline\n\npipeline = create_pytorch_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\n</code></pre> <pre><code>from mlpotion.integrations.flowyml.tensorflow import create_tf_experiment_pipeline\n\npipeline = create_tf_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.85,\n    threshold_metric=\"accuracy\",\n)\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#experiment-comparison","title":"Experiment Comparison \ud83d\udcca","text":"<p>After running multiple experiments, use FlowyML's built-in tools to compare:</p> <pre><code>from flowyml.core.experiment import ExperimentTracker\n\ntracker = ExperimentTracker(project=\"fraud-detection\")\n\n# List all experiments\nexperiments = tracker.list_experiments()\nfor exp in experiments:\n    print(f\"  {exp.name}: accuracy={exp.metrics.get('accuracy', '?')}\")\n\n# Compare two experiments\ncomparison = tracker.compare(\n    experiment_a=\"fraud-detection-v2\",\n    experiment_b=\"fraud-detection-v3\",\n)\nprint(comparison.summary())\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#best-practices","title":"Best Practices \ud83d\udca1","text":""},{"location":"tutorials/flowyml-experiment-tracking.html#1-always-name-your-experiments","title":"1. Always Name Your Experiments","text":"<p>Use descriptive, versioned names that you can search later:</p> <pre><code>ctx = Context(\n    experiment_name=\"fraud-detection-v3-lr0001-epochs50\",\n    project=\"fraud-detection\",\n)\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#2-use-checkpointing-for-long-training-runs","title":"2. Use Checkpointing for Long Training Runs","text":"<pre><code>pipeline = Pipeline(\n    name=\"long_training\",\n    context=ctx,\n    enable_checkpointing=True,  # Resume from last checkpoint\n)\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#3-set-conservative-initial-thresholds","title":"3. Set Conservative Initial Thresholds","text":"<p>Start with a lower threshold and tighten it as your model improves:</p> <pre><code># Start conservatively\npipeline = create_keras_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.70,   # Low threshold initially\n)\n\n# After baseline is established\npipeline = create_keras_experiment_pipeline(\n    context=ctx,\n    deploy_threshold=0.90,   # Tighter threshold for v2\n)\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#4-combine-with-scheduling","title":"4. Combine with Scheduling","text":"<p>Auto-retrain and conditionally deploy on a schedule:</p> <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_scheduled_pipeline\n\ninfo = create_keras_scheduled_pipeline(\n    context=ctx,\n    schedule=\"0 2 * * 0\",  # Weekly Sunday 2 AM\n)\n\n# The scheduled pipeline will auto-deploy only good models\n# if you add the conditional gate\n</code></pre>"},{"location":"tutorials/flowyml-experiment-tracking.html#what-you-learned","title":"What You Learned \ud83c\udf93","text":"<ol> <li>\u2705 How to use the experiment pipeline template</li> <li>\u2705 How to build manual conditional deployment gates</li> <li>\u2705 How <code>FlowymlKerasCallback</code> auto-captures training metrics</li> <li>\u2705 How to create multi-metric quality gates</li> <li>\u2705 How to compare experiments across runs</li> <li>\u2705 Best practices for production experiment workflows</li> </ol>"},{"location":"tutorials/flowyml-experiment-tracking.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>Scheduled Retraining \u2192 \u2014 Cron-based pipelines</li> <li>Custom Pipelines \u2192 \u2014 Build your own step combinations</li> <li>FlowyML Integration Guide \u2192 \u2014 Full API reference</li> </ul> <p> Your models now deploy themselves \u2014 only when they're good enough! \ud83d\udea6 </p>"},{"location":"tutorials/flowyml-quickstart.html","title":"FlowyML Quick Start Tutorial \ud83d\ude80","text":"<p>Build a complete artifact-centric ML pipeline in under 10 minutes using MLPotion + FlowyML. Unlike a basic standalone pipeline, this approach gives you typed artifacts, automatic metadata, DAG resolution, caching, and lineage tracking \u2014 all out of the box.</p> <p>Time: ~10 minutes Level: Beginner Prerequisites: Python 3.10+</p>"},{"location":"tutorials/flowyml-quickstart.html#what-well-build","title":"What We'll Build \ud83c\udfaf","text":"<p>A house-price regression pipeline that:</p> <ol> <li>\ud83d\udce5 Loads CSV data \u2192 returns a Dataset artifact</li> <li>\ud83c\udf93 Trains a neural network \u2192 returns a Model + Metrics artifact</li> <li>\ud83d\udcca Evaluates performance \u2192 returns a Metrics artifact</li> <li>\ud83d\udcbe Saves the model \u2192 returns a Model artifact with save path</li> </ol> <p>All wired together as a FlowyML DAG with automatic dependency resolution.</p>"},{"location":"tutorials/flowyml-quickstart.html#step-1-install","title":"Step 1: Install \ud83d\udce6","text":"<pre><code>pip install mlpotion[flowyml,keras]\n</code></pre>"},{"location":"tutorials/flowyml-quickstart.html#step-2-prepare-synthetic-data","title":"Step 2: Prepare Synthetic Data \ud83d\udcca","text":"<pre><code># create_data.py\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nn = 10_000\n\ndata = pd.DataFrame({\n    \"square_feet\": np.random.randint(500, 5000, n),\n    \"bedrooms\": np.random.randint(1, 6, n),\n    \"bathrooms\": np.random.randint(1, 4, n),\n    \"age_years\": np.random.randint(0, 100, n),\n})\n\ndata[\"price\"] = (\n    data[\"square_feet\"] * 200\n    + data[\"bedrooms\"] * 10_000\n    + data[\"bathrooms\"] * 15_000\n    - data[\"age_years\"] * 500\n    + np.random.randn(n) * 50_000\n)\n\nsplit = int(0.8 * len(data))\ndata[:split].to_csv(\"train.csv\", index=False)\ndata[split:].to_csv(\"test.csv\", index=False)\n\nprint(f\"\u2705 Created {split} training + {n - split} test samples\")\n</code></pre> <pre><code>python create_data.py\n</code></pre>"},{"location":"tutorials/flowyml-quickstart.html#step-3-build-the-pipeline","title":"Step 3: Build the Pipeline \ud83c\udfd7\ufe0f","text":"<pre><code># flowyml_pipeline.py\nimport keras\nfrom flowyml.core.context import Context\nfrom mlpotion.integrations.flowyml.keras import create_keras_training_pipeline\n\n\ndef create_model(input_dim: int = 4) -&gt; keras.Model:\n    \"\"\"Create a simple regression model.\"\"\"\n    model = keras.Sequential([\n        keras.layers.Dense(128, activation=\"relu\", input_shape=(input_dim,)),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(64, activation=\"relu\"),\n        keras.layers.Dense(1),\n    ])\n    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\n\ndef main():\n    # 1. Define all hyperparameters in a single Context\n    ctx = Context(\n        file_path=\"train.csv\",\n        label_name=\"price\",\n        batch_size=32,\n        epochs=20,\n        learning_rate=0.001,\n        experiment_name=\"house-prices-v1\",\n    )\n\n    # 2. Create the pipeline \u2014 one line!\n    pipeline = create_keras_training_pipeline(\n        name=\"house_price_training\",\n        context=ctx,\n        project_name=\"house-prices\",\n    )\n\n    # 3. Run it\n    print(\"\ud83d\ude80 Running FlowyML pipeline...\")\n    result = pipeline.run()\n\n    # 4. Access the artifacts\n    print(\"\\n\u2705 Pipeline complete!\")\n    print(f\"   Steps executed: {len(result.steps)}\")\n\n    # The pipeline steps return typed artifacts:\n    # - load_data   \u2192 Dataset artifact\n    # - train_model \u2192 (Model, Metrics) artifacts\n    # - evaluate    \u2192 Metrics artifact\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/flowyml-quickstart.html#step-4-run-the-pipeline","title":"Step 4: Run the Pipeline \ud83c\udfc3","text":"<pre><code>python flowyml_pipeline.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\ude80 Running FlowyML pipeline...\n\ud83d\udce6 Loaded dataset: 250 batches, batch_size=32, source=train.csv\n\ud83c\udfaf Training complete: 20 epochs, metrics captured: ['loss', 'mae', ...]\n\ud83d\udcca Evaluation: {'loss': 15234.56, 'mae': 98.21}\n\n\u2705 Pipeline complete!\n   Steps executed: 3\n</code></pre>"},{"location":"tutorials/flowyml-quickstart.html#step-5-understand-what-happened","title":"Step 5: Understand What Happened \ud83d\udd0d","text":""},{"location":"tutorials/flowyml-quickstart.html#the-dag-that-ran","title":"The DAG That Ran","text":"<pre><code>load_data (outputs: dataset)\n    \u2193\ntrain_model (inputs: dataset \u2192 outputs: model, training_metrics)\n    \u2193\nevaluate_model (inputs: model, dataset \u2192 outputs: metrics)\n</code></pre> <p>FlowyML automatically resolved the dependency graph from the <code>inputs</code>/<code>outputs</code> declarations on each step.</p>"},{"location":"tutorials/flowyml-quickstart.html#the-artifacts-created","title":"The Artifacts Created","text":"Step Artifact Type Auto-Extracted Metadata <code>load_data</code> <code>dataset</code> <code>Dataset</code> source, batch_size, batches, label_name <code>train_model</code> <code>model</code> <code>Model</code> layers, parameters, optimizer info <code>train_model</code> <code>training_metrics</code> <code>Metrics</code> loss, mae, epochs, learning_rate <code>evaluate_model</code> <code>metrics</code> <code>Metrics</code> loss, mae (on eval data)"},{"location":"tutorials/flowyml-quickstart.html#caching-in-action","title":"Caching In Action","text":"<p>Run the pipeline again \u2014 notice that <code>load_data</code> is skipped because its <code>cache=\"code_hash\"</code> detects no code changes:</p> <pre><code>python flowyml_pipeline.py\n# \ud83d\udce6 load_data \u2014 CACHED \u2713\n# \ud83c\udfaf Training complete...\n</code></pre>"},{"location":"tutorials/flowyml-quickstart.html#step-6-use-individual-steps-standalone","title":"Step 6: Use Individual Steps Standalone \ud83e\udde9","text":"<p>Every step can also be called directly \u2014 no pipeline required:</p> <pre><code>from mlpotion.integrations.flowyml.keras import load_data, evaluate_model\n\n# Load data standalone\ndataset = load_data(file_path=\"test.csv\", batch_size=64, label_name=\"price\")\nprint(type(dataset))                    # &lt;class 'flowyml.assets.dataset.Dataset'&gt;\nprint(dataset.metadata.properties)      # {'source': 'test.csv', ...}\n\n# Evaluate standalone\nmetrics = evaluate_model(model=trained_model, data=dataset)\nprint(metrics.get_metric(\"mae\"))        # 102.34\n</code></pre>"},{"location":"tutorials/flowyml-quickstart.html#what-you-learned","title":"What You Learned \ud83c\udf93","text":"<ol> <li>\u2705 How to install MLPotion with FlowyML support</li> <li>\u2705 How to configure a pipeline with <code>Context</code></li> <li>\u2705 How to use a pipeline template (<code>create_keras_training_pipeline</code>)</li> <li>\u2705 How FlowyML auto-resolves the DAG from step I/O declarations</li> <li>\u2705 How every step returns typed artifacts with metadata</li> <li>\u2705 How caching skips unchanged steps automatically</li> <li>\u2705 How to use individual steps standalone</li> </ol>"},{"location":"tutorials/flowyml-quickstart.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>Custom Pipelines \u2192 \u2014 Compose your own step combinations</li> <li>Experiment Tracking \u2192 \u2014 Conditional deploy + experiment comparison</li> <li>Scheduled Retraining \u2192 \u2014 Cron-based periodic pipelines</li> <li>FlowyML Integration Guide \u2192 \u2014 Full reference docs</li> </ul> <p> Congratulations \u2014 you've built your first artifact-centric ML pipeline! \ud83c\udf89 </p>"},{"location":"tutorials/flowyml-scheduled-retraining.html","title":"Scheduled Retraining Pipelines \u23f0","text":"<p>Set up automatic periodic retraining for your ML models using FlowyML's built-in scheduler. Keep your models fresh with new data without manual intervention.</p> <p>Time: ~10 minutes Level: Intermediate Prerequisites: Completed the FlowyML Quick Start</p>"},{"location":"tutorials/flowyml-scheduled-retraining.html#what-well-build","title":"What We'll Build \ud83c\udfaf","text":"<p>A scheduled pipeline that:</p> <ol> <li>\u23f0 Runs automatically on a cron schedule (e.g., every Sunday at 2 AM)</li> <li>\ud83d\udce5 Loads the latest training data</li> <li>\ud83c\udf93 Retrains the model from scratch</li> <li>\ud83d\udcca Evaluates on the latest test data</li> <li>\ud83d\udce4 Exports the new model for serving</li> </ol> <p>All with <code>enable_cache=False</code> to ensure fresh data on every run.</p>"},{"location":"tutorials/flowyml-scheduled-retraining.html#option-1-using-the-pre-built-template","title":"Option 1: Using the Pre-Built Template \ud83c\udfed","text":"<p>The fastest way \u2014 one function call:</p> <pre><code># scheduled_pipeline.py\nimport keras\nfrom flowyml.core.context import Context\nfrom mlpotion.integrations.flowyml.keras import create_keras_scheduled_pipeline\n\n\ndef create_model() -&gt; keras.Model:\n    model = keras.Sequential([\n        keras.layers.Dense(128, activation=\"relu\", input_shape=(4,)),\n        keras.layers.Dense(64, activation=\"relu\"),\n        keras.layers.Dense(1),\n    ])\n    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\n\ndef main():\n    ctx = Context(\n        file_path=\"data/latest/train.csv\",   # Points to latest data\n        label_name=\"price\",\n        batch_size=64,\n        epochs=30,\n        learning_rate=0.001,\n        export_path=\"models/production/\",\n        export_format=\"keras\",\n    )\n\n    # Create scheduled pipeline \u2014 returns dict with pipeline + scheduler\n    info = create_keras_scheduled_pipeline(\n        name=\"weekly_retraining\",\n        context=ctx,\n        project_name=\"house-prices\",\n        schedule=\"0 2 * * 0\",    # Every Sunday at 2 AM\n        timezone=\"UTC\",\n    )\n\n    pipeline = info[\"pipeline\"]\n    scheduler = info[\"scheduler\"]\n\n    # Option A: Run once immediately (for testing)\n    print(\"\ud83c\udfc3 Running pipeline once for testing...\")\n    result = pipeline.run()\n    print(f\"\u2705 Test run complete!\")\n\n    # Option B: Start the scheduler for automatic retraining\n    print(\"\\n\u23f0 Starting scheduler (Ctrl+C to stop)...\")\n    scheduler.start()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#what-the-template-does","title":"What the Template Does","text":"<p>The <code>create_keras_scheduled_pipeline</code> factory:</p> <ol> <li>Creates a <code>Pipeline</code> with <code>enable_cache=False</code> (fresh data each run)</li> <li>Enables checkpointing for long-running jobs</li> <li>Adds steps: <code>load_data \u2192 train_model \u2192 evaluate_model \u2192 export_model</code></li> <li>Creates and configures a <code>PipelineScheduler</code></li> <li>Returns both as a dict: <code>{\"pipeline\": ..., \"scheduler\": ...}</code></li> </ol>"},{"location":"tutorials/flowyml-scheduled-retraining.html#option-2-build-it-manually","title":"Option 2: Build It Manually \ud83c\udfd7\ufe0f","text":"<p>For full control over the pipeline and scheduler:</p> <pre><code># scheduled_manual.py\nfrom flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom flowyml.core.scheduler import PipelineScheduler\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    train_model,\n    evaluate_model,\n    export_model,\n    save_model,\n)\n\n\ndef main():\n    ctx = Context(\n        file_path=\"data/latest/train.csv\",\n        label_name=\"price\",\n        batch_size=64,\n        epochs=30,\n        export_path=\"models/production/\",\n        save_path=\"models/archive/model_latest.keras\",\n    )\n\n    # Build the pipeline\n    pipeline = Pipeline(\n        name=\"manual_scheduled_retraining\",\n        context=ctx,\n        enable_cache=False,           # Always use fresh data\n        enable_checkpointing=True,    # Resume on failure\n        project_name=\"house-prices\",\n    )\n\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n    pipeline.add_step(export_model)\n    pipeline.add_step(save_model)      # Also save a backup\n\n    # Configure the scheduler\n    scheduler = PipelineScheduler()\n    scheduler.schedule(\n        pipeline=pipeline,\n        cron=\"0 2 * * 0\",     # Every Sunday at 2 AM\n        timezone=\"UTC\",\n    )\n\n    # Start\n    print(\"\u23f0 Scheduler registered. Starting...\")\n    scheduler.start()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#cron-expression-reference","title":"Cron Expression Reference \ud83d\udcc5","text":"<p>The <code>schedule</code> parameter uses standard cron syntax:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0-59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0-23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 day of month (1-31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500 month (1-12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500 day of week (0-6, Sunday=0)\n\u2502 \u2502 \u2502 \u2502 \u2502\n* * * * *\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#common-patterns","title":"Common Patterns","text":"Schedule Cron Expression Use Case Every Sunday 2 AM <code>0 2 * * 0</code> Weekly retraining Every day midnight <code>0 0 * * *</code> Daily retraining Every 6 hours <code>0 */6 * * *</code> Real-time model freshness Every Monday &amp; Thursday 3 AM <code>0 3 * * 1,4</code> Bi-weekly retraining First day of month 1 AM <code>0 1 1 * *</code> Monthly retraining Every 15 minutes <code>*/15 * * * *</code> Frequent updates (streaming data) Weekdays only 6 AM <code>0 6 * * 1-5</code> Business-hours retraining"},{"location":"tutorials/flowyml-scheduled-retraining.html#cross-framework-scheduled-pipelines","title":"Cross-Framework Scheduled Pipelines \ud83d\udd00","text":"<p>All three frameworks support scheduled pipelines with the same interface:</p> KerasPyTorchTensorFlow <pre><code>from mlpotion.integrations.flowyml.keras import create_keras_scheduled_pipeline\n\ninfo = create_keras_scheduled_pipeline(\n    context=ctx,\n    schedule=\"0 2 * * 0\",\n)\n</code></pre> <pre><code>from mlpotion.integrations.flowyml.pytorch import create_pytorch_scheduled_pipeline\n\ninfo = create_pytorch_scheduled_pipeline(\n    context=ctx,\n    schedule=\"0 2 * * 0\",\n)\n</code></pre> <pre><code>from mlpotion.integrations.flowyml.tensorflow import create_tf_scheduled_pipeline\n\ninfo = create_tf_scheduled_pipeline(\n    context=ctx,\n    schedule=\"0 2 * * 0\",\n)\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#combining-scheduling-with-conditional-deployment","title":"Combining Scheduling with Conditional Deployment \ud83d\udea6","text":"<p>The most powerful pattern: schedule retraining + only deploy good models.</p> <pre><code>from flowyml.core.context import Context\nfrom flowyml.core.pipeline import Pipeline\nfrom flowyml.core.scheduler import PipelineScheduler\nfrom flowyml.core.conditional import If\nfrom mlpotion.integrations.flowyml.keras import (\n    load_data,\n    train_model,\n    evaluate_model,\n    export_model,\n    save_model,\n)\n\n\ndef main():\n    ctx = Context(\n        file_path=\"data/latest/train.csv\",\n        label_name=\"is_fraud\",\n        batch_size=64,\n        epochs=50,\n        experiment_name=\"fraud-scheduled\",\n        export_path=\"models/production/\",\n        save_path=\"models/archive/latest.keras\",\n    )\n\n    pipeline = Pipeline(\n        name=\"scheduled_conditional_retrain\",\n        context=ctx,\n        enable_cache=False,\n        enable_experiment_tracking=True,\n        enable_checkpointing=True,\n    )\n\n    # Training DAG\n    pipeline.add_step(load_data)\n    pipeline.add_step(train_model)\n    pipeline.add_step(evaluate_model)\n\n    # Only deploy if accuracy \u2265 90%\n    deploy_gate = If(\n        condition=lambda m: m.get_metric(\"accuracy\", 0) &gt;= 0.90,\n        then_steps=[export_model, save_model],\n        name=\"deploy_if_accuracy_above_0.90\",\n    )\n    pipeline.control_flows.append(deploy_gate)\n\n    # Schedule weekly\n    scheduler = PipelineScheduler()\n    scheduler.schedule(pipeline=pipeline, cron=\"0 2 * * 0\", timezone=\"UTC\")\n\n    print(\"\u23f0 Scheduled weekly retraining with quality gate\")\n    print(\"   \u2192 Models only deploy if accuracy \u2265 90%\")\n    scheduler.start()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This gives you: - \ud83d\udd04 Automatic weekly retraining - \ud83d\udcc8 Full experiment tracking on every run - \ud83d\udea6 Quality gate \u2014 bad models never reach production - \u267b\ufe0f Checkpointing \u2014 long runs resume on failure - \ud83d\udcca Metrics history across all scheduled runs</p>"},{"location":"tutorials/flowyml-scheduled-retraining.html#monitoring-scheduled-runs","title":"Monitoring Scheduled Runs \ud83d\udcca","text":""},{"location":"tutorials/flowyml-scheduled-retraining.html#check-run-history","title":"Check Run History","text":"<pre><code>from flowyml.core.experiment import ExperimentTracker\n\ntracker = ExperimentTracker(project=\"fraud-detection\")\nruns = tracker.list_experiments()\n\nfor run in runs:\n    print(\n        f\"  {run.name} | \"\n        f\"accuracy={run.metrics.get('accuracy', '?')} | \"\n        f\"deployed={run.metrics.get('deployed', False)}\"\n    )\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#pipeline-logs","title":"Pipeline Logs","text":"<p>Each scheduled run produces its own logs with step-by-step output:</p> <pre><code>[2026-02-09 02:00:00] \u23f0 Scheduled run started: weekly_retraining\n[2026-02-09 02:00:01] \ud83d\udce6 Loaded dataset: 500 batches, source=data/latest/train.csv\n[2026-02-09 02:03:45] \ud83c\udfaf Training complete: 50 epochs\n[2026-02-09 02:03:47] \ud83d\udcca Evaluation: {accuracy: 0.93, loss: 0.12}\n[2026-02-09 02:03:47] \ud83d\udea6 Accuracy 0.93 \u2265 0.90 \u2192 deploying\n[2026-02-09 02:03:48] \ud83d\udce4 Exported model to: models/production/\n[2026-02-09 02:03:48] \ud83d\udcbe Saved model to: models/archive/latest.keras\n[2026-02-09 02:03:48] \u2705 Scheduled run complete!\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#production-deployment-tips","title":"Production Deployment Tips \ud83d\udca1","text":""},{"location":"tutorials/flowyml-scheduled-retraining.html#1-use-a-process-manager","title":"1. Use a Process Manager","text":"<p>For production, run the scheduler with a process manager like <code>supervisord</code>:</p> <pre><code># supervisord.conf\n[program:model_retraining]\ncommand=python scheduled_pipeline.py\ndirectory=/app\nautostart=true\nautorestart=true\nstderr_logfile=/var/log/retraining.err.log\nstdout_logfile=/var/log/retraining.out.log\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#2-point-to-dynamic-data-sources","title":"2. Point to Dynamic Data Sources","text":"<p>Don't hardcode file paths. Use symlinks or dynamic paths:</p> <pre><code>ctx = Context(\n    file_path=\"data/latest/train.csv\",  # Symlinked to newest data\n    # Or use a date pattern:\n    # file_path=f\"data/{datetime.now().strftime('%Y-%m')}/train.csv\",\n)\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#3-always-enable-checkpointing","title":"3. Always Enable Checkpointing","text":"<pre><code>pipeline = Pipeline(\n    name=\"scheduled_retraining\",\n    context=ctx,\n    enable_checkpointing=True,  # \u2190 Always set this for scheduled pipelines\n)\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#4-combine-with-alerting","title":"4. Combine with Alerting","text":"<p>Add a notification step for failed runs:</p> <pre><code>@step(name=\"alert_on_failure\", tags={\"stage\": \"alerting\"})\ndef alert_on_failure(metrics):\n    if metrics.get_metric(\"accuracy\", 0) &lt; 0.70:\n        send_slack_alert(\"\u26a0\ufe0f Model accuracy dropped below 70%!\")\n</code></pre>"},{"location":"tutorials/flowyml-scheduled-retraining.html#what-you-learned","title":"What You Learned \ud83c\udf93","text":"<ol> <li>\u2705 How to create scheduled pipelines with cron syntax</li> <li>\u2705 How to build manual scheduled pipelines</li> <li>\u2705 Cron expression patterns for common schedules</li> <li>\u2705 How to combine scheduling with conditional deployment</li> <li>\u2705 How to monitor scheduled runs</li> <li>\u2705 Production deployment best practices</li> </ol>"},{"location":"tutorials/flowyml-scheduled-retraining.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>Custom Pipelines \u2192 \u2014 Build your own step combinations</li> <li>Experiment Tracking \u2192 \u2014 Conditional deploy + metrics</li> <li>FlowyML Integration Guide \u2192 \u2014 Full API reference</li> </ul> <p> Your models now retrain themselves on schedule! \u23f0\ud83e\udd16 </p>"},{"location":"tutorials/multi-framework.html","title":"Multi-Framework Tutorial \ud83d\udd00","text":"<p>Learn how to use the same MLPotion components across TensorFlow, PyTorch, and Keras!</p>"},{"location":"tutorials/multi-framework.html#the-power-of-protocols","title":"The Power of Protocols \ud83d\udcaa","text":"<p>MLPotion's protocol-based design means you can switch frameworks with minimal code changes:</p> <pre><code># Same pattern, different framework!\n\n# TensorFlow\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader\nloader = TFCSVDataLoader(\"data.csv\", label_name=\"target\")\ndataset = loader.load()  # Returns tf.data.Dataset\n\n# PyTorch\nfrom mlpotion.frameworks.pytorch import PyTorchCSVDataset\ndataset = PyTorchCSVDataset(\"data.csv\", label_name=\"target\")\n\n# Keras\nfrom mlpotion.frameworks.keras import KerasCSVDataLoader\nloader = KerasCSVDataLoader(\"data.csv\", label_name=\"target\")\ndataset = loader.load()  # Returns keras dataset\n</code></pre>"},{"location":"tutorials/multi-framework.html#framework-agnostic-code","title":"Framework-Agnostic Code \ud83c\udfaf","text":"<p>Write code that works with any framework:</p> <pre><code>from typing import Protocol\nfrom mlpotion.core.protocols import DataLoader, ModelTrainer\n\ndef train_model(\n    loader: DataLoader,\n    trainer: ModelTrainer,\n    config,\n):\n    \"\"\"Works with TensorFlow, PyTorch, or Keras!\"\"\"\n    dataset = loader.load()\n    result = trainer.train(model, dataset, config)\n    return result\n</code></pre>"},{"location":"tutorials/multi-framework.html#switching-frameworks-mid-project","title":"Switching Frameworks Mid-Project \ud83d\udd04","text":"<pre><code># Start with TensorFlow for prototyping\nfrom mlpotion.frameworks.tensorflow import *\n\n# Later: Switch to PyTorch for custom research\nfrom mlpotion.frameworks.pytorch import *\n\n# Finally: Deploy with Keras for production\nfrom mlpotion.frameworks.keras import *\n</code></pre> <p>The same MLPotion patterns (to some possible extents) work everywhere!</p> <p> One API, Multiple Frameworks! \ud83c\udfa8 </p>"},{"location":"tutorials/zenml-integration.html","title":"ZenML Pipeline Tutorial \ud83d\udd04","text":"<p>Transform your MLPotion pipeline into a production-ready MLOps workflow with ZenML tracking, versioning, and reproducibility!</p> <p>Note: ZenML is just one integration example. MLPotion is designed to be orchestrator-agnostic and works with Prefect, Airflow, Kubeflow, and any other orchestration platform. See ZenML Integration Guide for extending to other orchestrators.</p>"},{"location":"tutorials/zenml-integration.html#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<pre><code>poetry add mlpotion -E tensorflow -E zenml\nzenml init\n</code></pre>"},{"location":"tutorials/zenml-integration.html#converting-your-pipeline","title":"Converting Your Pipeline \ud83d\udd04","text":""},{"location":"tutorials/zenml-integration.html#before-standalone-pipeline","title":"Before: Standalone Pipeline","text":"<pre><code>\"\"\"Basic TensorFlow usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion TensorFlow workflow:\n1. Load data from CSV\n2. Optimize dataset for performance\n3. Create a TensorFlow model\n4. Train the model\n5. Evaluate the model\n6. Save and export the model\n\"\"\"\n\nimport tensorflow as tf\n\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\ndef main() -&gt; None:\n    \"\"\"Run basic TensorFlow training pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow Basic Usage\")\n    print(\"=\" * 60)\n\n    # 1. Load data\n    print(\"\\n1. Loading data...\")\n    loader = CSVDataLoader(\n        file_pattern=\"examples/data/sample.csv\",\n        label_name=\"target\",\n        batch_size=1,  # Load unbatched, let DatasetOptimizer handle batching\n    )\n    dataset = loader.load()\n    print(f\"Dataset: {dataset}\")\n\n    # Unbatch the dataset first (since CSVDataLoader batches by default)\n    dataset = dataset.unbatch()\n\n    # Transform OrderedDict to single tensor\n    def prepare_features(features, label):\n        \"\"\"Convert OrderedDict of features to single tensor.\"\"\"\n        feature_list = [features[key] for key in sorted(features.keys())]\n        stacked_features = tf.stack(feature_list, axis=-1)\n        return stacked_features, label\n\n    dataset = dataset.map(prepare_features)\n\n    # 2. Optimize dataset\n    print(\"\\n2. Optimizing dataset...\")\n    optimizer = DatasetOptimizer(batch_size=8, shuffle_buffer_size=100)\n    dataset = optimizer.optimize(dataset)\n\n    # 3. Create model\n    print(\"\\n3. Creating model...\")\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n        ]\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n    print(model.summary())\n\n    # 4. Train model\n    print(\"\\n4. Training model...\")\n    trainer = ModelTrainer()\n    config = ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n    result = trainer.train(\n        model=model,\n        data=dataset,\n        config=config,\n    )\n\n    print(\"\\nTraining completed!\")\n    print(f\"{result=}\")\n\n    # 5. Evaluate model\n    print(\"\\n5. Evaluating model...\")\n    evaluator = ModelEvaluator()\n    from mlpotion.frameworks.tensorflow import ModelEvaluationConfig\n\n    eval_config = ModelEvaluationConfig(batch_size=8, verbose=1)\n    eval_result = evaluator.evaluate(\n        model=model,\n        data=dataset,\n        config=eval_config,\n    )\n    print(f\"{eval_result=}\")\n\n    # 6. Save model\n    print(\"\\n6. Saving model...\")\n    model_path = \"/tmp/tensorflow_model.keras\"\n    persistence = ModelPersistence(\n        path=model_path,\n        model=model,\n    )\n    persistence.save(\n        save_format=\".keras\",\n    )\n    print(f\"Model saved to: {model_path}\")\n\n    # 7. Load model\n    print(\"\\n7. Loading model...\")\n    loaded_model, metadata = persistence.load()\n    print(f\"Model loaded successfully: {type(loaded_model)}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/zenml-integration.html#after-zenml-pipeline","title":"After: ZenML Pipeline","text":"<pre><code>\"\"\"TensorFlow training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's TensorFlow components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\n\nimport tensorflow as tf\nfrom zenml import pipeline, step\n\nfrom mlpotion.frameworks.tensorflow import ModelTrainingConfig\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    evaluate_model,\n    export_model,\n    load_data,\n    optimize_data,\n    save_model,\n    train_model,\n)\n\n\n@step(enable_cache=False)  # Disable caching to ensure fresh model\ndef create_model() -&gt; tf.keras.Model:\n    \"\"\"Create and compile a TensorFlow model that accepts dict inputs.\n\n    Returns:\n        Compiled TensorFlow/Keras model ready for training.\n    \"\"\"\n    # Create inputs for each feature (10 features: feature_0 to feature_9)\n    # After batching, make_csv_dataset produces tensors with shape (batch_size,) for each scalar feature\n    # The materializer now correctly preserves this shape as (None,) where None is the batch dimension\n    inputs = {}\n    feature_list = []\n\n    for i in range(10):\n        # Each input has shape (1,) per sample after batching and materializer roundtrip\n        # The materializer preserves the concrete shape (batch_size, 1)\n        inp = tf.keras.Input(shape=(1,), name=f\"feature_{i}\", dtype=tf.float32)\n        inputs[f\"feature_{i}\"] = inp\n        # Already shape (batch_size, 1), no need to reshape\n        feature_list.append(inp)\n\n    # Concatenate all features along the last axis\n    # This will create shape (batch_size, 10)\n    concatenated = tf.keras.layers.Concatenate(axis=-1)(feature_list)\n\n    # Build the model architecture\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(concatenated)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(1)(x)\n\n    # Create the functional model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n\n    return model\n\n\n@step\ndef create_training_config() -&gt; ModelTrainingConfig:\n    \"\"\"Create training configuration.\n\n    Returns:\n        Training configuration with hyperparameters.\n    \"\"\"\n    return ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n\n@pipeline(enable_cache=False)\ndef tensorflow_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/tensorflow_model.keras\",\n    export_path: str = \"/tmp/tensorflow_model_export\",\n):\n    \"\"\"Complete TensorFlow training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Optimize dataset for performance\n    3. Create and configure model\n    4. Train model\n    5. Evaluate model\n    6. Save model\n    7. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataset = load_data(\n        file_path=file_path,\n        batch_size=1,\n        label_name=label_name,\n    )\n\n    # Step 2: Optimize dataset\n    optimized_dataset = optimize_data(\n        dataset=dataset,\n        batch_size=8,\n        shuffle_buffer_size=100,\n    )\n\n    # Step 3: Create model and config\n    model = create_model()\n\n    # Step 4: Train model\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    trained_model, training_metrics = train_model(\n        model=model,\n        dataset=optimized_dataset,\n        **_config_train,\n    )\n\n    # Step 5: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=trained_model,\n        dataset=optimized_dataset,\n    )\n\n    # Step 6: Save model\n    save_model(\n        model=trained_model,\n        save_path=model_save_path,\n    )\n\n    # Step 7: Export model for serving\n    export_model(\n        model=trained_model,\n        export_path=export_path,\n        export_format=\"keras\",\n    )\n\n    return trained_model, training_metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the TensorFlow ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = tensorflow_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre>"},{"location":"tutorials/zenml-integration.html#benefits-you-get","title":"Benefits You Get \ud83c\udf1f","text":"<ul> <li>\u2705 Automatic artifact versioning</li> <li>\u2705 Full pipeline lineage tracking</li> <li>\u2705 Experiment comparison</li> <li>\u2705 Model registry integration</li> <li>\u2705 Reproducible runs</li> <li>\u2705 Caching of unchanged steps</li> <li>\u2705 Step output tracking with metadata</li> </ul>"},{"location":"tutorials/zenml-integration.html#viewing-pipeline-runs","title":"Viewing Pipeline Runs \ud83d\udd0d","text":"<pre><code># List all pipeline runs\nzenml pipeline runs list\n\n# View details of a specific run\nzenml pipeline runs describe &lt;RUN_ID&gt;\n\n# Compare different runs\nzenml pipeline runs compare &lt;RUN_ID_1&gt; &lt;RUN_ID_2&gt;\n</code></pre> <p>See the ZenML Integration Guide for complete documentation and examples with PyTorch and Keras!</p> <p> Ready for production MLOps! \ud83d\ude80 </p>"}]}