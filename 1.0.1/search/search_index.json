{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"MLPotion: Brew Your ML Magic! \ud83e\uddea\u2728","text":"<p> <p>Provided and maintained by \ud83e\udd84 UnicoLab</p> </p> <p>Welcome, fellow alchemist! \ud83e\uddd9\u200d\u2642\ufe0f Ready to brew some machine learning magic without getting locked in a cauldron?</p> <p>MLPotion is your chest of modular, mix-and-match ML building blocks that work across Keras, TensorFlow, and PyTorch. Think of it as LEGO\u00ae for ML pipelines, but with fewer foot injuries and more flexibility!</p>"},{"location":"index.html#why-mlpotion","title":"Why MLPotion? \ud83e\udd14","text":"<p>Ever felt trapped by a framework that forces you to do things \"their way\"? We've been there. That's why we created MLPotion:</p> <ul> <li>\ud83c\udfaf Framework Agnostic: Write once, run anywhere (well, on Keras, TensorFlow, or PyTorch)</li> <li>\ud83e\uddf1 Modular by Design: Pick the pieces you need, leave the rest in the box</li> <li>\ud83d\udd2c Type-Safe: Python 3.10+ typing that actually helps you (mypy approved!)</li> <li>\ud83d\ude80 Production Ready: Built for the real world, not just notebooks</li> <li>\ud83c\udfa8 Orchestration Flexible: Works standalone OR with ZenML, Prefect, Airflow - your choice!</li> <li>\ud83d\udce6 Install What You Need: Core package works without any ML frameworks (you only install what you need)!</li> <li>\ud83e\udd1d Community-Driven: Missing something? Contribute it back - we love community additions!</li> </ul>"},{"location":"index.html#whats-in-the-potion","title":"What's in the Potion? \ud83e\uddea","text":"\u2697\ufe0f Core Ingredients <ul> <li>Type-safe protocols for all components</li> <li>Framework-agnostic result types</li> <li>Consistent error handling</li> <li>Zero-dependency core package</li> </ul> \ud83d\udd27 Framework Support <ul> <li>Keras 3.0+ - The friendly one</li> <li>TensorFlow 2.15+ - The production workhorse</li> <li>PyTorch 2.0+ - The researcher's favorite</li> </ul> \ud83d\udcca Data Processing <ul> <li>CSV loaders for all frameworks</li> <li>Dataset optimization utilities</li> <li>Data transformers</li> <li>Preprocessing pipelines</li> </ul> \ud83c\udf93 Training &amp; Evaluation <ul> <li>Unified training interface</li> <li>Comprehensive evaluation tools</li> <li>Rich result objects</li> <li>Training history tracking</li> </ul> \ud83d\udcbe Model Management <ul> <li>Save/load model checkpoints</li> <li>Export to production formats</li> <li>Model inspection utilities</li> <li>Multiple export formats</li> </ul> \ud83d\udd04 Orchestration Integration <ul> <li>ZenML integration built-in</li> <li>Extensible to Prefect, Airflow, etc.</li> <li>Works standalone (no orchestration needed!)</li> <li>Community contributions welcome</li> </ul>"},{"location":"index.html#the-mlpotion-philosophy","title":"The MLPotion Philosophy \ud83c\udfad","text":"<p>\"A good potion doesn't force you to drink it a certain way. It just... works.\"</p> <p>\u2014 Ancient ML Alchemist Proverb (we just made that up)</p> <p>We believe in:</p> <ol> <li>Flexibility &gt; Convention: Your project, your rules</li> <li>Simplicity &gt; Complexity: If it's hard to use, we failed</li> <li>Type Safety &gt; Runtime Surprises: Catch errors before they bite</li> <li>Modularity &gt; Monoliths: Use what you need, ignore the rest</li> <li>Consistency &gt; Chaos: Same patterns across all frameworks</li> <li>Community &gt; Corporate: Built by the community, for the community</li> </ol>"},{"location":"index.html#extensibility-community-contributions","title":"Extensibility &amp; Community Contributions \ud83c\udf1f","text":"<p>MLPotion is designed to be extensible. While we provide ZenML integration out-of-the-box, you can easily integrate with:</p> <ul> <li>Prefect: Wrap components as Prefect tasks</li> <li>Airflow: Use as operators in DAGs</li> <li>Kubeflow: Deploy in Kubeflow pipelines</li> <li>Your Custom Orchestrator: The building blocks work anywhere!</li> </ul> <p>Missing a feature? We actively encourage community contributions! Whether it's:</p> <ul> <li>A new data loader (Parquet, Avro, databases)</li> <li>Integration with another orchestration framework</li> <li>Framework-specific optimizations</li> <li>New export formats</li> </ul> <p>Your contributions help everyone. Check out our Contributing Guide to get started!</p>"},{"location":"index.html#whos-this-for","title":"Who's This For? \ud83c\udfaf","text":"<p>You'll love MLPotion if you:</p> <ul> <li>Switch between frameworks and hate rewriting everything</li> <li>Value heavily tested code that you can reuse</li> <li>Value type safety and IDE autocomplete (who doesn't?)</li> <li>Want production-ready code without enterprise bloat</li> <li>Believe ML pipelines should be composable and testable</li> </ul> <p>You might want something else if you:</p> <ul> <li>Do not like modularity</li> <li>Do not like reusability</li> <li>Are too lazy to contribute something that you can't already find here</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started \ud83d\ude80","text":"<p>Ready to start brewing? Here's your path:</p> 1 \ud83d\udce5 Install MLPotion <p>Choose your framework flavor</p> Installation Guide \u2192 2 \u26a1 Quick Start <p>Get up and running in 5 minutes</p> Quick Start \u2192 3 \ud83e\udde0 Learn Concepts <p>Understand the architecture</p> Core Concepts \u2192 4 \ud83c\udfa8 Build Pipelines <p>Create your first pipeline</p> First Pipeline \u2192"},{"location":"index.html#show-me-the-code","title":"Show Me the Code! \ud83d\udcbb","text":""},{"location":"index.html#standalone-usage-framework-only","title":"Standalone Usage (Framework-Only)","text":"KerasTensorFlowPyTorch <pre><code>\"\"\"Basic Keras usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion Keras workflow:\n1. Load data from CSV\n2. Create a Keras model\n3. Train the model\n4. Evaluate the model\n5. Save and export the model\n\"\"\"\n\nimport tensorflow as tf\nfrom loguru import logger\n\nfrom mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\n# =================== METHODS ===========================\ndef create_model(input_dim: int = 10) -&gt; tf.keras.Model:\n    \"\"\"Create a simple feedforward neural network.\n\n    Args:\n        input_dim: Number of input features.\n\n    Returns:\n        Compiled Keras model.\n    \"\"\"\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n        ]\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n    return model\n\n\n# =================== MAIN ===========================\n\n# 1. Load data (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n1. Loading data from CSV...\")\nloader = CSVDataLoader(\n    file_pattern=\"docs/examples/data/sample.csv\",\n    label_name=\"target\",\n    batch_size=8,\n    shuffle=True,\n)\ndataset = loader.load()\nlogger.info(f\"Dataset created: {dataset}\")\n\n# 2. Create model (CUSTOM)\nlogger.info(\"\\n2. Creating Keras model...\")\nmodel = create_model(input_dim=10)\nlogger.info(model.summary())\n\n# 3. Train model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n3. Training model...\")\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=10,\n    batch_size=8,\n    learning_rate=0.001,\n    validation_split=0.2,\n    verbose=1,\n)\n\nresult = trainer.train(\n    model=model,\n    data=dataset,\n    config=config,\n)\n\nlogger.info(\"\\nTraining completed!\")\nlogger.info(f\"Final training results: {result}\")\n\n# 4. Evaluate model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n4. Evaluating model...\")\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(\n    model=model,\n    data=dataset,\n    config=config,\n)\n\nlogger.info(\"Evaluation completed!\")\nlogger.info(f\"Evaluation results: {eval_result}\")\n\n# 5. Save model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n5. Saving model...\")\nmodel_path = \"/tmp/keras_model.keras\"\n\npersistence = ModelPersistence(\n    path=model_path,\n    model=model,\n)\npersistence.save(\n    save_format=\"keras\",\n)\nlogger.info(f\"Model saved to: {model_path}\")\n\n# 6. Load model (\u267b\ufe0f REUSABLE)\nlogger.info(\"\\n6. Loading model...\")\nloaded_model, metadata = persistence.load()\nlogger.info(f\"Model loaded successfully: {type(loaded_model)}\")\n\nlogger.info(\"\\n\" + \"=\" * 60)\nlogger.info(\"Complete!\")\nlogger.info(\"=\" * 60)\n</code></pre> <pre><code>\"\"\"Basic TensorFlow usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion TensorFlow workflow:\n1. Load data from CSV\n2. Optimize dataset for performance\n3. Create a TensorFlow model\n4. Train the model\n5. Evaluate the model\n6. Save and export the model\n\"\"\"\n\nimport tensorflow as tf\n\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\ndef main() -&gt; None:\n    \"\"\"Run basic TensorFlow training pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow Basic Usage\")\n    print(\"=\" * 60)\n\n    # 1. Load data\n    print(\"\\n1. Loading data...\")\n    loader = CSVDataLoader(\n        file_pattern=\"examples/data/sample.csv\",\n        label_name=\"target\",\n        batch_size=1,  # Load unbatched, let DatasetOptimizer handle batching\n    )\n    dataset = loader.load()\n    print(f\"Dataset: {dataset}\")\n\n    # Unbatch the dataset first (since CSVDataLoader batches by default)\n    dataset = dataset.unbatch()\n\n    # Transform OrderedDict to single tensor\n    def prepare_features(features, label):\n        \"\"\"Convert OrderedDict of features to single tensor.\"\"\"\n        feature_list = [features[key] for key in sorted(features.keys())]\n        stacked_features = tf.stack(feature_list, axis=-1)\n        return stacked_features, label\n\n    dataset = dataset.map(prepare_features)\n\n    # 2. Optimize dataset\n    print(\"\\n2. Optimizing dataset...\")\n    optimizer = DatasetOptimizer(batch_size=8, shuffle_buffer_size=100)\n    dataset = optimizer.optimize(dataset)\n\n    # 3. Create model\n    print(\"\\n3. Creating model...\")\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n        ]\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n    print(model.summary())\n\n    # 4. Train model\n    print(\"\\n4. Training model...\")\n    trainer = ModelTrainer()\n    config = ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n    result = trainer.train(\n        model=model,\n        data=dataset,\n        config=config,\n    )\n\n    print(\"\\nTraining completed!\")\n    print(f\"{result=}\")\n\n    # 5. Evaluate model\n    print(\"\\n5. Evaluating model...\")\n    evaluator = ModelEvaluator()\n    from mlpotion.frameworks.tensorflow import ModelEvaluationConfig\n\n    eval_config = ModelEvaluationConfig(batch_size=8, verbose=1)\n    eval_result = evaluator.evaluate(\n        model=model,\n        data=dataset,\n        config=eval_config,\n    )\n    print(f\"{eval_result=}\")\n\n    # 6. Save model\n    print(\"\\n6. Saving model...\")\n    model_path = \"/tmp/tensorflow_model.keras\"\n    persistence = ModelPersistence(\n        path=model_path,\n        model=model,\n    )\n    persistence.save(\n        save_format=\".keras\",\n    )\n    print(f\"Model saved to: {model_path}\")\n\n    # 7. Load model\n    print(\"\\n7. Loading model...\")\n    loaded_model, metadata = persistence.load()\n    print(f\"Model loaded successfully: {type(loaded_model)}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>\"\"\"Basic PyTorch usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion PyTorch workflow:\n1. Load data from CSV\n2. Create a PyTorch model\n3. Train the model\n4. Evaluate the model\n5. Save and export the model\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom mlpotion.frameworks.pytorch import (\n    CSVDataset,\n    CSVDataLoader,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\nclass SimpleModel(nn.Module):\n    \"\"\"Simple feedforward neural network.\n\n    Args:\n        input_dim: Number of input features.\n        hidden_dim: Size of hidden layer.\n    \"\"\"\n\n    def __init__(self, input_dim: int = 10, hidden_dim: int = 64) -&gt; None:\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim, 32)\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network.\"\"\"\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        return self.fc3(x)\n\n\ndef main() -&gt; None:\n    \"\"\"Run basic PyTorch training pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - PyTorch Basic Usage\")\n    print(\"=\" * 60)\n\n    # 1. Load data\n    print(\"\\n1. Loading data...\")\n    dataset = CSVDataset(\n        file_pattern=\"examples/data/sample.csv\",\n        label_name=\"target\",\n    )\n    print(f\"Dataset size: {len(dataset)}\")\n\n    # 2. Create DataLoader\n    print(\"\\n2. Creating DataLoader...\")\n    factory = CSVDataLoader(batch_size=8, shuffle=True)\n    dataloader = factory.load(dataset)\n\n    # 3. Create model\n    print(\"\\n3. Creating model...\")\n    model = SimpleModel(input_dim=10, hidden_dim=64)\n    print(model)\n\n    # 4. Train model\n    print(\"\\n4. Training model...\")\n    trainer = ModelTrainer()\n    config = ModelTrainingConfig(\n        epochs=10,\n        learning_rate=0.001,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        verbose=1,\n    )\n    result = trainer.train(\n        model=model,\n        dataloader=dataloader,\n        config=config,\n    )\n\n    print(\"\\nTraining completed!\")\n    print(f\"Training time: {result.training_time:.2f}s\")\n    print(f\"Final loss: {result.metrics['loss']:.4f}\")\n\n    # 5. Evaluate model\n    print(\"\\n5. Evaluating model...\")\n    evaluator = ModelEvaluator()\n    eval_result = evaluator.evaluate(model, dataloader, config)\n\n    print(f\"Evaluation completed in {eval_result.evaluation_time:.2f}s\")\n    print(\"Evaluation metrics:\")\n    for metric_name, metric_value in eval_result.metrics.items():\n        print(f\"  - {metric_name}: {metric_value:.4f}\")\n\n    # 6. Save model\n    print(\"\\n6. Saving model...\")\n    persistence = ModelPersistence(\n        path=\"/tmp/pytorch_model.pth\",\n        model=model,\n    )\n    model_path = \"/tmp/pytorch_model.pth\"\n    persistence.save()\n    print(f\"Model saved to: {model_path}\")\n\n    # 7. Load model\n    print(\"\\n7. Loading model...\")\n    loaded_model, metadata = persistence.load()\n    print(f\"Model loaded successfully: {type(loaded_model)}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"index.html#zenml-pipeline-examples-mlops-mode","title":"ZenML Pipeline Examples (MLOps Mode)","text":"KerasTensorFlowPyTorch <pre><code>\"\"\"Keras training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's Keras components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\nimport keras\nfrom zenml import pipeline, step\n\nfrom mlpotion.integrations.zenml.keras.steps import (\n    evaluate_model,\n    export_model,\n    load_data,\n    save_model,\n    train_model,\n)\n\n\n@step\ndef create_model() -&gt; keras.Model:\n    \"\"\"Create and compile a Keras model.\n\n    Returns:\n        Compiled Keras model ready for training.\n    \"\"\"\n    model = keras.Sequential(\n        [\n            keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n            keras.layers.Dropout(0.2),\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dropout(0.2),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n\n    return model\n\n\n@pipeline(enable_cache=False)\ndef keras_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/keras_model.keras\",\n    export_path: str = \"/tmp/keras_model_export\",\n):\n    \"\"\"Complete Keras training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Create and configure model\n    3. Train model\n    4. Evaluate model\n    5. Save model\n    6. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataset = load_data(\n        file_path=file_path,\n        label_name=label_name,\n        batch_size=8,\n        shuffle=True,\n    )\n\n    # Step 2: Create model and config\n    model = create_model()\n\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    # Step 3: Train model\n    trained_model, training_metrics = train_model(\n        model=model,\n        data=dataset,\n        **_config_train,\n    )\n    # Step 4: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=trained_model,\n        data=dataset,\n        verbose=1,\n    )\n\n    # # Step 5: Save model\n    save_model(\n        model=trained_model,\n        save_path=model_save_path,\n    )\n\n    # # Step 6: Export model for serving\n    export_model(\n        model=trained_model,\n        export_path=export_path,\n        export_format=\"tf\",\n    )\n    return trained_model, training_metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the Keras ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - Keras ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Initialize ZenML (if not already initialized)\n    try:\n        from zenml.client import Client\n\n        client = Client()\n        print(f\"\u2705 ZenML initialized. Active stack: {client.active_stack_model.name}\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  ZenML client error: {e}\")\n        print(\"Run 'zenml init' if you haven't already\")\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = keras_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre> <pre><code>\"\"\"TensorFlow training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's TensorFlow components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\n\nimport tensorflow as tf\nfrom zenml import pipeline, step\n\nfrom mlpotion.frameworks.tensorflow import ModelTrainingConfig\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    evaluate_model,\n    export_model,\n    load_data,\n    optimize_data,\n    save_model,\n    train_model,\n)\n\n\n@step(enable_cache=False)  # Disable caching to ensure fresh model\ndef create_model() -&gt; tf.keras.Model:\n    \"\"\"Create and compile a TensorFlow model that accepts dict inputs.\n\n    Returns:\n        Compiled TensorFlow/Keras model ready for training.\n    \"\"\"\n    # Create inputs for each feature (10 features: feature_0 to feature_9)\n    # After batching, make_csv_dataset produces tensors with shape (batch_size,) for each scalar feature\n    # The materializer now correctly preserves this shape as (None,) where None is the batch dimension\n    inputs = {}\n    feature_list = []\n\n    for i in range(10):\n        # Each input has shape (1,) per sample after batching and materializer roundtrip\n        # The materializer preserves the concrete shape (batch_size, 1)\n        inp = tf.keras.Input(shape=(1,), name=f\"feature_{i}\", dtype=tf.float32)\n        inputs[f\"feature_{i}\"] = inp\n        # Already shape (batch_size, 1), no need to reshape\n        feature_list.append(inp)\n\n    # Concatenate all features along the last axis\n    # This will create shape (batch_size, 10)\n    concatenated = tf.keras.layers.Concatenate(axis=-1)(feature_list)\n\n    # Build the model architecture\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(concatenated)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(1)(x)\n\n    # Create the functional model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n\n    return model\n\n\n@step\ndef create_training_config() -&gt; ModelTrainingConfig:\n    \"\"\"Create training configuration.\n\n    Returns:\n        Training configuration with hyperparameters.\n    \"\"\"\n    return ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n\n@pipeline(enable_cache=False)\ndef tensorflow_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/tensorflow_model.keras\",\n    export_path: str = \"/tmp/tensorflow_model_export\",\n):\n    \"\"\"Complete TensorFlow training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Optimize dataset for performance\n    3. Create and configure model\n    4. Train model\n    5. Evaluate model\n    6. Save model\n    7. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataset = load_data(\n        file_path=file_path,\n        batch_size=1,\n        label_name=label_name,\n    )\n\n    # Step 2: Optimize dataset\n    optimized_dataset = optimize_data(\n        dataset=dataset,\n        batch_size=8,\n        shuffle_buffer_size=100,\n    )\n\n    # Step 3: Create model and config\n    model = create_model()\n\n    # Step 4: Train model\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    trained_model, training_metrics = train_model(\n        model=model,\n        dataset=optimized_dataset,\n        **_config_train,\n    )\n\n    # Step 5: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=trained_model,\n        dataset=optimized_dataset,\n    )\n\n    # Step 6: Save model\n    save_model(\n        model=trained_model,\n        save_path=model_save_path,\n    )\n\n    # Step 7: Export model for serving\n    export_model(\n        model=trained_model,\n        export_path=export_path,\n        export_format=\"keras\",\n    )\n\n    return trained_model, training_metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the TensorFlow ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = tensorflow_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre> <pre><code>\"\"\"PyTorch training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's PyTorch components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom zenml import pipeline, step\n\nfrom mlpotion.integrations.zenml.pytorch.steps import (\n    evaluate_model,\n    export_model,\n    load_csv_data,\n    save_model,\n    train_model,\n)\n\n\nclass SimpleModel(nn.Module):\n    \"\"\"Simple feedforward neural network.\n\n    Args:\n        input_dim: Number of input features.\n        hidden_dim: Size of hidden layer.\n    \"\"\"\n\n    def __init__(self, input_dim: int = 10, hidden_dim: int = 64) -&gt; None:\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim, 32)\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network.\"\"\"\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        return self.fc3(x)\n\n\n@step\ndef create_model() -&gt; nn.Module:\n    \"\"\"Create a PyTorch model.\n\n    Returns:\n        PyTorch model ready for training.\n    \"\"\"\n    model = SimpleModel(input_dim=10, hidden_dim=64)\n    return model\n\n\n@pipeline(enable_cache=False)\ndef pytorch_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/pytorch_model.pth\",\n    export_path: str = \"/tmp/pytorch_model_export.pt\",\n):\n    \"\"\"Complete PyTorch training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Create and configure model\n    3. Train model\n    4. Evaluate model\n    5. Save model\n    6. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataloader = load_csv_data(\n        file_path=file_path,\n        label_name=label_name,\n        batch_size=8,\n        shuffle=True,\n    )\n\n    # Step 2: Create model and config\n    model = create_model()\n\n    # Step 3: Train model\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    model, metrics = train_model(\n        model=model,\n        dataloader=dataloader,\n        **_config_train,\n    )\n\n    # Step 4: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=model,\n        dataloader=dataloader,\n    )\n\n    # Step 5: Save model\n    save_model(\n        model=model,\n        save_path=model_save_path,\n    )\n\n    # # Step 6: Export model for serving (TorchScript)\n    export_model(\n        model=model,\n        export_path=export_path,\n        export_format=\"torchscript\",\n    )\n    return model, metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the PyTorch ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - PyTorch ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Initialize ZenML (if not already initialized)\n    try:\n        from zenml.client import Client\n\n        client = Client()\n        print(f\"\u2705 ZenML initialized. Active stack: {client.active_stack_model.name}\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  ZenML client error: {e}\")\n        print(\"Run 'zenml init' if you haven't already\")\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = pytorch_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre>"},{"location":"index.html#feature-comparison","title":"Feature Comparison \ud83d\udcca","text":"Feature MLPotion Framework-Only All-in-One Solutions Multi-framework \u2705 Yes \u274c No \u26a0\ufe0f Limited Type Safety \u2705 Full \u26a0\ufe0f Partial \u26a0\ufe0f Partial Modular Install \u2705 Yes \u274c No \u274c No ZenML Native \u2705 Yes \u274c Manual \u26a0\ufe0f Adapters Learning Curve \ud83d\udcc8 Gentle \ud83d\udcc8 Framework-specific \ud83d\udcc8 Steep Production Ready \u2705 Yes \u26a0\ufe0f DIY \u2705 Yes Flexibility \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f \ud83c\udf1f\ud83c\udf1f"},{"location":"index.html#community-support","title":"Community &amp; Support \ud83e\udd1d","text":"\ud83d\udc19 GitHub Star, fork, contribute! \ud83d\udc1b Issues Report bugs, request features \ud83e\udd84 UnicoLab Enterprise AI solutions \ud83d\udcdc MIT License Free and open source"},{"location":"index.html#whats-next","title":"What's Next? \ud83d\uddfa\ufe0f","text":"\ud83d\udcda Learn the Basics <p>New to MLPotion? Start here!</p> <ul> <li>Installation</li> <li>Quick Start</li> <li>Core Concepts</li> </ul> \ud83d\udd27 Framework Guides <p>Deep dive into your framework</p> <ul> <li>TensorFlow Guide</li> <li>PyTorch Guide</li> <li>Keras Guide</li> </ul> \ud83c\udf93 Tutorials <p>Learn by building</p> <ul> <li>Your First Pipeline</li> <li>ZenML Integration</li> <li>Multi-Framework Project</li> </ul> \ud83d\udcd6 API Reference <p>Detailed documentation</p> <ul> <li>Core APIs</li> <li>Framework APIs</li> <li>Integrations</li> </ul> <p> Ready to brew some ML magic? Let's get started! \ud83e\uddea\u2728 Built with \u2764\ufe0f for the ML community by \ud83e\udd84 UnicoLab </p>"},{"location":"api/core.html","title":"Core API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's framework-agnostic core components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p>"},{"location":"api/core.html#protocols","title":"Protocols","text":""},{"location":"api/core.html#mlpotion.core.protocols","title":"mlpotion.core.protocols","text":"<p>Framework-agnostic protocols using Python 3.10+ type hints.</p> <p>These protocols define interfaces that work across TensorFlow, PyTorch, and other frameworks.</p>"},{"location":"api/core.html#mlpotion.core.protocols-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.protocols.DataLoader","title":"DataLoader","text":"<p>         Bases: <code>Protocol[DatasetT]</code></p> <p>Protocol for data loading components.</p> <p>This protocol defines the interface for loading data into a framework-specific format. Any class that implements a <code>load()</code> method returning a dataset satisfies this protocol.</p> Type Parameters <p>DatasetT: The type of the dataset returned (e.g., <code>tf.data.Dataset</code>, <code>torch.utils.data.DataLoader</code>).</p> Example <pre><code>class MyCSVLoader:\n    def load(self) -&gt; tf.data.Dataset:\n        # Implementation details...\n        return dataset\n\nloader: DataLoader = MyCSVLoader()\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.DataLoader-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.DataLoader.load","title":"load","text":"<pre><code>load() -&gt; DatasetT\n</code></pre> <p>Load data and return a framework-specific dataset.</p> <p>Returns:</p> Name Type Description <code>DatasetT</code> <code>DatasetT</code> <p>The loaded dataset in a framework-specific format.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def load(self) -&gt; DatasetT:\n    \"\"\"Load data and return a framework-specific dataset.\n\n    Returns:\n        DatasetT: The loaded dataset in a framework-specific format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.DataTransformer","title":"DataTransformer","text":"<p>         Bases: <code>Protocol[DatasetT, ModelT]</code></p> <p>Protocol for data transformation components.</p> <p>This protocol defines the interface for transforming datasets using a model, commonly used for tasks like generating embeddings or pre-processing data.</p> Type Parameters <p>DatasetT: The type of the dataset. ModelT: The type of the model used for transformation.</p>"},{"location":"api/core.html#mlpotion.core.protocols.DataTransformer-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.DataTransformer.transform","title":"transform","text":"<pre><code>transform(\n    dataset: DatasetT, model: ModelT, **kwargs: Any\n) -&gt; DatasetT\n</code></pre> <p>Transform a dataset using a model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetT</code> <p>The input dataset to transform.</p> required <code>model</code> <code>ModelT</code> <p>The model to use for the transformation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional framework-specific arguments (e.g., batch_size, device).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DatasetT</code> <code>DatasetT</code> <p>The transformed dataset.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def transform(\n    self,\n    dataset: DatasetT,\n    model: ModelT,\n    **kwargs: Any,\n) -&gt; DatasetT:\n    \"\"\"Transform a dataset using a model.\n\n    Args:\n        dataset: The input dataset to transform.\n        model: The model to use for the transformation.\n        **kwargs: Additional framework-specific arguments (e.g., batch_size, device).\n\n    Returns:\n        DatasetT: The transformed dataset.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.DatasetOptimizer","title":"DatasetOptimizer","text":"<p>         Bases: <code>Protocol[DatasetT]</code></p> <p>Protocol for dataset optimization components.</p> <p>This protocol defines the interface for optimizing datasets, such as applying batching, caching, prefetching, or shuffling.</p> Type Parameters <p>DatasetT: The type of the dataset to be optimized.</p>"},{"location":"api/core.html#mlpotion.core.protocols.DatasetOptimizer-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.DatasetOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: DatasetT) -&gt; DatasetT\n</code></pre> <p>Optimize a dataset for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DatasetT</code> <p>The input dataset to optimize.</p> required <p>Returns:</p> Name Type Description <code>DatasetT</code> <code>DatasetT</code> <p>The optimized dataset, ready for consumption by a model.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def optimize(self, dataset: DatasetT) -&gt; DatasetT:\n    \"\"\"Optimize a dataset for training or inference.\n\n    Args:\n        dataset: The input dataset to optimize.\n\n    Returns:\n        DatasetT: The optimized dataset, ready for consumption by a model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelEvaluator","title":"ModelEvaluator","text":"<p>         Bases: <code>Protocol[ModelT, DatasetT]</code></p> <p>Protocol for model evaluation components.</p> <p>This protocol defines the interface for evaluating models.</p> Type Parameters <p>ModelT: The type of the model to be evaluated. DatasetT: The type of the dataset used for evaluation.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: ModelT,\n    dataset: DatasetT,\n    config: EvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a model on a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model to evaluate.</p> required <code>dataset</code> <code>DatasetT</code> <p>The dataset to evaluate on.</p> required <code>config</code> <code>EvaluationConfig</code> <p>Configuration object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>An object containing the evaluation metrics.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def evaluate(\n    self,\n    model: ModelT,\n    dataset: DatasetT,\n    config: EvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a model on a given dataset.\n\n    Args:\n        model: The model to evaluate.\n        dataset: The dataset to evaluate on.\n        config: Configuration object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: An object containing the evaluation metrics.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelExporter","title":"ModelExporter","text":"<p>         Bases: <code>Protocol[ModelT]</code></p> <p>Protocol for model export components.</p> <p>This protocol defines the interface for exporting models to various formats for deployment.</p> Type Parameters <p>ModelT: The type of the model to be exported.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelExporter-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelExporter.export","title":"export","text":"<pre><code>export(model: ModelT, config: ExportConfig) -&gt; ExportResult\n</code></pre> <p>Export a model for serving or deployment.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model to export.</p> required <code>config</code> <code>ExportConfig</code> <p>Configuration object containing export parameters (path, format, etc.).</p> required <p>Returns:</p> Name Type Description <code>ExportResult</code> <code>ExportResult</code> <p>An object containing details about the exported model.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def export(\n    self,\n    model: ModelT,\n    config: ExportConfig,\n) -&gt; ExportResult:\n    \"\"\"Export a model for serving or deployment.\n\n    Args:\n        model: The model to export.\n        config: Configuration object containing export parameters (path, format, etc.).\n\n    Returns:\n        ExportResult: An object containing details about the exported model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelInspector","title":"ModelInspector","text":"<p>         Bases: <code>Protocol[ModelT]</code></p> <p>Protocol for model inspection components.</p> <p>This protocol defines the interface for inspecting models to extract metadata such as layer configuration, input/output shapes, and parameter counts.</p> Type Parameters <p>ModelT: The type of the model to be inspected.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelInspector-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelInspector.inspect","title":"inspect","text":"<pre><code>inspect(model: ModelT) -&gt; dict[str, Any]\n</code></pre> <p>Inspect a model and return structured metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model to inspect.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing model metadata (inputs, outputs, parameters, layers, etc.).</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def inspect(self, model: ModelT) -&gt; dict[str, Any]:\n    \"\"\"Inspect a model and return structured metadata.\n\n    Args:\n        model: The model to inspect.\n\n    Returns:\n        dict[str, Any]: A dictionary containing model metadata (inputs, outputs, parameters, layers, etc.).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence","title":"ModelPersistence","text":"<p>         Bases: <code>Protocol[ModelT]</code></p> <p>Protocol for model persistence operations (save/load).</p> <p>This protocol defines the interface for saving and loading models to/from disk.</p> Type Parameters <p>ModelT: The type of the model to be persisted.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence.load","title":"load","text":"<pre><code>load(**kwargs: Any) -&gt; tuple[ModelT, dict[str, Any] | None]\n</code></pre> <p>Load a model from the configured path.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional framework-specific loading options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ModelT, dict[str, Any] | None]</code> <p>tuple[ModelT, dict[str, Any] | None]: A tuple containing the loaded model and optional metadata.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def load(self, **kwargs: Any) -&gt; tuple[ModelT, dict[str, Any] | None]:\n    \"\"\"Load a model from the configured path.\n\n    Args:\n        **kwargs: Additional framework-specific loading options.\n\n    Returns:\n        tuple[ModelT, dict[str, Any] | None]: A tuple containing the loaded model and optional metadata.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelPersistence.save","title":"save","text":"<pre><code>save(**kwargs: Any) -&gt; None\n</code></pre> <p>Save a model to the configured path.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional framework-specific saving options.</p> <code>{}</code> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def save(self, **kwargs: Any) -&gt; None:\n    \"\"\"Save a model to the configured path.\n\n    Args:\n        **kwargs: Additional framework-specific saving options.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#mlpotion.core.protocols.ModelTrainer","title":"ModelTrainer","text":"<p>         Bases: <code>Protocol[ModelT, DatasetT]</code></p> <p>Protocol for model training components.</p> <p>This protocol defines the standard interface for training models across different frameworks.</p> Type Parameters <p>ModelT: The type of the model to be trained. DatasetT: The type of the dataset used for training.</p>"},{"location":"api/core.html#mlpotion.core.protocols.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.protocols.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: ModelT,\n    dataset: DatasetT,\n    config: TrainingConfig,\n    validation_dataset: DatasetT | None = None,\n) -&gt; TrainingResult[ModelT]\n</code></pre> <p>Train a model using the provided dataset and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelT</code> <p>The model instance to train.</p> required <code>dataset</code> <code>DatasetT</code> <p>The training dataset.</p> required <code>config</code> <code>TrainingConfig</code> <p>Configuration object containing training parameters (epochs, learning rate, etc.).</p> required <code>validation_dataset</code> <code>DatasetT | None</code> <p>Optional dataset for validation during training.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[ModelT]</code> <p>TrainingResult[ModelT]: An object containing the trained model, training history, and metrics.</p> Source code in <code>mlpotion/core/protocols.py</code> <pre><code>def train(\n    self,\n    model: ModelT,\n    dataset: DatasetT,\n    config: TrainingConfig,\n    validation_dataset: DatasetT | None = None,\n) -&gt; TrainingResult[ModelT]:\n    \"\"\"Train a model using the provided dataset and configuration.\n\n    Args:\n        model: The model instance to train.\n        dataset: The training dataset.\n        config: Configuration object containing training parameters (epochs, learning rate, etc.).\n        validation_dataset: Optional dataset for validation during training.\n\n    Returns:\n        TrainingResult[ModelT]: An object containing the trained model, training history, and metrics.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core.html#result-types","title":"Result Types","text":""},{"location":"api/core.html#mlpotion.core.results","title":"mlpotion.core.results","text":"<p>Result types using dataclasses with Python 3.10+ features.</p>"},{"location":"api/core.html#mlpotion.core.results-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.results.EvaluationResult","title":"EvaluationResult  <code>dataclass</code>","text":"<p>Result container for model evaluation operations.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>dict[str, float]</code> <p>A dictionary of evaluation metric values.</p> <code>config</code> <code>Any</code> <p>The configuration object used for this evaluation.</p> <code>evaluation_time</code> <code>float | None</code> <p>The total time taken for evaluation in seconds.</p>"},{"location":"api/core.html#mlpotion.core.results.EvaluationResult-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.results.EvaluationResult.get_metric","title":"get_metric","text":"<pre><code>get_metric(name: str) -&gt; float | None\n</code></pre> <p>Retrieve a specific metric value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric to retrieve.</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>float | None: The value of the metric, or None if not found.</p> Source code in <code>mlpotion/core/results.py</code> <pre><code>def get_metric(self, name: str) -&gt; float | None:\n    \"\"\"Retrieve a specific metric value.\n\n    Args:\n        name: The name of the metric to retrieve.\n\n    Returns:\n        float | None: The value of the metric, or None if not found.\n    \"\"\"\n    return self.metrics.get(name)\n</code></pre>"},{"location":"api/core.html#mlpotion.core.results.ExportResult","title":"ExportResult  <code>dataclass</code>","text":"<p>Result container for model export operations.</p> <p>Attributes:</p> Name Type Description <code>export_path</code> <code>str</code> <p>The absolute path where the model was exported.</p> <code>format</code> <code>str</code> <p>The format of the exported model (e.g., 'saved_model', 'onnx').</p> <code>config</code> <code>Any</code> <p>The configuration object used for this export.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata generated during export.</p>"},{"location":"api/core.html#mlpotion.core.results.InspectionResult","title":"InspectionResult  <code>dataclass</code>","text":"<p>Result container for model inspection operations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the model.</p> <code>backend</code> <code>str</code> <p>The framework backend used (e.g., 'tensorflow', 'pytorch').</p> <code>trainable</code> <code>bool</code> <p>Whether the model is trainable.</p> <code>inputs</code> <code>list[dict[str, Any]]</code> <p>List of input specifications (shape, dtype, etc.).</p> <code>input_names</code> <code>list[str]</code> <p>List of input names.</p> <code>outputs</code> <code>list[dict[str, Any]]</code> <p>List of output specifications.</p> <code>output_names</code> <code>list[str]</code> <p>List of output names.</p> <code>parameters</code> <code>dict[str, int]</code> <p>Dictionary of parameter counts (total, trainable, non_trainable).</p> <code>signatures</code> <code>dict[str, Any]</code> <p>Model signatures (specific to TensorFlow SavedModel).</p> <code>layers</code> <code>list[dict[str, Any]]</code> <p>List of layer details (name, class, args).</p>"},{"location":"api/core.html#mlpotion.core.results.LoadingResult","title":"LoadingResult  <code>dataclass</code>","text":"<p>         Bases: <code>Generic[ModelT]</code></p> <p>Result from model loading.</p>"},{"location":"api/core.html#mlpotion.core.results.TrainingResult","title":"TrainingResult  <code>dataclass</code>","text":"<p>         Bases: <code>Generic[ModelT]</code></p> <p>Result container for model training operations.</p> <p>This dataclass encapsulates all relevant information produced during a model training session.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelT</code> <p>The trained model instance.</p> <code>history</code> <code>dict[str, list[float]]</code> <p>A dictionary mapping metric names to lists of values per epoch.</p> <code>metrics</code> <code>dict[str, float]</code> <p>A dictionary of the final metric values after training.</p> <code>config</code> <code>Any</code> <p>The configuration object used for this training session.</p> <code>training_time</code> <code>float | None</code> <p>The total time taken for training in seconds.</p> <code>best_epoch</code> <code>int | None</code> <p>The epoch number where the best performance was achieved (if applicable).</p>"},{"location":"api/core.html#mlpotion.core.results.TrainingResult-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.core.results.TrainingResult.get_history","title":"get_history","text":"<pre><code>get_history(metric: str) -&gt; list[float] | None\n</code></pre> <p>Retrieve the history of a specific metric over epochs.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>The name of the metric to retrieve history for.</p> required <p>Returns:</p> Type Description <code>list[float] | None</code> <p>list[float] | None: The list of metric values per epoch, or None if not found.</p> Source code in <code>mlpotion/core/results.py</code> <pre><code>def get_history(self, metric: str) -&gt; list[float] | None:\n    \"\"\"Retrieve the history of a specific metric over epochs.\n\n    Args:\n        metric: The name of the metric to retrieve history for.\n\n    Returns:\n        list[float] | None: The list of metric values per epoch, or None if not found.\n    \"\"\"\n    return self.history.get(metric)\n</code></pre>"},{"location":"api/core.html#mlpotion.core.results.TrainingResult.get_metric","title":"get_metric","text":"<pre><code>get_metric(name: str) -&gt; float | None\n</code></pre> <p>Retrieve a specific final metric value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric to retrieve.</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>float | None: The value of the metric, or None if not found.</p> Source code in <code>mlpotion/core/results.py</code> <pre><code>def get_metric(self, name: str) -&gt; float | None:\n    \"\"\"Retrieve a specific final metric value.\n\n    Args:\n        name: The name of the metric to retrieve.\n\n    Returns:\n        float | None: The value of the metric, or None if not found.\n    \"\"\"\n    return self.metrics.get(name)\n</code></pre>"},{"location":"api/core.html#mlpotion.core.results.TransformationResult","title":"TransformationResult  <code>dataclass</code>","text":"<p>Result from data transformation.</p>"},{"location":"api/core.html#configurations","title":"Configurations","text":""},{"location":"api/core.html#mlpotion.core.config","title":"mlpotion.core.config","text":"<p>Framework-agnostic base configuration models using Pydantic 2.x.</p> <p>This module contains only truly framework-agnostic configuration classes. Framework-specific configurations should be defined in their respective framework modules (keras, tensorflow, pytorch).</p>"},{"location":"api/core.html#mlpotion.core.config-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.config.EvaluationConfig","title":"EvaluationConfig","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Base configuration for model evaluation.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Batch size for evaluation (must be &gt;= 1).</p> <code>verbose</code> <code>int</code> <p>Verbosity level (0=silent, 1=progress bar, 2=one line per epoch).</p> <code>framework_options</code> <code>dict[str, Any]</code> <p>Dictionary for framework-specific options.</p>"},{"location":"api/core.html#mlpotion.core.config.ExportConfig","title":"ExportConfig","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Base configuration for model export.</p> <p>Attributes:</p> Name Type Description <code>export_path</code> <code>str</code> <p>Destination path for the exported model.</p> <code>format</code> <code>str</code> <p>Format identifier for the export (e.g., 'saved_model', 'onnx').</p> <code>include_optimizer</code> <code>bool</code> <p>Whether to include the optimizer state in the export.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata to include with the export.</p>"},{"location":"api/core.html#mlpotion.core.config.TrainingConfig","title":"TrainingConfig","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Base configuration for model training.</p> <p>This class defines the standard configuration parameters for training models. Framework-specific configurations should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>epochs</code> <code>int</code> <p>Number of training epochs (must be &gt;= 1).</p> <code>batch_size</code> <code>int</code> <p>Batch size for training (must be &gt;= 1).</p> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer (must be &gt; 0.0).</p> <code>validation_split</code> <code>float</code> <p>Fraction of data to use for validation (0.0 to 1.0).</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the training data.</p> <code>verbose</code> <code>int</code> <p>Verbosity level (0=silent, 1=progress bar, 2=one line per epoch).</p> <code>framework_options</code> <code>dict[str, Any]</code> <p>Dictionary for framework-specific options that don't fit standard fields.</p>"},{"location":"api/core.html#exceptions","title":"Exceptions","text":""},{"location":"api/core.html#mlpotion.core.exceptions","title":"mlpotion.core.exceptions","text":"<p>Exception hierarchy for MLPotion.</p>"},{"location":"api/core.html#mlpotion.core.exceptions-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.core.exceptions.ConfigurationError","title":"ConfigurationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when there is an issue with the provided configuration.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.DataLoadingError","title":"DataLoadingError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during data loading or preprocessing.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.DataTransformationError","title":"DataTransformationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during data transformation operations.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.EvaluationError","title":"EvaluationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model evaluation.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ExportError","title":"ExportError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model export operations.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.FrameworkNotInstalledError","title":"FrameworkNotInstalledError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when a required framework (e.g., TensorFlow, PyTorch) is not installed or cannot be imported.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.MLPotionError","title":"MLPotionError","text":"<p>         Bases: <code>Exception</code></p> <p>Base exception for all MLPotion errors.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelEvaluatorError","title":"ModelEvaluatorError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs specifically within a ModelEvaluator component.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelExporterError","title":"ModelExporterError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs specifically within a ModelExporter component.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelInspectorError","title":"ModelInspectorError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model inspection.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelPersistenceError","title":"ModelPersistenceError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during model saving or loading.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ModelTrainerError","title":"ModelTrainerError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs specifically within a ModelTrainer component.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.TrainingError","title":"TrainingError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when an error occurs during the model training process.</p>"},{"location":"api/core.html#mlpotion.core.exceptions.ValidationError","title":"ValidationError","text":"<p>         Bases: <code>MLPotionError</code></p> <p>Raised when data or model validation fails.</p>"},{"location":"api/core.html#utilities","title":"Utilities","text":""},{"location":"api/core.html#framework-detection","title":"Framework Detection","text":""},{"location":"api/core.html#mlpotion.utils.framework","title":"mlpotion.utils.framework","text":"<p>Framework detection and validation utilities.</p>"},{"location":"api/core.html#mlpotion.utils.framework-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.utils.framework.FrameworkChecker","title":"FrameworkChecker","text":"<p>Utility class to check availability of ML frameworks.</p>"},{"location":"api/core.html#mlpotion.utils.framework.FrameworkChecker-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.utils.framework.FrameworkChecker.is_available","title":"is_available  <code>classmethod</code>","text":"<pre><code>is_available(framework: FrameworkName) -&gt; bool\n</code></pre> <p>Check whether a framework is installed and importable.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>FrameworkName</code> <p>Framework identifier supported by this checker.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the framework can be imported, otherwise False.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided framework is not known.</p> Example <pre><code>if FrameworkChecker.is_available(\"torch\"):\n    print(\"PyTorch is installed!\")\nelse:\n    print(\"PyTorch is missing.\")\n</code></pre> Source code in <code>mlpotion/utils/framework.py</code> <pre><code>@classmethod\ndef is_available(cls, framework: FrameworkName) -&gt; bool:\n    \"\"\"Check whether a framework is installed and importable.\n\n    Args:\n        framework: Framework identifier supported by this checker.\n\n    Returns:\n        True if the framework can be imported, otherwise False.\n\n    Raises:\n        ValueError: If the provided framework is not known.\n\n    Example:\n        ```python\n        if FrameworkChecker.is_available(\"torch\"):\n            print(\"PyTorch is installed!\")\n        else:\n            print(\"PyTorch is missing.\")\n        ```\n    \"\"\"\n    if framework not in cls._FRAMEWORK_IMPORTS:\n        msg = f\"Unsupported framework: {framework}\"\n        logger.error(msg)\n        raise ValueError(msg)\n\n    module_name = cls._FRAMEWORK_IMPORTS[framework]\n\n    try:\n        import_module(module_name)\n        return True\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/core.html#mlpotion.utils.framework-functions","title":"Functions","text":""},{"location":"api/core.html#mlpotion.utils.framework.get_available_frameworks","title":"get_available_frameworks","text":"<pre><code>get_available_frameworks() -&gt; list[FrameworkName]\n</code></pre> <p>Get list of available frameworks.</p> <p>Returns:</p> Type Description <code>list[FrameworkName]</code> <p>List of framework names that are installed</p> Source code in <code>mlpotion/utils/framework.py</code> <pre><code>def get_available_frameworks() -&gt; list[FrameworkName]:\n    \"\"\"Get list of available frameworks.\n\n    Returns:\n        List of framework names that are installed\n    \"\"\"\n    frameworks: list[FrameworkName] = list(FrameworkChecker._FRAMEWORK_IMPORTS.keys())\n    return [f for f in frameworks if is_framework_available(f)]\n</code></pre>"},{"location":"api/core.html#mlpotion.utils.framework.require_framework","title":"require_framework","text":"<pre><code>require_framework(\n    framework: FrameworkName, install_command: str\n) -&gt; None\n</code></pre> <p>Require a framework to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>FrameworkName</code> <p>Framework name</p> required <code>install_command</code> <code>str</code> <p>Installation command to show in error</p> required <p>Raises:</p> Type Description <code>FrameworkNotInstalledError</code> <p>If framework is not installed</p> Source code in <code>mlpotion/utils/framework.py</code> <pre><code>def require_framework(framework: FrameworkName, install_command: str) -&gt; None:\n    \"\"\"Require a framework to be installed.\n\n    Args:\n        framework: Framework name\n        install_command: Installation command to show in error\n\n    Raises:\n        FrameworkNotInstalledError: If framework is not installed\n    \"\"\"\n    if not is_framework_available(framework):\n        raise FrameworkNotInstalledError(\n            f\"{framework} is not installed. \"\n            f\"Install it with: poetry add {install_command}\"\n        )\n</code></pre>"},{"location":"api/core.html#decorators","title":"Decorators","text":"<p> For framework-specific APIs, see the respective framework documentation </p>"},{"location":"api/core.html#mlpotion.utils.decorators","title":"mlpotion.utils.decorators","text":""},{"location":"api/core.html#mlpotion.utils.decorators-classes","title":"Classes","text":""},{"location":"api/core.html#mlpotion.utils.decorators.trycatch","title":"trycatch","text":"<pre><code>trycatch(\n    error: Type[Exception], success_msg: str | None = None\n) -&gt; None\n</code></pre> <p>Decorator for wrapping methods with unified exception handling and logging.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Type[Exception]</code> <p>Exception type to raise when unexpected errors occur.</p> required <code>success_msg</code> <code>str | None</code> <p>Optional success message to log on method completion.</p> <code>None</code> Example <pre><code>@trycatch(error=DataLoadingError, success_msg=\"Dataset loaded\")\ndef load(self):\n    ...\n</code></pre> Source code in <code>mlpotion/utils/decorators.py</code> <pre><code>def __init__(\n    self,\n    error: Type[Exception],\n    success_msg: str | None = None,\n) -&gt; None:\n    self.error = error\n    self.success_msg = success_msg\n</code></pre>"},{"location":"api/frameworks/keras.html","title":"Keras API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's Keras components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p> <p>Extensibility</p> <p>These components are built using protocol-based design, making MLPotion easy to extend. Want to add new data sources, training methods, or integrations? See Contributing Guide.</p>"},{"location":"api/frameworks/keras.html#data-loading","title":"Data Loading","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders","title":"mlpotion.frameworks.keras.data.loaders","text":"<p>Keras data loaders.</p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVDataLoader","title":"CSVDataLoader  <code>dataclass</code>","text":"<p>         Bases: <code>DataLoader[CSVSequence]</code></p> <p>Loader for CSV files into a Keras-ready Sequence.</p> <p>This class provides a high-level interface for loading data from CSV files into a <code>CSVSequence</code> compatible with Keras models. It handles file globbing, reading, column selection, and label separation.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load (e.g., \"data/*.csv\").</p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch in the resulting sequence.</p> <code>column_names</code> <code>list[str] | None</code> <p>List of column names to use as features. If None, all columns except the label are used.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as labels. If None, no labels are returned (inference mode).</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data between epochs.</p> <code>dtype</code> <code>np.dtype | str</code> <p>Data type to use for the loaded data (default: \"float32\").</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras.data.loaders import CSVDataLoader\n\n# Define a simple model\nmodel = keras.Sequential([\n    keras.layers.Dense(10, input_shape=(5,), activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Create loader\nloader = CSVDataLoader(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    batch_size=64,\n    shuffle=True\n)\n\n# Load data\ntrain_sequence = loader.load()\n\n# Train model\nmodel.fit(train_sequence, epochs=10)\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVDataLoader.load","title":"load","text":"<pre><code>load() -&gt; CSVSequence\n</code></pre> <p>Load CSV files and return a CSVSequence.</p> <p>Returns:</p> Type Description <code>CSVSequence</code> <p>CSVSequence that can be passed directly to <code>model.fit(...)</code>.</p> <p>Raises:</p> Type Description <code>DataLoadingError</code> <p>If files cannot be found or read.</p> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully created Keras CSV Sequence\",\n)\ndef load(self) -&gt; CSVSequence:\n    \"\"\"Load CSV files and return a CSVSequence.\n\n    Returns:\n        CSVSequence that can be passed directly to `model.fit(...)`.\n\n    Raises:\n        DataLoadingError: If files cannot be found or read.\n    \"\"\"\n    files = self._get_files()\n    df = self._load_dataframe(files)\n    df = self._select_columns(df)\n\n    features_np, labels_np = self._split_features_labels(df)\n\n    logger.info(\n        \"Creating CSVSequence: n_samples={n}, n_features={d}, labels={labels}\",\n        n=features_np.shape[0],\n        d=features_np.shape[1],\n        labels=\"yes\" if labels_np is not None else \"no\",\n    )\n\n    sequence = CSVSequence(\n        features=features_np,\n        labels=labels_np,\n        batch_size=self.batch_size,\n        shuffle=self.shuffle,\n        dtype=self.dtype,\n    )\n    return sequence\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence","title":"CSVSequence","text":"<pre><code>CSVSequence(\n    features: np.ndarray,\n    labels: np.ndarray | None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n    dtype: np.dtype | str = \"float32\",\n) -&gt; None\n</code></pre> <p>         Bases: <code>Sequence</code></p> <p>Keras Sequence for CSV data backed by NumPy arrays.</p> <p>This class implements the <code>keras.utils.Sequence</code> interface, allowing it to be used directly with <code>model.fit()</code>, <code>model.evaluate()</code>, and <code>model.predict()</code>. It handles data batching and shuffling efficiently in memory.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>np.ndarray</code> <p>The feature data as a 2D NumPy array.</p> <code>labels</code> <code>np.ndarray | None</code> <p>The label data as a NumPy array, or None if not available.</p> <code>batch_size</code> <code>int</code> <p>The size of each data batch.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data at the end of each epoch.</p> <code>dtype</code> <code>np.dtype | str</code> <p>The data type of the features and labels.</p> Example <pre><code>from mlpotion.frameworks.keras.data.loaders import CSVSequence\nimport numpy as np\n\n# Create dummy data\nX = np.random.rand(100, 10)\ny = np.random.randint(0, 2, 100)\n\n# Create sequence\nsequence = CSVSequence(\n    features=X,\n    labels=y,\n    batch_size=32,\n    shuffle=True\n)\n\n# Use in training\nmodel.fit(sequence, epochs=5)\n</code></pre> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def __init__(\n    self,\n    features: np.ndarray,\n    labels: np.ndarray | None,\n    batch_size: int = 32,\n    shuffle: bool = True,\n    dtype: np.dtype | str = \"float32\",\n) -&gt; None:\n    if features.ndim != 2:\n        raise ValueError(\n            f\"features must be 2D (n_samples, n_features), got shape {features.shape}\"\n        )\n\n    if labels is not None and len(labels) != len(features):\n        raise ValueError(\n            f\"features and labels must have same length, \"\n            f\"got {len(features)} != {len(labels)}\"\n        )\n\n    self._features = features.astype(dtype, copy=False)\n    self._labels = labels.astype(dtype, copy=False) if labels is not None else None\n    self._batch_size = int(batch_size)\n    self._shuffle = bool(shuffle)\n    self._indices = np.arange(len(self._features))\n\n    if self._shuffle:\n        np.random.shuffle(self._indices)\n\n    logger.info(\n        \"Initialized CSVSequence: \"\n        f\"n_samples={len(self._features)}, \"\n        f\"batch_size={self._batch_size}, \"\n        f\"shuffle={self._shuffle}, \"\n        f\"labels={'yes' if self._labels is not None else 'no'}\"\n    )\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; Any\n</code></pre> <p>Get batch by index.</p> <p>Returns:</p> Type Description <code>Any</code> <ul> <li>(x_batch, y_batch) if labels are available</li> </ul> <code>Any</code> <ul> <li>x_batch otherwise</li> </ul> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Any:\n    \"\"\"Get batch by index.\n\n    Returns:\n        - (x_batch, y_batch) if labels are available\n        - x_batch otherwise\n    \"\"\"\n    if idx &lt; 0 or idx &gt;= len(self):\n        raise IndexError(f\"Batch index out of range: {idx}\")\n\n    start = idx * self._batch_size\n    end = min(start + self._batch_size, len(self._features))\n    batch_idx = self._indices[start:end]\n\n    x_batch = self._features[batch_idx]\n    if self._labels is not None:\n        y_batch = self._labels[batch_idx]\n        return x_batch, y_batch\n\n    return x_batch\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Number of batches per epoch.</p> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of batches per epoch.\"\"\"\n    n_samples = len(self._features)\n    return int(np.ceil(n_samples / self._batch_size))\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.data.loaders.CSVSequence.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end() -&gt; None\n</code></pre> <p>Shuffle indices between epochs if enabled.</p> Source code in <code>mlpotion/frameworks/keras/data/loaders.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Shuffle indices between epochs if enabled.\"\"\"\n    if self._shuffle:\n        np.random.shuffle(self._indices)\n</code></pre>"},{"location":"api/frameworks/keras.html#training","title":"Training","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers","title":"mlpotion.frameworks.keras.training.trainers","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer","title":"ModelTrainer  <code>dataclass</code>","text":"<p>         Bases: <code>ModelTrainerProtocol[Model, Sequence]</code></p> <p>Generic trainer for Keras 3 models.</p> <p>This class implements the <code>ModelTrainerProtocol</code> for Keras models, providing a standardized interface for training. It wraps the standard <code>model.fit()</code> method but adds flexibility and consistency checks.</p> <p>It supports: - Automatic model compilation if <code>compile_params</code> are provided. - Handling of various data formats (tuples, dicts, generators). - Standardized return format (dictionary of history metrics).</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelTrainer\n\n# Prepare data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize trainer\ntrainer = ModelTrainer()\n\n# Train\nhistory = trainer.train(\n    model=model,\n    data=(X_train, y_train),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    fit_params={\n        \"epochs\": 5,\n        \"batch_size\": 32,\n        \"verbose\": 1\n    }\n)\n\nprint(history['loss'])\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]\n</code></pre> <p>Train a Keras model using the provided dataset and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to train.</p> required <code>dataset</code> <code>Any</code> <p>The training data. Can be a tuple <code>(x, y)</code>, a dictionary, a <code>Sequence</code>, or a generator.</p> required <code>config</code> <code>ModelTrainingConfig</code> <p>Configuration object containing training parameters.</p> required <code>validation_dataset</code> <code>Any | None</code> <p>Optional validation data.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[Model]</code> <p>TrainingResult[Model]: An object containing the trained model, training history, and metrics.</p> Source code in <code>mlpotion/frameworks/keras/training/trainers.py</code> <pre><code>@trycatch(\n    error=ModelTrainerError,\n    success_msg=\"\u2705 Successfully trained Keras model\",\n)\ndef train(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]:\n    \"\"\"Train a Keras model using the provided dataset and configuration.\n\n    Args:\n        model: The Keras model to train.\n        dataset: The training data. Can be a tuple `(x, y)`, a dictionary, a `Sequence`, or a generator.\n        config: Configuration object containing training parameters.\n        validation_dataset: Optional validation data.\n\n    Returns:\n        TrainingResult[Model]: An object containing the trained model, training history, and metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare compile parameters from config\n    compile_params = {\n        \"optimizer\": self._get_optimizer(config),\n        \"loss\": config.loss,\n        \"metrics\": config.metrics,\n    }\n\n    # Compile if needed or if forced by config (though we usually respect existing compilation)\n    # Here we'll ensure it's compiled. If the user wants to use their own compilation,\n    # they should probably compile it before passing it, but our config implies we control it.\n    # However, to be safe and flexible:\n    if not self._is_compiled(model):\n        if not config.optimizer or not config.loss:\n            raise RuntimeError(\n                \"Model is not compiled and config does not provide optimizer and loss. \"\n                \"Either compile the model beforehand or provide optimizer and loss in config.\"\n            )\n        logger.info(\"Compiling model with config parameters.\")\n        model.compile(**compile_params)\n    else:\n        logger.info(\"Model already compiled. Using existing compilation settings.\")\n\n    # Prepare fit parameters\n    fit_kwargs = {\n        \"epochs\": config.epochs,\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"shuffle\": config.shuffle,\n        \"validation_split\": config.validation_split,\n        \"callbacks\": self._prepare_callbacks(config),\n    }\n\n    if validation_dataset is not None:\n        fit_kwargs[\"validation_data\"] = validation_dataset\n\n    # Add any framework-specific options\n    fit_kwargs.update(config.framework_options)\n\n    logger.info(\"Starting Keras model training...\")\n    logger.debug(f\"Training data type: {type(dataset)!r}\")\n    logger.debug(f\"Fit parameters: {fit_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    history_obj = self._call_fit(model=model, data=dataset, fit_kwargs=fit_kwargs)\n\n    training_time = time.time() - start_time\n\n    # Convert History object to dict[str, list[float]]\n    history_dict = self._history_to_dict(history_obj)\n\n    # Extract final metrics\n    final_metrics = {}\n    for k, v in history_dict.items():\n        if v:\n            final_metrics[k] = v[-1]\n\n    logger.info(\"Training completed.\")\n    logger.debug(f\"Training history: {history_dict}\")\n\n    return TrainingResult(\n        model=model,\n        history=history_dict,\n        metrics=final_metrics,\n        config=config,\n        training_time=training_time,\n        best_epoch=None,  # Keras history doesn't explicitly track \"best\" unless using callbacks\n    )\n</code></pre>"},{"location":"api/frameworks/keras.html#evaluation","title":"Evaluation","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators","title":"mlpotion.frameworks.keras.evaluation.evaluators","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator","title":"ModelEvaluator  <code>dataclass</code>","text":"<p>         Bases: <code>ModelEvaluatorProtocol[Model, Sequence]</code></p> <p>Generic evaluator for Keras 3 models.</p> <p>This class implements the <code>ModelEvaluatorProtocol</code> for Keras models. It wraps the <code>model.evaluate()</code> method to provide a consistent evaluation interface.</p> <p>It ensures that the evaluation result is always returned as a dictionary of metric names to values, regardless of how the model was compiled or what arguments were passed.</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelEvaluator\n\n# Prepare data\nX_test = np.random.rand(20, 10)\ny_test = np.random.randint(0, 2, 20)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize evaluator\nevaluator = ModelEvaluator()\n\n# Evaluate\nmetrics = evaluator.evaluate(\n    model=model,\n    data=(X_test, y_test),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    eval_params={\"batch_size\": 32}\n)\n\nprint(metrics)  # {'loss': 0.693..., 'accuracy': 0.5...}\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a Keras model on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to evaluate.</p> required <code>dataset</code> <code>Any</code> <p>The evaluation data. Can be a tuple <code>(x, y)</code>, a dictionary, or a <code>Sequence</code>.</p> required <code>config</code> <code>ModelEvaluationConfig</code> <p>Configuration object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>An object containing the evaluation metrics.</p> Source code in <code>mlpotion/frameworks/keras/evaluation/evaluators.py</code> <pre><code>@trycatch(\n    error=ModelEvaluatorError,\n    success_msg=\"\u2705 Successfully evaluated Keras model\",\n)\ndef evaluate(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a Keras model on the given data.\n\n    Args:\n        model: The Keras model to evaluate.\n        dataset: The evaluation data. Can be a tuple `(x, y)`, a dictionary, or a `Sequence`.\n        config: Configuration object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: An object containing the evaluation metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare eval parameters\n    eval_kwargs = {\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"return_dict\": True,\n    }\n\n    # Add any framework-specific options\n    eval_kwargs.update(config.framework_options)\n\n    # We assume the model is already compiled. If not, Keras will raise an error\n    # unless we provide compile params, but EvaluationConfig doesn't typically carry them.\n    # The user should ensure the model is compiled (e.g. after loading or training).\n    if not self._is_compiled(model):\n        logger.warning(\n            \"Model is not compiled. Evaluation might fail if loss/metrics are not defined.\"\n        )\n\n    logger.info(\"Evaluating Keras model...\")\n    logger.debug(f\"Evaluation data type: {type(dataset)!r}\")\n    logger.debug(f\"Evaluation parameters: {eval_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    result = self._call_evaluate(model=model, data=dataset, eval_kwargs=eval_kwargs)\n\n    evaluation_time = time.time() - start_time\n\n    # At this point, result should be a dict[str, float]\n    if not isinstance(result, dict):\n        # Defensive fallback if user or Keras changed behavior\n        logger.warning(\n            f\"`model.evaluate` did not return a dict (got {type(result)!r}). \"\n            \"Wrapping into a dict under key 'metric_0'.\"\n        )\n        result = {\"metric_0\": float(result)}\n\n    metrics = {str(k): float(v) for k, v in result.items()}\n    logger.info(f\"Evaluation result: {metrics}\")\n\n    return EvaluationResult(\n        metrics=metrics,\n        config=config,\n        evaluation_time=evaluation_time,\n    )\n</code></pre>"},{"location":"api/frameworks/keras.html#persistence","title":"Persistence","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence","title":"mlpotion.frameworks.keras.deployment.persistence","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence","title":"ModelPersistence","text":"<pre><code>ModelPersistence(\n    path: str | Path, model: Model | None = None\n) -&gt; None\n</code></pre> <p>         Bases: <code>ModelPersistenceProtocol[Model]</code></p> <p>Persistence helper for Keras models.</p> <p>This class manages saving and loading of Keras models. It supports standard Keras formats (<code>.keras</code>, <code>.h5</code>) and SavedModel directories. It also integrates with <code>ModelInspector</code> to provide model metadata upon loading.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Path</code> <p>The file path for the model artifact.</p> <code>model</code> <code>Model | None</code> <p>The Keras model instance (optional).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelPersistence\n\n# Define model\nmodel = keras.Sequential([keras.layers.Dense(1)])\n\n# Save\nsaver = ModelPersistence(path=\"models/my_model.keras\", model=model)\nsaver.save()\n\n# Load\nloader = ModelPersistence(path=\"models/my_model.keras\")\nloaded_model, metadata = loader.load(inspect=True)\nprint(metadata['parameters'])\n</code></pre> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>def __init__(self, path: str | Path, model: Model | None = None) -&gt; None:\n    self._path = Path(path)\n    self._model = model\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence-attributes","title":"Attributes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.model","title":"model  <code>writable</code> <code>property</code>","text":"<pre><code>model: Model | None\n</code></pre> <p>Currently attached Keras model (may be None before loading).</p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.path","title":"path  <code>writable</code> <code>property</code>","text":"<pre><code>path: Path\n</code></pre> <p>Filesystem path where the model is saved/loaded.</p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.load","title":"load","text":"<pre><code>load(\n    *, inspect: bool = True, **kwargs: Any\n) -&gt; tuple[Model, dict[str, Any] | None]\n</code></pre> <p>Load a Keras model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inspect</code> <code>bool</code> <p>Whether to inspect the loaded model and return metadata.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>keras.models.load_model()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Model</code> <p>tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and</p> <code>dict[str, Any] | None</code> <p>optional inspection metadata.</p> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If the model file cannot be found or loaded.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully loaded Keras model\",\n)\ndef load(\n    self,\n    *,\n    inspect: bool = True,\n    **kwargs: Any,\n) -&gt; tuple[Model, dict[str, Any] | None]:\n    \"\"\"Load a Keras model from disk.\n\n    Args:\n        inspect: Whether to inspect the loaded model and return metadata.\n        **kwargs: Additional arguments passed to `keras.models.load_model()`.\n\n    Returns:\n        tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and\n        optional inspection metadata.\n\n    Raises:\n        ModelPersistenceError: If the model file cannot be found or loaded.\n    \"\"\"\n    path = self._ensure_path_exists()\n\n    logger.info(f\"Loading Keras model from: {path!s}\")\n    model = keras.models.load_model(path.as_posix(), **kwargs)\n\n    self._model = model  # keep instance in sync\n\n    inspection_result: dict[str, Any] | None = None\n    if inspect:\n        logger.info(\"Inspecting loaded Keras model with ModelInspector.\")\n        inspector = ModelInspector()\n        inspection_result = inspector.inspect(model)\n\n    return model, inspection_result\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.save","title":"save","text":"<pre><code>save(overwrite: bool = True, **kwargs: Any) -&gt; None\n</code></pre> <p>Save the attached model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>model.save()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If no model is attached or if the file exists and <code>overwrite</code> is False.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully saved Keras model\",\n)\ndef save(\n    self,\n    overwrite: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the attached model to disk.\n\n    Args:\n        overwrite: Whether to overwrite the file if it already exists.\n        **kwargs: Additional arguments passed to `model.save()`.\n\n    Raises:\n        ModelPersistenceError: If no model is attached or if the file exists and `overwrite` is False.\n    \"\"\"\n    model = self._ensure_model()\n    target = self._path\n\n    if target.exists() and not overwrite:\n        raise ModelPersistenceError(\n            f\"Target path already exists and overwrite=False: {target!s}\"\n        )\n\n    logger.info(f\"Saving Keras model to: {target!s}\")\n    target.parent.mkdir(parents=True, exist_ok=True)\n\n    # Keras 3 generally infers format from the path; `save_format` is\n    # deprecated / discouraged in newer APIs, so we do NOT pass it.\n    model.save(target.as_posix(), **kwargs)\n    logger.info(\"Keras model saved successfully.\")\n</code></pre>"},{"location":"api/frameworks/keras.html#export","title":"Export","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters","title":"mlpotion.frameworks.keras.deployment.exporters","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter","title":"ModelExporter","text":"<p>         Bases: <code>ModelExporterProtocol[Model]</code></p> <p>Generic exporter for Keras 3 models.</p> <p>This class implements <code>ModelExporterProtocol</code> and supports exporting Keras models to various formats, including native Keras formats (<code>.keras</code>, <code>.h5</code>) and inference formats like TensorFlow SavedModel or ONNX (via <code>model.export</code>).</p> <p>It also supports creating export archives with custom endpoints using <code>keras.export.ExportArchive</code>.</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelExporter\n\nmodel = keras.Sequential([keras.layers.Dense(1)])\nexporter = ModelExporter()\n\n# Export as standard Keras file\nexporter.export(model, \"models/model.keras\")\n\n# Export for serving (TF SavedModel)\nexporter.export(model, \"models/serving\", export_format=\"tf_saved_model\")\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter.export","title":"export","text":"<pre><code>export(model: Model, path: str, **kwargs: Any) -&gt; None\n</code></pre> <p>Export a Keras model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to export.</p> required <code>path</code> <code>str</code> <p>The destination path or directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional export options: - <code>export_format</code> (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc. - <code>dataset</code> (Iterable): Optional data for model warmup. - <code>endpoint_name</code> (str): Name for custom endpoint (uses ExportArchive). - <code>input_specs</code> (list[InputSpec]): Input signatures for custom endpoint. - <code>config</code> (dict): Extra arguments for the underlying save/export method.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelExporterError</code> <p>If export fails.</p> Source code in <code>mlpotion/frameworks/keras/deployment/exporters.py</code> <pre><code>@trycatch(\n    error=ModelExporterError,\n    success_msg=\"\u2705 Successfully Exported model\",\n)\ndef export(self, model: Model, path: str, **kwargs: Any) -&gt; None:\n    \"\"\"Export a Keras model to disk.\n\n    Args:\n        model: The Keras model to export.\n        path: The destination path or directory.\n        **kwargs: Additional export options:\n            - `export_format` (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc.\n            - `dataset` (Iterable): Optional data for model warmup.\n            - `endpoint_name` (str): Name for custom endpoint (uses ExportArchive).\n            - `input_specs` (list[InputSpec]): Input signatures for custom endpoint.\n            - `config` (dict): Extra arguments for the underlying save/export method.\n\n    Raises:\n        ModelExporterError: If export fails.\n    \"\"\"\n    export_path = Path(path)\n\n    export_format: str | None = kwargs.pop(\"export_format\", None)\n    dataset: Iterable[Any] | None = kwargs.pop(\"dataset\", None)\n    endpoint_name: str | None = kwargs.pop(\"endpoint_name\", None)\n    input_specs: Sequence[InputSpec] | None = kwargs.pop(\"input_specs\", None)\n    config: Mapping[str, Any] | None = kwargs.pop(\"config\", None)\n\n    if kwargs:\n        logger.warning(\n            \"Unused export kwargs passed to ModelExporter: \"\n            f\"{list(kwargs.keys())}\"\n        )\n\n    self._validate_model(model)\n    self._validate_config(config)\n\n    # Determine mode if export_format isn't explicitly set\n    if export_format is None:\n        export_format = self._infer_export_format_from_path(export_path)\n\n    logger.info(\n        f\"Exporting Keras model '{model.name}' to {export_path!s} \"\n        f\"with format '{export_format}'\"\n    )\n\n    # Optional warm-up pass\n    self._warmup_if_needed(model=model, dataset=dataset)\n\n    # Choose strategy\n    try:\n        if self._is_native_keras_format(export_format):\n            self._save_native_keras(model=model, path=export_path, config=config)\n        elif endpoint_name is not None or input_specs is not None:\n            self._export_with_export_archive(\n                model=model,\n                path=export_path,\n                endpoint_name=endpoint_name or self.default_endpoint_name,\n                input_specs=input_specs,\n                export_format=export_format,\n            )\n        else:\n            self._export_with_model_export(\n                model=model,\n                path=export_path,\n                export_format=export_format,\n                config=config,\n            )\n    except ValueError as err:\n        logger.warning(\n            f\"Export error: {err} \"\n            \"(you may need to build the model by calling it on example data \"\n            \"before exporting)\"\n        )\n\n    logger.info(f\"Model export completed: {export_path!s}\")\n</code></pre>"},{"location":"api/frameworks/keras.html#model-inspection","title":"Model Inspection","text":"<p> See the Keras Guide for usage examples </p>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection","title":"mlpotion.frameworks.keras.models.inspection","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection-classes","title":"Classes","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection.ModelInspector","title":"ModelInspector  <code>dataclass</code>","text":"<p>         Bases: <code>ModelInspectorProtocol[ModelLike]</code></p> <p>Inspector for Keras models.</p> <p>This class analyzes Keras models to extract metadata such as input/output shapes, parameter counts, layer details, and signatures. It is useful for validating models before training or deployment, and for generating model reports.</p> <p>Attributes:</p> Name Type Description <code>include_layers</code> <code>bool</code> <p>Whether to include detailed information about each layer.</p> <code>include_signatures</code> <code>bool</code> <p>Whether to include model signatures (if available).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelInspector\n\nmodel = keras.Sequential([keras.layers.Dense(1, input_shape=(10,))])\ninspector = ModelInspector()\n\ninfo = inspector.inspect(model)\nprint(f\"Total params: {info['parameters']['total']}\")\nprint(f\"Inputs: {info['inputs']}\")\n</code></pre>"},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection.ModelInspector-functions","title":"Functions","text":""},{"location":"api/frameworks/keras.html#mlpotion.frameworks.keras.models.inspection.ModelInspector.inspect","title":"inspect","text":"<pre><code>inspect(model: ModelLike) -&gt; dict[str, Any]\n</code></pre> <p>Inspect a Keras model and return structured metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelLike</code> <p>The Keras model to inspect.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing model metadata: - <code>name</code>: Model name. - <code>backend</code>: Keras backend used. - <code>trainable</code>: Whether the model is trainable. - <code>inputs</code>: List of input specifications. - <code>outputs</code>: List of output specifications. - <code>parameters</code>: Dictionary of parameter counts. - <code>layers</code>: List of layer details (if <code>include_layers=True</code>). - <code>signatures</code>: Model signatures (if <code>include_signatures=True</code>).</p> Source code in <code>mlpotion/frameworks/keras/models/inspection.py</code> <pre><code>@trycatch(\n    error=ModelInspectorError,\n    success_msg=\"\u2705 Successfully inspected Keras model\",\n)\ndef inspect(self, model: ModelLike) -&gt; dict[str, Any]:\n    \"\"\"Inspect a Keras model and return structured metadata.\n\n    Args:\n        model: The Keras model to inspect.\n\n    Returns:\n        dict[str, Any]: A dictionary containing model metadata:\n            - `name`: Model name.\n            - `backend`: Keras backend used.\n            - `trainable`: Whether the model is trainable.\n            - `inputs`: List of input specifications.\n            - `outputs`: List of output specifications.\n            - `parameters`: Dictionary of parameter counts.\n            - `layers`: List of layer details (if `include_layers=True`).\n            - `signatures`: Model signatures (if `include_signatures=True`).\n    \"\"\"\n    if not isinstance(model, keras.Model):\n        raise TypeError(\n            f\"ModelInspector expects a keras.Model, got {type(model)!r}\"\n        )\n\n    logger.info(\"Inspecting Keras model...\")\n\n    backend_name = self._get_backend_name()\n\n    info: dict[str, Any] = {\n        \"name\": model.name,\n        \"backend\": backend_name,\n        \"trainable\": model.trainable,\n    }\n\n    info[\"inputs\"] = self._get_inputs(model)\n    info[\"input_names\"] = [input[\"name\"] for input in info[\"inputs\"]]\n    info[\"outputs\"] = self._get_outputs(model)\n    info[\"output_names\"] = [output[\"name\"] for output in info[\"outputs\"]]\n    info[\"parameters\"] = self._get_param_counts(model)\n\n    if self.include_signatures:\n        info[\"signatures\"] = self._get_signatures(model)\n\n    if self.include_layers:\n        info[\"layers\"] = self._get_layers_summary(model)\n\n    logger.debug(f\"Keras model inspection result: {info}\")\n    return info\n</code></pre>"},{"location":"api/frameworks/pytorch.html","title":"PyTorch API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's PyTorch components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p> <p>Extensibility</p> <p>These components are built using protocol-based design, making MLPotion easy to extend. Want to add new data sources, training methods, or integrations? See Contributing Guide.</p>"},{"location":"api/frameworks/pytorch.html#data-loading","title":"Data Loading","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets","title":"mlpotion.frameworks.pytorch.data.datasets","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset","title":"CSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>Dataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>PyTorch Dataset for CSV files with on-demand tensor conversion.</p> <p>This class loads CSV data into memory (using Pandas) and provides a map-style PyTorch Dataset. It supports filtering columns, separating labels, and efficient on-demand tensor conversion to minimize memory usage.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load. If None, all columns are loaded.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label. If None, no labels are returned.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features (default: <code>torch.float32</code>).</p> Example <pre><code>from mlpotion.frameworks.pytorch import CSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create dataset\ndataset = CSVDataset(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    column_names=[\"feature1\", \"feature2\", \"target_class\"]\n)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Iterate\nfor features, labels in dataloader:\n    print(features.shape, labels.shape)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n</code></pre> <p>Get item at index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global row index.</p> required <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor] | torch.Tensor</code> <p>(features, label) tuple if labels exist, else just features.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __getitem__(\n    self,\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n    \"\"\"Get item at index.\n\n    Args:\n        idx: Global row index.\n\n    Returns:\n        (features, label) tuple if labels exist, else just features.\n    \"\"\"\n    if self._features_df is None:\n        raise IndexError(\"Dataset is empty or not properly initialized.\")\n\n    row = self._features_df.iloc[idx]\n\n    # Convert to numpy array and then to tensor\n    features_np: np.ndarray = row.to_numpy(dtype=\"float32\", copy=False)\n    features = torch.as_tensor(features_np, dtype=self._dtype)\n\n    if self._labels is not None:\n        label_val = self._labels[idx]\n        label = torch.as_tensor(label_val, dtype=self._dtype)\n        return features, label\n\n    return features\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return dataset length.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return dataset length.\"\"\"\n    if self._features_df is None:\n        return 0\n    return len(self._features_df)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.CSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Eagerly load CSV files into a DataFrame and validate configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Eagerly load CSV files into a DataFrame and validate configuration.\"\"\"\n    try:\n        files = self._resolve_files()\n        df = self._load_dataframe(files)\n        df = self._select_columns(df)\n        self._split_features_labels(df)\n\n        logger.info(\n            \"Initialized CSVDataset with \"\n            \"n_rows={rows}, n_features={features}, labels={labels}\",\n            rows=len(self._features_df) if self._features_df is not None else 0,\n            features=len(self._feature_cols),\n            labels=\"yes\" if self._labels is not None else \"no\",\n        )\n    except DataLoadingError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise DataLoadingError(f\"Failed to load CSV dataset: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset","title":"StreamingCSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>IterableDataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>Streaming PyTorch IterableDataset for large CSV files.</p> <p>This dataset is designed for datasets that are too large to fit in memory. It reads CSV files in chunks (using Pandas) and streams samples one by one. It is compatible with PyTorch's <code>IterableDataset</code> interface.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load.</p> <code>label_name</code> <code>str | None</code> <p>Name of the label column.</p> <code>chunksize</code> <code>int</code> <p>Number of rows to read into memory at a time per file.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features.</p> Example <pre><code>from mlpotion.frameworks.pytorch import StreamingCSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create streaming dataset\ndataset = StreamingCSVDataset(\n    file_pattern=\"data/large_dataset_*.csv\",\n    label_name=\"target\",\n    chunksize=10000\n)\n\n# Create DataLoader (shuffle must be False for IterableDataset)\ndataloader = DataLoader(dataset, batch_size=64)\n\nfor features, labels in dataloader:\n    # Train model...\n    pass\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; (\n    Iterator[\n        tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n    ]\n)\n</code></pre> <p>Yield samples one by one across all CSV files.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]:\n    \"\"\"Yield samples one by one across all CSV files.\"\"\"\n    for file_path in self.files:\n        logger.info(f\"Streaming CSV file: {file_path}\")\n        try:\n            # Use pandas chunked reading\n            chunk_iter = pd.read_csv(\n                file_path,\n                usecols=self.column_names,\n                chunksize=self.chunksize,\n            )\n        except TypeError:\n            # If usecols=None is not accepted by some pandas version\n            chunk_iter = pd.read_csv(\n                file_path,\n                chunksize=self.chunksize,\n            )\n\n        for chunk_df in chunk_iter:\n            # Validate label column if needed\n            if self.label_name:\n                if self.label_name not in chunk_df.columns:\n                    raise DataLoadingError(\n                        f\"Label column '{self.label_name}' not found in \"\n                        f\"file {file_path} (columns: {list(chunk_df.columns)})\"\n                    )\n                labels_np = chunk_df[self.label_name].to_numpy()\n                features_df = chunk_df.drop(columns=[self.label_name])\n            else:\n                labels_np = None\n                features_df = chunk_df\n\n            # Convert whole chunk to numpy once\n            features_np = features_df.to_numpy(dtype=\"float32\", copy=False)\n\n            if labels_np is not None:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    y = torch.as_tensor(labels_np[row_idx], dtype=self.dtype)\n                    yield x, y\n            else:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    yield x\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.datasets.StreamingCSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Resolve files eagerly and log basic configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/datasets.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Resolve files eagerly and log basic configuration.\"\"\"\n    self.files = self._resolve_files()\n    logger.info(\n        \"Initialized StreamingCSVDataset with {n_files} file(s), \"\n        \"chunksize={chunksize}, label_name={label}\",\n        n_files=len(self.files),\n        chunksize=self.chunksize,\n        label=self.label_name,\n    )\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders","title":"mlpotion.frameworks.pytorch.data.loaders","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataLoader","title":"CSVDataLoader  <code>dataclass</code>","text":"<p>         Bases: <code>Generic[T_co]</code></p> <p>Factory for creating configured PyTorch DataLoaders.</p> <p>This class simplifies the creation of <code>torch.utils.data.DataLoader</code> instances by encapsulating common configuration options and handling differences between map-style and iterable datasets (e.g., automatically disabling shuffling for iterables).</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data (ignored for IterableDatasets).</p> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>pin_memory</code> <code>bool</code> <p>Whether to copy tensors into CUDA pinned memory.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch.</p> <code>persistent_workers</code> <code>bool | None</code> <p>Whether to keep workers alive between epochs.</p> <code>prefetch_factor</code> <code>int | None</code> <p>Number of batches loaded in advance by each worker.</p> Example <pre><code>from mlpotion.frameworks.pytorch import CSVDataLoader, CSVDataset\n\n# 1. Create a dataset\ndataset = CSVDataset(\"data.csv\", label_name=\"target\")\n\n# 2. Configure the loader factory\nloader_factory = CSVDataLoader(\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\n# 3. Create the actual DataLoader\ntrain_loader = loader_factory.load(dataset)\n\n# 4. Use it\nfor X, y in train_loader:\n    ...\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataLoader.load","title":"load","text":"<pre><code>load(\n    dataset: Dataset[T_co] | IterableDataset[T_co],\n) -&gt; DataLoader[T_co]\n</code></pre> <p>Load a configured :class:<code>DataLoader</code> from a dataset.</p> <p>This method is aware of :class:<code>IterableDataset</code> vs map-style :class:<code>Dataset</code> and will:</p> <ul> <li>Disable shuffling for iterable datasets (with a warning if   <code>shuffle=True</code> was requested).</li> <li>Apply worker-related options only when valid.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[T_co] | IterableDataset[T_co]</code> <p>PyTorch :class:<code>Dataset</code> or :class:<code>IterableDataset</code>.</p> required <p>Returns:</p> Name Type Description <code>Configured</code> <code>DataLoader[T_co]</code> <p>class:<code>torch.utils.data.DataLoader</code> instance.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully Loading data\",\n)\ndef load(\n    self,\n    dataset: Dataset[T_co] | IterableDataset[T_co],\n) -&gt; DataLoader[T_co]:\n    \"\"\"Load a configured :class:`DataLoader` from a dataset.\n\n    This method is aware of :class:`IterableDataset` vs map-style\n    :class:`Dataset` and will:\n\n    - Disable shuffling for iterable datasets (with a warning if\n      ``shuffle=True`` was requested).\n    - Apply worker-related options only when valid.\n\n    Args:\n        dataset: PyTorch :class:`Dataset` or :class:`IterableDataset`.\n\n    Returns:\n        Configured :class:`torch.utils.data.DataLoader` instance.\n    \"\"\"\n    is_iterable = isinstance(dataset, IterableDataset)\n    effective_shuffle = self._resolve_shuffle(is_iterable=is_iterable)\n\n    loader_kwargs = self._build_loader_kwargs(\n        dataset=dataset,\n        shuffle=effective_shuffle,\n        is_iterable=is_iterable,\n    )\n\n    logger.info(\n        \"Creating DataLoader with config: \"\n        \"batch_size={batch_size}, shuffle={shuffle}, \"\n        \"num_workers={num_workers}, pin_memory={pin_memory}, \"\n        \"drop_last={drop_last}, persistent_workers={persistent_workers}, \"\n        \"prefetch_factor={prefetch_factor}, dataset_type={dtype}\",\n        batch_size=self.batch_size,\n        shuffle=effective_shuffle,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=self.drop_last,\n        persistent_workers=self.persistent_workers,\n        prefetch_factor=self.prefetch_factor,\n        dtype=\"IterableDataset\" if is_iterable else \"Dataset\",\n    )\n\n    return DataLoader(**loader_kwargs)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset","title":"CSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>Dataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>PyTorch Dataset for CSV files with on-demand tensor conversion.</p> <p>This class loads CSV data into memory (using Pandas) and provides a map-style PyTorch Dataset. It supports filtering columns, separating labels, and efficient on-demand tensor conversion to minimize memory usage.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load. If None, all columns are loaded.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label. If None, no labels are returned.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features (default: <code>torch.float32</code>).</p> Example <pre><code>from mlpotion.frameworks.pytorch import CSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create dataset\ndataset = CSVDataset(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    column_names=[\"feature1\", \"feature2\", \"target_class\"]\n)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Iterate\nfor features, labels in dataloader:\n    print(features.shape, labels.shape)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n</code></pre> <p>Get item at index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global row index.</p> required <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor] | torch.Tensor</code> <p>(features, label) tuple if labels exist, else just features.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __getitem__(\n    self,\n    idx: int,\n) -&gt; tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n    \"\"\"Get item at index.\n\n    Args:\n        idx: Global row index.\n\n    Returns:\n        (features, label) tuple if labels exist, else just features.\n    \"\"\"\n    if self._features_df is None:\n        raise IndexError(\"Dataset is empty or not properly initialized.\")\n\n    row = self._features_df.iloc[idx]\n\n    # Convert to numpy array and then to tensor\n    features_np: np.ndarray = row.to_numpy(dtype=\"float32\", copy=False)\n    features = torch.as_tensor(features_np, dtype=self._dtype)\n\n    if self._labels is not None:\n        label_val = self._labels[idx]\n        label = torch.as_tensor(label_val, dtype=self._dtype)\n        return features, label\n\n    return features\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return dataset length.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return dataset length.\"\"\"\n    if self._features_df is None:\n        return 0\n    return len(self._features_df)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.CSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Eagerly load CSV files into a DataFrame and validate configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Eagerly load CSV files into a DataFrame and validate configuration.\"\"\"\n    try:\n        files = self._resolve_files()\n        df = self._load_dataframe(files)\n        df = self._select_columns(df)\n        self._split_features_labels(df)\n\n        logger.info(\n            \"Initialized CSVDataset with \"\n            \"n_rows={rows}, n_features={features}, labels={labels}\",\n            rows=len(self._features_df) if self._features_df is not None else 0,\n            features=len(self._feature_cols),\n            labels=\"yes\" if self._labels is not None else \"no\",\n        )\n    except DataLoadingError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise DataLoadingError(f\"Failed to load CSV dataset: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset","title":"StreamingCSVDataset  <code>dataclass</code>","text":"<p>         Bases: <code>IterableDataset[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]</code></p> <p>Streaming PyTorch IterableDataset for large CSV files.</p> <p>This dataset is designed for datasets that are too large to fit in memory. It reads CSV files in chunks (using Pandas) and streams samples one by one. It is compatible with PyTorch's <code>IterableDataset</code> interface.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load.</p> <code>label_name</code> <code>str | None</code> <p>Name of the label column.</p> <code>chunksize</code> <code>int</code> <p>Number of rows to read into memory at a time per file.</p> <code>dtype</code> <code>torch.dtype</code> <p>The data type for the features.</p> Example <pre><code>from mlpotion.frameworks.pytorch import StreamingCSVDataset\nfrom torch.utils.data import DataLoader\n\n# Create streaming dataset\ndataset = StreamingCSVDataset(\n    file_pattern=\"data/large_dataset_*.csv\",\n    label_name=\"target\",\n    chunksize=10000\n)\n\n# Create DataLoader (shuffle must be False for IterableDataset)\ndataloader = DataLoader(dataset, batch_size=64)\n\nfor features, labels in dataloader:\n    # Train model...\n    pass\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; (\n    Iterator[\n        tuple[torch.Tensor, torch.Tensor] | torch.Tensor\n    ]\n)\n</code></pre> <p>Yield samples one by one across all CSV files.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[tuple[torch.Tensor, torch.Tensor] | torch.Tensor]:\n    \"\"\"Yield samples one by one across all CSV files.\"\"\"\n    for file_path in self.files:\n        logger.info(\"Streaming CSV file: {path}\", path=file_path)\n        try:\n            chunk_iter = pd.read_csv(\n                file_path,\n                usecols=self.column_names,\n                chunksize=self.chunksize,\n            )\n        except TypeError:\n            # If usecols=None is not accepted by some pandas version\n            chunk_iter = pd.read_csv(\n                file_path,\n                chunksize=self.chunksize,\n            )\n\n        for chunk_df in chunk_iter:\n            if self.label_name:\n                if self.label_name not in chunk_df.columns:\n                    raise DataLoadingError(\n                        f\"Label column '{self.label_name}' not found in \"\n                        f\"file {file_path} (columns: {list(chunk_df.columns)})\"\n                    )\n                labels_np = chunk_df[self.label_name].to_numpy()\n                features_df = chunk_df.drop(columns=[self.label_name])\n            else:\n                labels_np = None\n                features_df = chunk_df\n\n            features_np = features_df.to_numpy(dtype=\"float32\", copy=False)\n\n            if labels_np is not None:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    y = torch.as_tensor(labels_np[row_idx], dtype=self.dtype)\n                    yield x, y\n            else:\n                for row_idx in range(features_np.shape[0]):\n                    x = torch.as_tensor(\n                        features_np[row_idx],\n                        dtype=self.dtype,\n                    )\n                    yield x\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.data.loaders.StreamingCSVDataset.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Resolve files eagerly and log basic configuration.</p> Source code in <code>mlpotion/frameworks/pytorch/data/loaders.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Resolve files eagerly and log basic configuration.\"\"\"\n    self.files = self._resolve_files()\n    logger.info(\n        \"Initialized StreamingCSVDataset with {n_files} file(s), \"\n        \"chunksize={chunksize}, label_name={label}\",\n        n_files=len(self.files),\n        chunksize=self.chunksize,\n        label=self.label_name,\n    )\n</code></pre>"},{"location":"api/frameworks/pytorch.html#training","title":"Training","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers","title":"mlpotion.frameworks.pytorch.training.trainers","text":"<p>PyTorch model training.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers.ModelTrainer","title":"ModelTrainer","text":"<p>         Bases: <code>ModelTrainerProtocol[nn.Module, DataLoader]</code></p> <p>Generic trainer for PyTorch models.</p> <p>This class implements the <code>ModelTrainerProtocol</code> for PyTorch models. It handles the training loop, device placement, loss calculation, backpropagation, and validation.</p> <p>It supports: - Supervised learning (batch is <code>(inputs, targets)</code>). - Unsupervised/Self-supervised learning (batch is <code>inputs</code> only, loss is <code>fn(outputs, inputs)</code>). - Custom loss functions (string alias, <code>nn.Module</code>, or callable). - Automatic device management (CPU/GPU).</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>nn.Module</code> <p>The PyTorch model to train.</p> <code>dataloader</code> <code>DataLoader</code> <p>The training data loader.</p> <code>config</code> <code>ModelTrainingConfig</code> <p>Configuration for training (epochs, optimizer, etc.).</p> Example <pre><code>import torch\nimport torch.nn as nn\nfrom mlpotion.frameworks.pytorch import ModelTrainer\nfrom mlpotion.frameworks.pytorch.config import ModelTrainingConfig\n\n# Define model\nmodel = nn.Linear(10, 1)\n\n# Define config\nconfig = ModelTrainingConfig(\n    epochs=5,\n    learning_rate=0.01,\n    optimizer=\"adam\",\n    loss_fn=\"mse\",\n    device=\"cpu\"\n)\n\n# Initialize trainer\ntrainer = ModelTrainer()\n\n# Train\nresult = trainer.train(model, train_loader, config, val_loader)\nprint(result.metrics)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.training.trainers.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelTrainingConfig,\n    validation_dataloader: DataLoader[Any] | None = None,\n) -&gt; TrainingResult[nn.Module]\n</code></pre> <p>Train a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model (<code>nn.Module</code>) to train.</p> required <code>dataloader</code> <code>DataLoader[Any]</code> <p>The <code>DataLoader</code> providing training data.</p> required <code>config</code> <code>ModelTrainingConfig</code> <p>A <code>ModelTrainingConfig</code> object containing training parameters.</p> required <code>validation_dataloader</code> <code>DataLoader[Any] | None</code> <p>Optional <code>DataLoader</code> for validation.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[nn.Module]</code> <p>TrainingResult[nn.Module]: A dataclass containing the trained model,</p> <code>TrainingResult[nn.Module]</code> <p>training history (loss/metrics per epoch), and final metrics.</p> <p>Raises:</p> Type Description <code>TrainingError</code> <p>If the training loop encounters an error (e.g., NaN loss,</p> Source code in <code>mlpotion/frameworks/pytorch/training/trainers.py</code> <pre><code>@trycatch(\n    error=ModelTrainerError,\n    success_msg=\"\u2705 Successfully trained PyTorch model\",\n)\ndef train(\n    self,\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelTrainingConfig,\n    validation_dataloader: DataLoader[Any] | None = None,\n) -&gt; TrainingResult[nn.Module]:\n    \"\"\"Train a PyTorch model.\n\n    Args:\n        model: The PyTorch model (`nn.Module`) to train.\n        dataloader: The `DataLoader` providing training data.\n        config: A `ModelTrainingConfig` object containing training parameters.\n        validation_dataloader: Optional `DataLoader` for validation.\n\n    Returns:\n        TrainingResult[nn.Module]: A dataclass containing the trained model,\n        training history (loss/metrics per epoch), and final metrics.\n\n    Raises:\n        TrainingError: If the training loop encounters an error (e.g., NaN loss,\n        device mismatch, empty dataloader).\n    \"\"\"\n    try:\n        logger.info(\"Starting PyTorch model training...\")\n        logger.info(\n            \"Config: epochs={epochs}, lr={lr}, optimizer={opt}, \"\n            \"loss_fn={loss_fn}, device={device}\",\n            epochs=config.epochs,\n            lr=config.learning_rate,\n            opt=config.optimizer,\n            loss_fn=config.loss_fn,\n            device=config.device,\n        )\n\n        # Setup device\n        device = torch.device(config.device)\n        model = model.to(device)\n\n        # Setup optimizer and loss\n        optimizer = self._create_optimizer(model, config)\n        criterion = self._create_loss_fn(config)\n\n        # Optional limit on batches per epoch\n        max_batches_per_epoch = getattr(config, \"max_batches_per_epoch\", None)\n        if max_batches_per_epoch is None:\n            max_batches_per_epoch = getattr(config, \"max_batches\", None)\n\n        history: dict[str, list[float]] = {\"loss\": []}\n        if validation_dataloader is not None:\n            history[\"val_loss\"] = []\n\n        # Initialize callbacks and TensorBoard\n        callbacks = self._prepare_callbacks(config)\n        tensorboard_writer = self._setup_tensorboard(config)\n\n        start_time = time.time()\n\n        # Call on_train_begin callbacks\n        for callback in callbacks:\n            if hasattr(callback, \"on_train_begin\"):\n                callback.on_train_begin()\n\n        for epoch in range(config.epochs):\n            model.train()\n            epoch_loss = 0.0\n            num_batches = 0\n\n            for batch in dataloader:\n                inputs, targets = self._prepare_batch(batch, device=device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n\n                # Supervised vs unsupervised / autoencoder\n                if targets is not None:\n                    loss = criterion(outputs, targets)\n                else:\n                    loss = criterion(outputs, inputs)\n\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += float(loss.item())\n                num_batches += 1\n\n                if (\n                    max_batches_per_epoch is not None\n                    and num_batches &gt;= max_batches_per_epoch\n                ):\n                    logger.info(\n                        \"Reached max_batches_per_epoch={mb}; \"\n                        \"stopping epoch {epoch} early.\",\n                        mb=max_batches_per_epoch,\n                        epoch=epoch + 1,\n                    )\n                    break\n\n            if num_batches == 0:\n                raise TrainingError(\"Training dataloader yielded no batches.\")\n\n            avg_loss = epoch_loss / num_batches\n            history[\"loss\"].append(avg_loss)\n\n            # Validation phase\n            if validation_dataloader is not None:\n                val_loss = self._validate(\n                    model=model,\n                    dataloader=validation_dataloader,\n                    criterion=criterion,\n                    device=device,\n                )\n                history[\"val_loss\"].append(val_loss)\n            else:\n                val_loss = None\n\n            # Logging\n            if getattr(config, \"verbose\", True):\n                msg = f\"Epoch {epoch + 1}/{config.epochs} - loss: {avg_loss:.4f}\"\n                if val_loss is not None:\n                    msg += f\" - val_loss: {val_loss:.4f}\"\n                logger.info(msg)\n\n            # TensorBoard logging\n            if tensorboard_writer is not None:\n                tensorboard_writer.add_scalar(\"loss\", avg_loss, epoch)\n                if val_loss is not None:\n                    tensorboard_writer.add_scalar(\"val_loss\", val_loss, epoch)\n\n            # Call on_epoch_end callbacks\n            for callback in callbacks:\n                if hasattr(callback, \"on_epoch_end\"):\n                    callback.on_epoch_end(\n                        epoch, {\"loss\": avg_loss, \"val_loss\": val_loss}\n                    )\n\n        training_time = time.time() - start_time\n\n        # Final metrics\n        metrics: dict[str, float] = {\"loss\": float(history[\"loss\"][-1])}\n        if \"val_loss\" in history and history[\"val_loss\"]:\n            metrics[\"val_loss\"] = float(history[\"val_loss\"][-1])\n\n        best_epoch = self._find_best_epoch(history)\n\n        # Call on_train_end callbacks\n        for callback in callbacks:\n            if hasattr(callback, \"on_train_end\"):\n                callback.on_train_end()\n\n        # Close TensorBoard writer\n        if tensorboard_writer is not None:\n            tensorboard_writer.close()\n\n        logger.info(\"Training completed in {t:.2f}s\", t=training_time)\n        logger.info(\"Final metrics: {metrics}\", metrics=metrics)\n\n        return TrainingResult(\n            model=model,\n            history=history,\n            metrics=metrics,\n            config=config,\n            training_time=training_time,\n            best_epoch=best_epoch,\n        )\n\n    except TrainingError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise TrainingError(f\"Training failed: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#evaluation","title":"Evaluation","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators","title":"mlpotion.frameworks.pytorch.evaluation.evaluators","text":"<p>PyTorch model evaluation.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators.ModelEvaluator","title":"ModelEvaluator","text":"<p>         Bases: <code>ModelEvaluatorProtocol[nn.Module, DataLoader]</code></p> <p>Generic evaluator for PyTorch models.</p> <p>This class implements the <code>ModelEvaluatorProtocol</code> for PyTorch models. It performs a full pass over the evaluation dataset, computing the average loss.</p> <p>It supports: - Supervised and unsupervised evaluation. - Custom loss functions. - Automatic device management.</p> Example <pre><code>from mlpotion.frameworks.pytorch import ModelEvaluator\nfrom mlpotion.frameworks.pytorch.config import ModelEvaluationConfig\n\nevaluator = ModelEvaluator()\nconfig = ModelEvaluationConfig(loss_fn=\"cross_entropy\", device=\"cuda\")\n\nresult = evaluator.evaluate(model, test_loader, config)\nprint(f\"Test Loss: {result.metrics['loss']}\")\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.evaluation.evaluators.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to evaluate.</p> required <code>dataloader</code> <code>DataLoader[Any]</code> <p>The <code>DataLoader</code> providing evaluation data.</p> required <code>config</code> <code>ModelEvaluationConfig</code> <p>A <code>ModelEvaluationConfig</code> object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>A dataclass containing the computed metrics (e.g., average loss)</p> <code>EvaluationResult</code> <p>and execution time.</p> <p>Raises:</p> Type Description <code>EvaluationError</code> <p>If evaluation fails.</p> Source code in <code>mlpotion/frameworks/pytorch/evaluation/evaluators.py</code> <pre><code>@trycatch(\n    error=ModelEvaluatorError,\n    success_msg=\"\u2705 Successfully evaluated PyTorch model\",\n)\ndef evaluate(\n    self,\n    model: nn.Module,\n    dataloader: DataLoader[Any],\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a PyTorch model.\n\n    Args:\n        model: The PyTorch model to evaluate.\n        dataloader: The `DataLoader` providing evaluation data.\n        config: A `ModelEvaluationConfig` object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: A dataclass containing the computed metrics (e.g., average loss)\n        and execution time.\n\n    Raises:\n        EvaluationError: If evaluation fails.\n    \"\"\"\n    try:\n        device_str = getattr(config, \"device\", \"cpu\")\n        logger.info(\"Starting PyTorch model evaluation...\")\n        logger.info(\n            f\"Config: device={device_str}, loss_fn={getattr(config, 'loss_fn', 'mse')}\"\n        )\n\n        device = torch.device(device_str)\n        model = model.to(device)\n        model.eval()\n\n        criterion = self._create_loss_fn(config)\n\n        # Support optional max_batches on the config\n        max_batches = getattr(config, \"max_batches\", None)\n\n        total_loss = 0.0\n        num_batches = 0\n        start_time = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                inputs, targets = self._prepare_batch(batch, device=device)\n\n                outputs = model(inputs)\n\n                if targets is not None:\n                    loss = criterion(outputs, targets)\n                else:\n                    loss = criterion(outputs, inputs)\n\n                total_loss += float(loss.item())\n                num_batches += 1\n\n                if max_batches is not None and num_batches &gt;= max_batches:\n                    logger.info(\n                        f\"Reached max_batches={max_batches}; \"\n                        \"stopping evaluation early.\"\n                    )\n                    break\n\n        if num_batches == 0:\n            raise EvaluationError(\"Evaluation dataloader yielded no batches.\")\n\n        avg_loss = total_loss / num_batches\n        evaluation_time = time.time() - start_time\n\n        metrics = {\"loss\": float(avg_loss)}\n\n        logger.info(f\"Evaluation completed in {evaluation_time:.2f}s\")\n        logger.info(f\"Metrics: {metrics}\")\n\n        return EvaluationResult(\n            metrics=metrics,\n            config=config,\n            evaluation_time=evaluation_time,\n        )\n\n    except EvaluationError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise EvaluationError(f\"Evaluation failed: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/pytorch.html#persistence","title":"Persistence","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence","title":"mlpotion.frameworks.pytorch.deployment.persistence","text":"<p>PyTorch model persistence.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence","title":"ModelPersistence  <code>dataclass</code>","text":"<p>         Bases: <code>ModelPersistenceProtocol[nn.Module]</code></p> <p>Persistence helper for PyTorch models.</p> <p>This class manages saving and loading of PyTorch models. It supports two modes: 1. State Dict (Recommended): Saves only the model parameters (<code>model.state_dict()</code>).    Requires the model class to be available when loading. 2. Full Model: Saves the entire model object using pickle. Less portable but easier to load.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str | Path</code> <p>The file path for the model artifact.</p> <code>model</code> <code>nn.Module | None</code> <p>The PyTorch model instance.</p> Example <p>Saving and Loading State Dict (Recommended): <pre><code>from mlpotion.frameworks.pytorch import ModelPersistence\nimport torch.nn as nn\n\n# Define model\nclass MyModel(nn.Module):\n    def __init__(self): super().__init__(); self.l = nn.Linear(1, 1)\n\nmodel = MyModel()\n\n# Save\nsaver = ModelPersistence(path=\"model.pth\", model=model)\nsaver.save(save_full_model=False)\n\n# Load\nloader = ModelPersistence(path=\"model.pth\")\n# We must provide the model class or an instance for state_dict loading\nloaded_model = loader.load(model_class=MyModel)\n</code></pre></p> Example <p>Saving and Loading Full Model: <pre><code># Save\nsaver.save(save_full_model=True)\n\n# Load (no model class needed)\nloader = ModelPersistence(path=\"model.pth\")\nloaded_model = loader.load()\n</code></pre></p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence-attributes","title":"Attributes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence.path_obj","title":"path_obj  <code>writable</code> <code>property</code>","text":"<pre><code>path_obj: Path\n</code></pre> <p>Return the model path as a <code>Path</code>.</p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence.load","title":"load","text":"<pre><code>load(\n    *,\n    model_class: type[nn.Module] | None = None,\n    map_location: str | torch.device | None = \"cpu\",\n    strict: bool = True,\n    model_kwargs: dict[str, Any] | None = None,\n    **torch_load_kwargs: Any\n) -&gt; tuple[nn.Module, dict[str, Any] | None]\n</code></pre> <p>Load a PyTorch model from disk.</p> <p>This method automatically detects if the file is a full model checkpoint or a state dict.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>type[nn.Module] | None</code> <p>The model class to instantiate if loading a state dict and no model instance is currently attached.</p> <code>None</code> <code>map_location</code> <code>str | torch.device | None</code> <p>Device to load the model onto (default: \"cpu\").</p> <code>'cpu'</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce state dict keys match the model.</p> <code>True</code> <code>model_kwargs</code> <code>dict[str, Any] | None</code> <p>Arguments to pass to <code>model_class</code> constructor.</p> <code>None</code> <code>**torch_load_kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>torch.load()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[nn.Module, dict[str, Any] | None]</code> <p>nn.Module: The loaded PyTorch model.</p> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If loading fails, or if <code>model_class</code> is missing</p> Source code in <code>mlpotion/frameworks/pytorch/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully loaded PyTorch model\",\n)\ndef load(\n    self,\n    *,\n    model_class: type[nn.Module] | None = None,\n    map_location: str | torch.device | None = \"cpu\",\n    strict: bool = True,\n    model_kwargs: dict[str, Any] | None = None,\n    **torch_load_kwargs: Any,\n) -&gt; tuple[nn.Module, dict[str, Any] | None]:\n    \"\"\"Load a PyTorch model from disk.\n\n    This method automatically detects if the file is a full model checkpoint or a\n    state dict.\n\n    Args:\n        model_class: The model class to instantiate if loading a state dict and no\n            model instance is currently attached.\n        map_location: Device to load the model onto (default: \"cpu\").\n        strict: Whether to strictly enforce state dict keys match the model.\n        model_kwargs: Arguments to pass to `model_class` constructor.\n        **torch_load_kwargs: Additional arguments passed to `torch.load()`.\n\n    Returns:\n        nn.Module: The loaded PyTorch model.\n\n    Raises:\n        ModelPersistenceError: If loading fails, or if `model_class` is missing\n        when required.\n    \"\"\"\n    path = self._ensure_path_exists()\n\n    logger.info(\"Loading PyTorch model from {path}\", path=str(path))\n\n    checkpoint = torch.load(path, map_location=map_location, **torch_load_kwargs)\n\n    # Case 1: full model was saved\n    if isinstance(checkpoint, nn.Module):\n        logger.info(\"Detected full-model checkpoint (nn.Module).\")\n        self.model = checkpoint\n        logger.info(\"PyTorch model loaded successfully from full-model checkpoint.\")\n        return checkpoint, None\n\n    # Case 2: dict-like checkpoint (state_dict or wrapped)\n    if isinstance(checkpoint, dict):\n        logger.info(\"Detected dict-like checkpoint; treating as state_dict.\")\n        state_dict = self._extract_state_dict(checkpoint)\n\n        # If we already have a model attached, reuse it; otherwise, we need model_class\n        if self.model is not None:\n            model = self.model\n            logger.info(\n                \"Using attached model instance of type {cls} for state_dict loading.\",\n                cls=type(model).__name__,\n            )\n        else:\n            if model_class is None:\n                raise ModelPersistenceError(\n                    \"model_class is required when loading from a state_dict \"\n                    \"checkpoint if no model is attached.\"\n                )\n            model = self._instantiate_model(model_class, model_kwargs)\n            self.model = model\n\n        missing, unexpected = model.load_state_dict(state_dict, strict=strict)\n\n        if strict:\n            logger.debug(\n                \"State_dict loaded with strict=True (no mismatch error raised).\"\n            )\n        else:\n            if missing:\n                logger.warning(f\"Missing keys in state_dict: {missing}\")\n            if unexpected:\n                logger.warning(f\"Unexpected keys in state_dict: {unexpected}\")\n\n        logger.info(\"PyTorch model loaded successfully from state_dict checkpoint.\")\n        return model, None\n\n    # Case 3: unsupported checkpoint structure\n    raise ModelPersistenceError(\n        f\"Unsupported checkpoint type: {type(checkpoint)!r}. \"\n        \"Expected nn.Module or dict-like object.\"\n    )\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.persistence.ModelPersistence.save","title":"save","text":"<pre><code>save(\n    *,\n    save_full_model: bool = False,\n    **torch_save_kwargs: Any\n) -&gt; None\n</code></pre> <p>Save the attached PyTorch model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>save_full_model</code> <code>bool</code> <p>If True, saves the entire model object (pickle). If False (default), saves only the <code>state_dict</code>.</p> <code>False</code> <code>**torch_save_kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>torch.save()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If no model is attached or saving fails.</p> Source code in <code>mlpotion/frameworks/pytorch/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully saved PyTorch model\",\n)\ndef save(\n    self,\n    *,\n    save_full_model: bool = False,\n    **torch_save_kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the attached PyTorch model to disk.\n\n    Args:\n        save_full_model: If True, saves the entire model object (pickle).\n            If False (default), saves only the `state_dict`.\n        **torch_save_kwargs: Additional arguments passed to `torch.save()`.\n\n    Raises:\n        ModelPersistenceError: If no model is attached or saving fails.\n    \"\"\"\n    model = self._ensure_model()\n    path = self.path_obj\n\n    logger.info(\n        \"Saving PyTorch model to {path} ({mode})\",\n        path=str(path),\n        mode=\"full model\" if save_full_model else \"state_dict\",\n    )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    if save_full_model:\n        logger.warning(\n            \"Saving a full model object. This is less portable and may break \"\n            \"if the code structure changes. Prefer saving a state_dict for \"\n            \"long-term storage.\"\n        )\n        torch.save(model, path, **torch_save_kwargs)\n    else:\n        torch.save(model.state_dict(), path, **torch_save_kwargs)\n\n    logger.info(\"PyTorch model saved successfully.\")\n</code></pre>"},{"location":"api/frameworks/pytorch.html#export","title":"Export","text":"<p> See the PyTorch Guide for usage examples </p>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters","title":"mlpotion.frameworks.pytorch.deployment.exporters","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters-classes","title":"Classes","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters.ModelExporter","title":"ModelExporter  <code>dataclass</code>","text":"<p>         Bases: <code>ModelExporterProtocol[nn.Module]</code></p> <p>Export PyTorch models to TorchScript, ONNX, or state_dict formats.</p> <p>This class implements the <code>ModelExporterProtocol</code> for PyTorch. It supports exporting models for deployment or interoperability.</p> <p>Supported formats: - torchscript: Exports via <code>torch.jit.script</code> or <code>torch.jit.trace</code>. - onnx: Exports to ONNX format (requires <code>example_input</code>). - state_dict: Saves the model parameters.</p> Example <pre><code>from mlpotion.frameworks.pytorch import ModelExporter\nfrom mlpotion.frameworks.pytorch.config import ModelExportConfig\nimport torch\n\n# Prepare model and input\nmodel = ...\nexample_input = torch.randn(1, 3, 224, 224)\n\n# Export to ONNX\nexporter = ModelExporter()\nconfig = ModelExportConfig(\n    export_path=\"models/model.onnx\",\n    format=\"onnx\",\n    example_input=example_input\n)\n\nresult = exporter.export(model, config)\n</code></pre>"},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters.ModelExporter-functions","title":"Functions","text":""},{"location":"api/frameworks/pytorch.html#mlpotion.frameworks.pytorch.deployment.exporters.ModelExporter.export","title":"export","text":"<pre><code>export(\n    model: nn.Module, config: ModelExportConfig\n) -&gt; ExportResult\n</code></pre> <p>Export a PyTorch model to the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to export.</p> required <code>config</code> <code>ModelExportConfig</code> <p>Configuration object specifying format, path, and other options.</p> required <p>Returns:</p> Name Type Description <code>ExportResult</code> <code>ExportResult</code> <p>A dataclass containing the path to the exported artifact and metadata.</p> <p>Raises:</p> Type Description <code>ExportError</code> <p>If the export process fails (e.g., invalid format, missing example input).</p> Source code in <code>mlpotion/frameworks/pytorch/deployment/exporters.py</code> <pre><code>@trycatch(\n    error=ModelExporterError,\n    success_msg=\"\u2705 Successfully Exported model\",\n)\ndef export(\n    self,\n    model: nn.Module,\n    config: ModelExportConfig,\n) -&gt; ExportResult:\n    \"\"\"Export a PyTorch model to the specified format.\n\n    Args:\n        model: The PyTorch model to export.\n        config: Configuration object specifying format, path, and other options.\n\n    Returns:\n        ExportResult: A dataclass containing the path to the exported artifact and metadata.\n\n    Raises:\n        ExportError: If the export process fails (e.g., invalid format, missing example input).\n    \"\"\"\n    try:\n        export_root = Path(config.export_path)\n        export_root.parent.mkdir(parents=True, exist_ok=True)\n\n        fmt = config.format.lower()\n        device_str = getattr(config, \"device\", \"cpu\")\n        device = torch.device(device_str)\n\n        logger.info(\n            \"Exporting PyTorch model \"\n            f\"[format={fmt}, device={device_str}, target={export_root}]\"\n        )\n\n        model = model.to(device)\n        model.eval()\n\n        # Dispatch\n        if fmt == \"torchscript\":\n            final_path = self._export_torchscript(\n                model=model,\n                export_root=export_root,\n                config=config,\n                device=device,\n            )\n        elif fmt == \"onnx\":\n            final_path = self._export_onnx(\n                model=model,\n                export_root=export_root,\n                config=config,\n                device=device,\n            )\n        elif fmt == \"state_dict\":\n            final_path = self._export_state_dict(\n                model=model,\n                export_root=export_root,\n            )\n        else:\n            raise ExportError(f\"Unknown export format: {config.format!r}\")\n\n        logger.success(f\"Model successfully exported \u2192 {final_path}\")\n\n        metadata: dict[str, Any] = {\n            \"model_type\": \"pytorch\",\n            \"format\": fmt,\n            \"device\": device_str,\n        }\n\n        return ExportResult(\n            export_path=str(final_path),\n            format=fmt,\n            config=config,\n            metadata=metadata,\n        )\n\n    except ExportError:\n        raise\n    except Exception as exc:  # noqa: BLE001\n        raise ExportError(f\"Export failed: {exc!s}\") from exc\n</code></pre>"},{"location":"api/frameworks/tensorflow.html","title":"TensorFlow API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's TensorFlow components.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p> <p>Extensibility</p> <p>These components are built using protocol-based design, making MLPotion easy to extend. Want to add new data sources, training methods, or integrations? See Contributing Guide.</p>"},{"location":"api/frameworks/tensorflow.html#data-loading","title":"Data Loading","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders","title":"mlpotion.frameworks.tensorflow.data.loaders","text":"<p>TensorFlow data loaders.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.CSVDataLoader","title":"CSVDataLoader","text":"<pre><code>CSVDataLoader(\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[dict[str, Any]], dict[str, Any]]\n    | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>         Bases: <code>DataLoader[tf.data.Dataset]</code></p> <p>Load CSV files into TensorFlow datasets.</p> <p>This class provides a convenient wrapper around <code>tf.data.experimental.make_csv_dataset</code>, adding validation, logging, and configuration management. It handles file pattern matching, column selection, and label separation.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the CSV files to load.</p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific columns to load. If None, all columns are loaded.</p> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label. If None, no labels are returned.</p> <code>map_fn</code> <code>Callable | None</code> <p>Optional function to map over the dataset (e.g., for preprocessing).</p> <code>config</code> <code>dict | None</code> <p>Additional configuration passed to <code>make_csv_dataset</code>.</p> Example <pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\n# Simple usage\nloader = CSVDataLoader(\n    file_pattern=\"data/train_*.csv\",\n    label_name=\"target_class\",\n    batch_size=64,\n    config={\"num_epochs\": 5, \"shuffle\": True}\n)\n\ndataset = loader.load()\n\n# Iterate\nfor features, labels in dataset:\n    print(features['some_column'].shape)\n    break\n</code></pre> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[dict[str, Any]], dict[str, Any]] | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None:\n    self.file_pattern = file_pattern\n    self.column_names = column_names\n    self.label_name = label_name\n    self.batch_size = batch_size\n    self.map_fn = map_fn\n\n    # set default config\n    _default_config = {\"ignore_errors\": True, \"num_epochs\": 1}\n    self.config: dict[str, Any] = dict(config or _default_config)\n\n    # Extract and validate num_epochs *once* so we don't risk duplicating kwargs\n    self.num_epochs = self._extract_and_validate_num_epochs()\n\n    self._validate_files_exist()\n    self._validate_finite_dataset()\n\n    logger.info(\n        \"{class_name} initialized with attrs: {attrs}\",\n        class_name=self.__class__.__name__,\n        attrs=vars(self),\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.CSVDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.CSVDataLoader.load","title":"load","text":"<pre><code>load() -&gt; tf.data.Dataset\n</code></pre> <p>Load CSV files into a TensorFlow dataset.</p> <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: A <code>tf.data.Dataset</code> yielding tuples of <code>(features, labels)</code> if <code>label_name</code></p> <code>tf.data.Dataset</code> <p>is provided, or just <code>features</code> (dict) if not.</p> <p>Raises:</p> Type Description <code>DataLoadingError</code> <p>If no files match the pattern, or if <code>num_epochs</code> is invalid.</p> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully loaded dataset\",\n)\ndef load(self) -&gt; tf.data.Dataset:\n    \"\"\"Load CSV files into a TensorFlow dataset.\n\n    Returns:\n        tf.data.Dataset: A `tf.data.Dataset` yielding tuples of `(features, labels)` if `label_name`\n        is provided, or just `features` (dict) if not.\n\n    Raises:\n        DataLoadingError: If no files match the pattern, or if `num_epochs` is invalid.\n    \"\"\"\n    dataset = tf.data.experimental.make_csv_dataset(\n        file_pattern=self.file_pattern,\n        batch_size=self.batch_size,\n        label_name=self.label_name,\n        column_names=self.column_names,\n        num_epochs=self.num_epochs,  # extracted and validated\n        **self.config,\n    )\n\n    if self.map_fn:\n        logger.info(\"Applying mapping function to dataset\")\n        dataset = dataset.map(self.map_fn)\n\n    # Attach metadata for CSV materializer\n    # This allows ZenML to efficiently serialize/deserialize the dataset\n    # by storing just the configuration instead of the actual data\n    dataset._csv_config = {\n        \"file_pattern\": self.file_pattern,\n        \"batch_size\": self.batch_size,\n        \"label_name\": self.label_name,\n        \"column_names\": self.column_names,\n        \"num_epochs\": self.num_epochs,\n        \"extra_params\": self.config,\n        \"transformations\": [],  # Will be populated by optimizer if used\n    }\n\n    return dataset\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.RecordDataLoader","title":"RecordDataLoader","text":"<pre><code>RecordDataLoader(\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[tf.Tensor], Any] | None = None,\n    element_spec_json: str | dict[str, Any] | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>         Bases: <code>DataLoader[tf.data.Dataset]</code></p> <p>Loader for TFRecord files into tf.data.Dataset.</p> <p>This class facilitates loading data from TFRecord files, which is the recommended format for high-performance TensorFlow pipelines. It supports parsing examples, handling nested structures via <code>element_spec</code>, and applying common dataset optimizations.</p> <p>Attributes:</p> Name Type Description <code>file_pattern</code> <code>str</code> <p>Glob pattern matching the TFRecord files.</p> <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>column_names</code> <code>list[str] | None</code> <p>Specific feature keys to extract.</p> <code>label_name</code> <code>str | None</code> <p>Key of the label feature.</p> <code>map_fn</code> <code>Callable | None</code> <p>Optional function to map over the dataset.</p> <code>element_spec_json</code> <code>str | dict | None</code> <p>JSON or dict describing the data structure (optional).</p> <code>config</code> <code>dict | None</code> <p>Configuration for reading (e.g., <code>num_parallel_reads</code>, <code>compression_type</code>).</p> Example <pre><code>from mlpotion.frameworks.tensorflow import RecordDataLoader\n\nloader = RecordDataLoader(\n    file_pattern=\"data/records/*.tfrecord\",\n    batch_size=128,\n    label_name=\"label\",\n    config={\n        \"compression_type\": \"GZIP\",\n        \"num_parallel_reads\": tf.data.AUTOTUNE\n    }\n)\n\ndataset = loader.load()\n</code></pre> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: str,\n    batch_size: int = 32,\n    column_names: list[str] | None = None,\n    label_name: str | None = None,\n    map_fn: Callable[[tf.Tensor], Any] | None = None,\n    element_spec_json: str | dict[str, Any] | None = None,\n    config: dict[str, Any] | None = None,\n) -&gt; None:\n    self.file_pattern = file_pattern\n    self.batch_size = batch_size\n    self.map_fn = map_fn\n    self.element_spec_json = element_spec_json\n    self.column_names = column_names\n    self.label_name = label_name\n\n    # set config\n    self.config = config or {}\n\n    # validate files exist\n    self._validate_files_exist()\n\n    logger.info(\n        \"{class_name} initialized with attrs: {attrs}\",\n        class_name=self.__class__.__name__,\n        attrs=vars(self),\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.RecordDataLoader-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.loaders.RecordDataLoader.load","title":"load","text":"<pre><code>load() -&gt; tf.data.Dataset\n</code></pre> <p>Load TFRecord files into a tf.data.Dataset.</p> <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: Parsed and optionally mapped dataset of (features, label) or features only.</p> <p>Raises:</p> Type Description <code>DataLoadingError</code> <p>on failure.</p> Source code in <code>mlpotion/frameworks/tensorflow/data/loaders.py</code> <pre><code>@trycatch(\n    error=DataLoadingError,\n    success_msg=\"\u2705 Successfully loaded TFRecord dataset\",\n)\ndef load(self) -&gt; tf.data.Dataset:\n    \"\"\"Load TFRecord files into a tf.data.Dataset.\n\n    Returns:\n        tf.data.Dataset: Parsed and optionally mapped dataset of (features, label) or features only.\n    Raises:\n        DataLoadingError: on failure.\n    \"\"\"\n    filenames = self._get_files_matching_pattern()\n\n    ds = tf.data.TFRecordDataset(\n        filenames=filenames,\n        compression_type=self.config.get(\"compression_type\", \"\"),\n        buffer_size=self.config.get(\"buffer_size\", None),\n        num_parallel_reads=self.config.get(\"num_parallel_reads\", tf.data.AUTOTUNE),\n    )\n\n    # Optionally repeat\n    if \"repeat_count\" in self.config:\n        ds = ds.repeat(self.config[\"repeat_count\"])\n\n    # Apply column/label selection\n    ds = ds.map(\n        self._apply_column_label_selection,\n        num_parallel_calls=self.config.get(\"num_parallel_reads\", tf.data.AUTOTUNE),\n    )\n\n    # Shuffle if requested\n    if \"shuffle_buffer_size\" in self.config:\n        ds = ds.shuffle(self.config[\"shuffle_buffer_size\"])\n\n    # Batch\n    ds = ds.batch(\n        self.batch_size, drop_remainder=self.config.get(\"drop_remainder\", False)\n    )\n\n    # Prefetch\n    ds = ds.prefetch(self.config.get(\"prefetch_buffer_size\", tf.data.AUTOTUNE))\n\n    # Apply mapping function\n    if self.map_fn:\n        logger.info(\"Applying mapping function to dataset\")\n        ds = ds.map(self.map_fn)\n\n    return ds\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers","title":"mlpotion.frameworks.tensorflow.data.optimizers","text":"<p>TensorFlow dataset optimization.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer","title":"DatasetOptimizer","text":"<pre><code>DatasetOptimizer(\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n) -&gt; None\n</code></pre> <p>         Bases: <code>DatasetOptimizerProtocol[tf.data.Dataset]</code></p> <p>Optimize TensorFlow datasets for training performance.</p> <p>This class applies a standard set of performance optimizations to a <code>tf.data.Dataset</code>: caching, shuffling, batching, and prefetching. These are critical for preventing data loading bottlenecks during training.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The number of samples per batch.</p> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Size of the shuffle buffer. If None, shuffling is disabled.</p> <code>prefetch</code> <code>bool</code> <p>Whether to prefetch data (uses <code>tf.data.AUTOTUNE</code>).</p> <code>cache</code> <code>bool</code> <p>Whether to cache the dataset in memory.</p> Example <pre><code>from mlpotion.frameworks.tensorflow import DatasetOptimizer\n\n# Create optimizer\noptimizer = DatasetOptimizer(\n    batch_size=32,\n    shuffle_buffer_size=1000,\n    cache=True,\n    prefetch=True\n)\n\n# Apply to a raw dataset\noptimized_dataset = optimizer.optimize(raw_dataset)\n</code></pre> <p>Initialize dataset optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>32</code> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Buffer size for shuffling (None = no shuffle)</p> <code>None</code> <code>prefetch</code> <code>bool</code> <p>Whether to prefetch batches</p> <code>True</code> <code>cache</code> <code>bool</code> <p>Whether to cache dataset in memory</p> <code>False</code> Source code in <code>mlpotion/frameworks/tensorflow/data/optimizers.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n) -&gt; None:\n    \"\"\"Initialize dataset optimizer.\n\n    Args:\n        batch_size: Batch size\n        shuffle_buffer_size: Buffer size for shuffling (None = no shuffle)\n        prefetch: Whether to prefetch batches\n        cache: Whether to cache dataset in memory\n    \"\"\"\n    self.batch_size = batch_size\n    self.shuffle_buffer_size = shuffle_buffer_size\n    self.prefetch = prefetch\n    self.cache = cache\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: DataOptimizationConfig,\n) -&gt; DatasetOptimizer\n</code></pre> <p>Create optimizer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DataOptimizationConfig</code> <p>Optimization configuration</p> required <p>Returns:</p> Type Description <code>DatasetOptimizer</code> <p>Configured optimizer instance</p> Source code in <code>mlpotion/frameworks/tensorflow/data/optimizers.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DataOptimizationConfig) -&gt; \"DatasetOptimizer\":\n    \"\"\"Create optimizer from configuration.\n\n    Args:\n        config: Optimization configuration\n\n    Returns:\n        Configured optimizer instance\n    \"\"\"\n    return cls(\n        batch_size=config.batch_size,\n        shuffle_buffer_size=config.shuffle_buffer_size,\n        prefetch=config.prefetch,\n        cache=config.cache,\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.data.optimizers.DatasetOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: tf.data.Dataset) -&gt; tf.data.Dataset\n</code></pre> <p>Optimize dataset for training.</p> <p>Applies optimizations in the following order: 1. Cache: Caches data in memory (if enabled). 2. Shuffle: Randomizes data order (if <code>shuffle_buffer_size</code> is set). 3. Batch: Groups data into batches. 4. Prefetch: Prepares the next batch while the current one is being processed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset</code> <p>The input <code>tf.data.Dataset</code>.</p> required <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: The optimized dataset pipeline.</p> Source code in <code>mlpotion/frameworks/tensorflow/data/optimizers.py</code> <pre><code>def optimize(self, dataset: tf.data.Dataset) -&gt; tf.data.Dataset:\n    \"\"\"Optimize dataset for training.\n\n    Applies optimizations in the following order:\n    1. **Cache**: Caches data in memory (if enabled).\n    2. **Shuffle**: Randomizes data order (if `shuffle_buffer_size` is set).\n    3. **Batch**: Groups data into batches.\n    4. **Prefetch**: Prepares the next batch while the current one is being processed.\n\n    Args:\n        dataset: The input `tf.data.Dataset`.\n\n    Returns:\n        tf.data.Dataset: The optimized dataset pipeline.\n    \"\"\"\n    logger.info(\"Applying dataset optimizations...\")\n\n    # Track transformations for CSV materializer\n    transformations = []\n    if hasattr(dataset, \"_csv_config\"):\n        transformations = dataset._csv_config.get(\"transformations\", [])\n\n    # Cache first (before shuffling/batching)\n    if self.cache:\n        logger.info(\"Caching dataset in memory\")\n        dataset = dataset.cache()\n        # Note: cache() doesn't need to be recorded as it's a performance optimization\n\n    # Shuffle before batching\n    if self.shuffle_buffer_size:\n        logger.info(f\"Shuffling with buffer size {self.shuffle_buffer_size}\")\n        dataset = dataset.shuffle(\n            buffer_size=self.shuffle_buffer_size,\n            reshuffle_each_iteration=True,\n        )\n        transformations.append(\n            {\n                \"type\": \"shuffle\",\n                \"params\": {\"buffer_size\": self.shuffle_buffer_size},\n            }\n        )\n\n    # Batch\n    logger.info(f\"Batching with size {self.batch_size}\")\n    dataset = dataset.batch(self.batch_size)\n    transformations.append(\n        {\n            \"type\": \"batch\",\n            \"params\": {\"batch_size\": self.batch_size},\n        }\n    )\n\n    # Prefetch last for best performance\n    if self.prefetch:\n        logger.info(\"Prefetching with AUTOTUNE\")\n        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        transformations.append(\n            {\n                \"type\": \"prefetch\",\n                \"params\": {\"buffer_size\": \"AUTOTUNE\"},\n            }\n        )\n\n    # Preserve CSV config if it exists\n    if hasattr(dataset, \"_csv_config\"):\n        dataset._csv_config[\"transformations\"] = transformations\n\n    return dataset\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#training","title":"Training","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers","title":"mlpotion.frameworks.tensorflow.training.trainers","text":"<p>TensorFlow model trainers.</p> <p>This module re-exports the Keras <code>ModelTrainer</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers.ModelTrainer","title":"ModelTrainer  <code>dataclass</code>","text":"<p>         Bases: <code>ModelTrainerProtocol[Model, Sequence]</code></p> <p>Generic trainer for Keras 3 models.</p> <p>This class implements the <code>ModelTrainerProtocol</code> for Keras models, providing a standardized interface for training. It wraps the standard <code>model.fit()</code> method but adds flexibility and consistency checks.</p> <p>It supports: - Automatic model compilation if <code>compile_params</code> are provided. - Handling of various data formats (tuples, dicts, generators). - Standardized return format (dictionary of history metrics).</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelTrainer\n\n# Prepare data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize trainer\ntrainer = ModelTrainer()\n\n# Train\nhistory = trainer.train(\n    model=model,\n    data=(X_train, y_train),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    fit_params={\n        \"epochs\": 5,\n        \"batch_size\": 32,\n        \"verbose\": 1\n    }\n)\n\nprint(history['loss'])\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.training.trainers.ModelTrainer-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.training.trainers.ModelTrainer.train","title":"train","text":"<pre><code>train(\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]\n</code></pre> <p>Train a Keras model using the provided dataset and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to train.</p> required <code>dataset</code> <code>Any</code> <p>The training data. Can be a tuple <code>(x, y)</code>, a dictionary, a <code>Sequence</code>, or a generator.</p> required <code>config</code> <code>ModelTrainingConfig</code> <p>Configuration object containing training parameters.</p> required <code>validation_dataset</code> <code>Any | None</code> <p>Optional validation data.</p> <code>None</code> <p>Returns:</p> Type Description <code>TrainingResult[Model]</code> <p>TrainingResult[Model]: An object containing the trained model, training history, and metrics.</p> Source code in <code>mlpotion/frameworks/keras/training/trainers.py</code> <pre><code>@trycatch(\n    error=ModelTrainerError,\n    success_msg=\"\u2705 Successfully trained Keras model\",\n)\ndef train(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelTrainingConfig,\n    validation_dataset: Any | None = None,\n) -&gt; TrainingResult[Model]:\n    \"\"\"Train a Keras model using the provided dataset and configuration.\n\n    Args:\n        model: The Keras model to train.\n        dataset: The training data. Can be a tuple `(x, y)`, a dictionary, a `Sequence`, or a generator.\n        config: Configuration object containing training parameters.\n        validation_dataset: Optional validation data.\n\n    Returns:\n        TrainingResult[Model]: An object containing the trained model, training history, and metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare compile parameters from config\n    compile_params = {\n        \"optimizer\": self._get_optimizer(config),\n        \"loss\": config.loss,\n        \"metrics\": config.metrics,\n    }\n\n    # Compile if needed or if forced by config (though we usually respect existing compilation)\n    # Here we'll ensure it's compiled. If the user wants to use their own compilation,\n    # they should probably compile it before passing it, but our config implies we control it.\n    # However, to be safe and flexible:\n    if not self._is_compiled(model):\n        if not config.optimizer or not config.loss:\n            raise RuntimeError(\n                \"Model is not compiled and config does not provide optimizer and loss. \"\n                \"Either compile the model beforehand or provide optimizer and loss in config.\"\n            )\n        logger.info(\"Compiling model with config parameters.\")\n        model.compile(**compile_params)\n    else:\n        logger.info(\"Model already compiled. Using existing compilation settings.\")\n\n    # Prepare fit parameters\n    fit_kwargs = {\n        \"epochs\": config.epochs,\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"shuffle\": config.shuffle,\n        \"validation_split\": config.validation_split,\n        \"callbacks\": self._prepare_callbacks(config),\n    }\n\n    if validation_dataset is not None:\n        fit_kwargs[\"validation_data\"] = validation_dataset\n\n    # Add any framework-specific options\n    fit_kwargs.update(config.framework_options)\n\n    logger.info(\"Starting Keras model training...\")\n    logger.debug(f\"Training data type: {type(dataset)!r}\")\n    logger.debug(f\"Fit parameters: {fit_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    history_obj = self._call_fit(model=model, data=dataset, fit_kwargs=fit_kwargs)\n\n    training_time = time.time() - start_time\n\n    # Convert History object to dict[str, list[float]]\n    history_dict = self._history_to_dict(history_obj)\n\n    # Extract final metrics\n    final_metrics = {}\n    for k, v in history_dict.items():\n        if v:\n            final_metrics[k] = v[-1]\n\n    logger.info(\"Training completed.\")\n    logger.debug(f\"Training history: {history_dict}\")\n\n    return TrainingResult(\n        model=model,\n        history=history_dict,\n        metrics=final_metrics,\n        config=config,\n        training_time=training_time,\n        best_epoch=None,  # Keras history doesn't explicitly track \"best\" unless using callbacks\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#evaluation","title":"Evaluation","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators","title":"mlpotion.frameworks.tensorflow.evaluation.evaluators","text":"<p>TensorFlow model evaluators.</p> <p>This module re-exports the Keras <code>ModelEvaluator</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators.ModelEvaluator","title":"ModelEvaluator  <code>dataclass</code>","text":"<p>         Bases: <code>ModelEvaluatorProtocol[Model, Sequence]</code></p> <p>Generic evaluator for Keras 3 models.</p> <p>This class implements the <code>ModelEvaluatorProtocol</code> for Keras models. It wraps the <code>model.evaluate()</code> method to provide a consistent evaluation interface.</p> <p>It ensures that the evaluation result is always returned as a dictionary of metric names to values, regardless of how the model was compiled or what arguments were passed.</p> Example <pre><code>import keras\nimport numpy as np\nfrom mlpotion.frameworks.keras import ModelEvaluator\n\n# Prepare data\nX_test = np.random.rand(20, 10)\ny_test = np.random.randint(0, 2, 20)\n\n# Define model\nmodel = keras.Sequential([\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Initialize evaluator\nevaluator = ModelEvaluator()\n\n# Evaluate\nmetrics = evaluator.evaluate(\n    model=model,\n    data=(X_test, y_test),\n    compile_params={\n        \"optimizer\": \"adam\",\n        \"loss\": \"binary_crossentropy\",\n        \"metrics\": [\"accuracy\"]\n    },\n    eval_params={\"batch_size\": 32}\n)\n\nprint(metrics)  # {'loss': 0.693..., 'accuracy': 0.5...}\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.evaluation.evaluators.ModelEvaluator-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.evaluation.evaluators.ModelEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult\n</code></pre> <p>Evaluate a Keras model on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to evaluate.</p> required <code>dataset</code> <code>Any</code> <p>The evaluation data. Can be a tuple <code>(x, y)</code>, a dictionary, or a <code>Sequence</code>.</p> required <code>config</code> <code>ModelEvaluationConfig</code> <p>Configuration object containing evaluation parameters.</p> required <p>Returns:</p> Name Type Description <code>EvaluationResult</code> <code>EvaluationResult</code> <p>An object containing the evaluation metrics.</p> Source code in <code>mlpotion/frameworks/keras/evaluation/evaluators.py</code> <pre><code>@trycatch(\n    error=ModelEvaluatorError,\n    success_msg=\"\u2705 Successfully evaluated Keras model\",\n)\ndef evaluate(\n    self,\n    model: Model,\n    dataset: Any,\n    config: ModelEvaluationConfig,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluate a Keras model on the given data.\n\n    Args:\n        model: The Keras model to evaluate.\n        dataset: The evaluation data. Can be a tuple `(x, y)`, a dictionary, or a `Sequence`.\n        config: Configuration object containing evaluation parameters.\n\n    Returns:\n        EvaluationResult: An object containing the evaluation metrics.\n    \"\"\"\n    self._validate_model(model)\n\n    # Prepare eval parameters\n    eval_kwargs = {\n        \"batch_size\": config.batch_size,\n        \"verbose\": config.verbose,\n        \"return_dict\": True,\n    }\n\n    # Add any framework-specific options\n    eval_kwargs.update(config.framework_options)\n\n    # We assume the model is already compiled. If not, Keras will raise an error\n    # unless we provide compile params, but EvaluationConfig doesn't typically carry them.\n    # The user should ensure the model is compiled (e.g. after loading or training).\n    if not self._is_compiled(model):\n        logger.warning(\n            \"Model is not compiled. Evaluation might fail if loss/metrics are not defined.\"\n        )\n\n    logger.info(\"Evaluating Keras model...\")\n    logger.debug(f\"Evaluation data type: {type(dataset)!r}\")\n    logger.debug(f\"Evaluation parameters: {eval_kwargs}\")\n\n    import time\n\n    start_time = time.time()\n\n    result = self._call_evaluate(model=model, data=dataset, eval_kwargs=eval_kwargs)\n\n    evaluation_time = time.time() - start_time\n\n    # At this point, result should be a dict[str, float]\n    if not isinstance(result, dict):\n        # Defensive fallback if user or Keras changed behavior\n        logger.warning(\n            f\"`model.evaluate` did not return a dict (got {type(result)!r}). \"\n            \"Wrapping into a dict under key 'metric_0'.\"\n        )\n        result = {\"metric_0\": float(result)}\n\n    metrics = {str(k): float(v) for k, v in result.items()}\n    logger.info(f\"Evaluation result: {metrics}\")\n\n    return EvaluationResult(\n        metrics=metrics,\n        config=config,\n        evaluation_time=evaluation_time,\n    )\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#persistence","title":"Persistence","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence","title":"mlpotion.frameworks.tensorflow.deployment.persistence","text":"<p>TensorFlow model persistence.</p> <p>This module re-exports the Keras <code>ModelPersistence</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence.ModelPersistence","title":"ModelPersistence","text":"<pre><code>ModelPersistence(\n    path: str | Path, model: Model | None = None\n) -&gt; None\n</code></pre> <p>         Bases: <code>ModelPersistenceProtocol[Model]</code></p> <p>Persistence helper for Keras models.</p> <p>This class manages saving and loading of Keras models. It supports standard Keras formats (<code>.keras</code>, <code>.h5</code>) and SavedModel directories. It also integrates with <code>ModelInspector</code> to provide model metadata upon loading.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Path</code> <p>The file path for the model artifact.</p> <code>model</code> <code>Model | None</code> <p>The Keras model instance (optional).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelPersistence\n\n# Define model\nmodel = keras.Sequential([keras.layers.Dense(1)])\n\n# Save\nsaver = ModelPersistence(path=\"models/my_model.keras\", model=model)\nsaver.save()\n\n# Load\nloader = ModelPersistence(path=\"models/my_model.keras\")\nloaded_model, metadata = loader.load(inspect=True)\nprint(metadata['parameters'])\n</code></pre> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>def __init__(self, path: str | Path, model: Model | None = None) -&gt; None:\n    self._path = Path(path)\n    self._model = model\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence.ModelPersistence-attributes","title":"Attributes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.model","title":"model  <code>writable</code> <code>property</code>","text":"<pre><code>model: Model | None\n</code></pre> <p>Currently attached Keras model (may be None before loading).</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.path","title":"path  <code>writable</code> <code>property</code>","text":"<pre><code>path: Path\n</code></pre> <p>Filesystem path where the model is saved/loaded.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.persistence.ModelPersistence-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.load","title":"load","text":"<pre><code>load(\n    *, inspect: bool = True, **kwargs: Any\n) -&gt; tuple[Model, dict[str, Any] | None]\n</code></pre> <p>Load a Keras model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inspect</code> <code>bool</code> <p>Whether to inspect the loaded model and return metadata.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>keras.models.load_model()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Model</code> <p>tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and</p> <code>dict[str, Any] | None</code> <p>optional inspection metadata.</p> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If the model file cannot be found or loaded.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully loaded Keras model\",\n)\ndef load(\n    self,\n    *,\n    inspect: bool = True,\n    **kwargs: Any,\n) -&gt; tuple[Model, dict[str, Any] | None]:\n    \"\"\"Load a Keras model from disk.\n\n    Args:\n        inspect: Whether to inspect the loaded model and return metadata.\n        **kwargs: Additional arguments passed to `keras.models.load_model()`.\n\n    Returns:\n        tuple[Model, dict[str, Any] | None]: A tuple containing the loaded model and\n        optional inspection metadata.\n\n    Raises:\n        ModelPersistenceError: If the model file cannot be found or loaded.\n    \"\"\"\n    path = self._ensure_path_exists()\n\n    logger.info(f\"Loading Keras model from: {path!s}\")\n    model = keras.models.load_model(path.as_posix(), **kwargs)\n\n    self._model = model  # keep instance in sync\n\n    inspection_result: dict[str, Any] | None = None\n    if inspect:\n        logger.info(\"Inspecting loaded Keras model with ModelInspector.\")\n        inspector = ModelInspector()\n        inspection_result = inspector.inspect(model)\n\n    return model, inspection_result\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.persistence.ModelPersistence.save","title":"save","text":"<pre><code>save(overwrite: bool = True, **kwargs: Any) -&gt; None\n</code></pre> <p>Save the attached model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>model.save()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelPersistenceError</code> <p>If no model is attached or if the file exists and <code>overwrite</code> is False.</p> Source code in <code>mlpotion/frameworks/keras/deployment/persistence.py</code> <pre><code>@trycatch(\n    error=ModelPersistenceError,\n    success_msg=\"\u2705 Successfully saved Keras model\",\n)\ndef save(\n    self,\n    overwrite: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save the attached model to disk.\n\n    Args:\n        overwrite: Whether to overwrite the file if it already exists.\n        **kwargs: Additional arguments passed to `model.save()`.\n\n    Raises:\n        ModelPersistenceError: If no model is attached or if the file exists and `overwrite` is False.\n    \"\"\"\n    model = self._ensure_model()\n    target = self._path\n\n    if target.exists() and not overwrite:\n        raise ModelPersistenceError(\n            f\"Target path already exists and overwrite=False: {target!s}\"\n        )\n\n    logger.info(f\"Saving Keras model to: {target!s}\")\n    target.parent.mkdir(parents=True, exist_ok=True)\n\n    # Keras 3 generally infers format from the path; `save_format` is\n    # deprecated / discouraged in newer APIs, so we do NOT pass it.\n    model.save(target.as_posix(), **kwargs)\n    logger.info(\"Keras model saved successfully.\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#export","title":"Export","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters","title":"mlpotion.frameworks.tensorflow.deployment.exporters","text":"<p>TensorFlow model exporters.</p> <p>This module re-exports the Keras <code>ModelExporter</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters.ModelExporter","title":"ModelExporter","text":"<p>         Bases: <code>ModelExporterProtocol[Model]</code></p> <p>Generic exporter for Keras 3 models.</p> <p>This class implements <code>ModelExporterProtocol</code> and supports exporting Keras models to various formats, including native Keras formats (<code>.keras</code>, <code>.h5</code>) and inference formats like TensorFlow SavedModel or ONNX (via <code>model.export</code>).</p> <p>It also supports creating export archives with custom endpoints using <code>keras.export.ExportArchive</code>.</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelExporter\n\nmodel = keras.Sequential([keras.layers.Dense(1)])\nexporter = ModelExporter()\n\n# Export as standard Keras file\nexporter.export(model, \"models/model.keras\")\n\n# Export for serving (TF SavedModel)\nexporter.export(model, \"models/serving\", export_format=\"tf_saved_model\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.deployment.exporters.ModelExporter-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.deployment.exporters.ModelExporter.export","title":"export","text":"<pre><code>export(model: Model, path: str, **kwargs: Any) -&gt; None\n</code></pre> <p>Export a Keras model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The Keras model to export.</p> required <code>path</code> <code>str</code> <p>The destination path or directory.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional export options: - <code>export_format</code> (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc. - <code>dataset</code> (Iterable): Optional data for model warmup. - <code>endpoint_name</code> (str): Name for custom endpoint (uses ExportArchive). - <code>input_specs</code> (list[InputSpec]): Input signatures for custom endpoint. - <code>config</code> (dict): Extra arguments for the underlying save/export method.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ModelExporterError</code> <p>If export fails.</p> Source code in <code>mlpotion/frameworks/keras/deployment/exporters.py</code> <pre><code>@trycatch(\n    error=ModelExporterError,\n    success_msg=\"\u2705 Successfully Exported model\",\n)\ndef export(self, model: Model, path: str, **kwargs: Any) -&gt; None:\n    \"\"\"Export a Keras model to disk.\n\n    Args:\n        model: The Keras model to export.\n        path: The destination path or directory.\n        **kwargs: Additional export options:\n            - `export_format` (str): \"keras\", \"h5\", \"tf_saved_model\", \"onnx\", etc.\n            - `dataset` (Iterable): Optional data for model warmup.\n            - `endpoint_name` (str): Name for custom endpoint (uses ExportArchive).\n            - `input_specs` (list[InputSpec]): Input signatures for custom endpoint.\n            - `config` (dict): Extra arguments for the underlying save/export method.\n\n    Raises:\n        ModelExporterError: If export fails.\n    \"\"\"\n    export_path = Path(path)\n\n    export_format: str | None = kwargs.pop(\"export_format\", None)\n    dataset: Iterable[Any] | None = kwargs.pop(\"dataset\", None)\n    endpoint_name: str | None = kwargs.pop(\"endpoint_name\", None)\n    input_specs: Sequence[InputSpec] | None = kwargs.pop(\"input_specs\", None)\n    config: Mapping[str, Any] | None = kwargs.pop(\"config\", None)\n\n    if kwargs:\n        logger.warning(\n            \"Unused export kwargs passed to ModelExporter: \"\n            f\"{list(kwargs.keys())}\"\n        )\n\n    self._validate_model(model)\n    self._validate_config(config)\n\n    # Determine mode if export_format isn't explicitly set\n    if export_format is None:\n        export_format = self._infer_export_format_from_path(export_path)\n\n    logger.info(\n        f\"Exporting Keras model '{model.name}' to {export_path!s} \"\n        f\"with format '{export_format}'\"\n    )\n\n    # Optional warm-up pass\n    self._warmup_if_needed(model=model, dataset=dataset)\n\n    # Choose strategy\n    try:\n        if self._is_native_keras_format(export_format):\n            self._save_native_keras(model=model, path=export_path, config=config)\n        elif endpoint_name is not None or input_specs is not None:\n            self._export_with_export_archive(\n                model=model,\n                path=export_path,\n                endpoint_name=endpoint_name or self.default_endpoint_name,\n                input_specs=input_specs,\n                export_format=export_format,\n            )\n        else:\n            self._export_with_model_export(\n                model=model,\n                path=export_path,\n                export_format=export_format,\n                config=config,\n            )\n    except ValueError as err:\n        logger.warning(\n            f\"Export error: {err} \"\n            \"(you may need to build the model by calling it on example data \"\n            \"before exporting)\"\n        )\n\n    logger.info(f\"Model export completed: {export_path!s}\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#model-inspection","title":"Model Inspection","text":"<p> See the TensorFlow Guide for usage examples </p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection","title":"mlpotion.frameworks.tensorflow.models.inspection","text":"<p>TensorFlow model inspection.</p> <p>This module re-exports the Keras <code>ModelInspector</code> implementation, as TensorFlow 2.x uses Keras as its high-level API.</p>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection-classes","title":"Classes","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection.ModelInspector","title":"ModelInspector  <code>dataclass</code>","text":"<p>         Bases: <code>ModelInspectorProtocol[ModelLike]</code></p> <p>Inspector for Keras models.</p> <p>This class analyzes Keras models to extract metadata such as input/output shapes, parameter counts, layer details, and signatures. It is useful for validating models before training or deployment, and for generating model reports.</p> <p>Attributes:</p> Name Type Description <code>include_layers</code> <code>bool</code> <p>Whether to include detailed information about each layer.</p> <code>include_signatures</code> <code>bool</code> <p>Whether to include model signatures (if available).</p> Example <pre><code>import keras\nfrom mlpotion.frameworks.keras import ModelInspector\n\nmodel = keras.Sequential([keras.layers.Dense(1, input_shape=(10,))])\ninspector = ModelInspector()\n\ninfo = inspector.inspect(model)\nprint(f\"Total params: {info['parameters']['total']}\")\nprint(f\"Inputs: {info['inputs']}\")\n</code></pre>"},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.tensorflow.models.inspection.ModelInspector-functions","title":"Functions","text":""},{"location":"api/frameworks/tensorflow.html#mlpotion.frameworks.keras.models.inspection.ModelInspector.inspect","title":"inspect","text":"<pre><code>inspect(model: ModelLike) -&gt; dict[str, Any]\n</code></pre> <p>Inspect a Keras model and return structured metadata.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelLike</code> <p>The Keras model to inspect.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing model metadata: - <code>name</code>: Model name. - <code>backend</code>: Keras backend used. - <code>trainable</code>: Whether the model is trainable. - <code>inputs</code>: List of input specifications. - <code>outputs</code>: List of output specifications. - <code>parameters</code>: Dictionary of parameter counts. - <code>layers</code>: List of layer details (if <code>include_layers=True</code>). - <code>signatures</code>: Model signatures (if <code>include_signatures=True</code>).</p> Source code in <code>mlpotion/frameworks/keras/models/inspection.py</code> <pre><code>@trycatch(\n    error=ModelInspectorError,\n    success_msg=\"\u2705 Successfully inspected Keras model\",\n)\ndef inspect(self, model: ModelLike) -&gt; dict[str, Any]:\n    \"\"\"Inspect a Keras model and return structured metadata.\n\n    Args:\n        model: The Keras model to inspect.\n\n    Returns:\n        dict[str, Any]: A dictionary containing model metadata:\n            - `name`: Model name.\n            - `backend`: Keras backend used.\n            - `trainable`: Whether the model is trainable.\n            - `inputs`: List of input specifications.\n            - `outputs`: List of output specifications.\n            - `parameters`: Dictionary of parameter counts.\n            - `layers`: List of layer details (if `include_layers=True`).\n            - `signatures`: Model signatures (if `include_signatures=True`).\n    \"\"\"\n    if not isinstance(model, keras.Model):\n        raise TypeError(\n            f\"ModelInspector expects a keras.Model, got {type(model)!r}\"\n        )\n\n    logger.info(\"Inspecting Keras model...\")\n\n    backend_name = self._get_backend_name()\n\n    info: dict[str, Any] = {\n        \"name\": model.name,\n        \"backend\": backend_name,\n        \"trainable\": model.trainable,\n    }\n\n    info[\"inputs\"] = self._get_inputs(model)\n    info[\"input_names\"] = [input[\"name\"] for input in info[\"inputs\"]]\n    info[\"outputs\"] = self._get_outputs(model)\n    info[\"output_names\"] = [output[\"name\"] for output in info[\"outputs\"]]\n    info[\"parameters\"] = self._get_param_counts(model)\n\n    if self.include_signatures:\n        info[\"signatures\"] = self._get_signatures(model)\n\n    if self.include_layers:\n        info[\"layers\"] = self._get_layers_summary(model)\n\n    logger.debug(f\"Keras model inspection result: {info}\")\n    return info\n</code></pre>"},{"location":"api/integrations/zenml.html","title":"ZenML Integration API Reference \ud83d\udcd6","text":"<p>Complete API reference for MLPotion's ZenML integration.</p> <p>Auto-Generated Documentation</p> <p>This page is automatically populated with API documentation from the source code.</p>"},{"location":"api/integrations/zenml.html#tensorflow-steps","title":"TensorFlow Steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps","title":"mlpotion.integrations.zenml.tensorflow.steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], EvaluationMetrics]\n</code></pre> <p>Evaluate a TensorFlow/Keras model using <code>ModelEvaluator</code>.</p> <p>This step computes metrics on a given dataset using the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to evaluate.</p> required <code>dataset</code> <code>tf.data.Dataset</code> <p>The evaluation <code>tf.data.Dataset</code>.</p> required <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, float], EvaluationMetrics]</code> <p>dict[str, float]: A dictionary of computed metrics.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef evaluate_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], \"EvaluationMetrics\"]:\n    \"\"\"Evaluate a TensorFlow/Keras model using `ModelEvaluator`.\n\n    This step computes metrics on a given dataset using the provided model.\n\n    Args:\n        model: The Keras model to evaluate.\n        dataset: The evaluation `tf.data.Dataset`.\n        verbose: Verbosity mode (0 or 1).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, float]: A dictionary of computed metrics.\n    \"\"\"\n    logger.info(\"Evaluating model\")\n\n    evaluator = ModelEvaluator()\n\n    config = ModelEvaluationConfig(\n        verbose=verbose,\n    )\n\n    result = evaluator.evaluate(\n        model=model,\n        dataset=dataset,\n        config=config,\n    )\n\n    metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"metrics\": metrics})\n\n    return metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str = \"keras\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, ExportPath]\n</code></pre> <p>Export a TensorFlow/Keras model to disk using <code>ModelExporter</code>.</p> <p>This step exports the model to a specified format (e.g., Keras format, SavedModel).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to export.</p> required <code>export_path</code> <code>str</code> <p>The destination path for the exported model.</p> required <code>export_format</code> <code>str</code> <p>The format to export to (default: \"keras\").</p> <code>'keras'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, ExportPath]</code> <p>The path to the exported model artifact.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str = \"keras\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"ExportPath\"]:\n    \"\"\"Export a TensorFlow/Keras model to disk using `ModelExporter`.\n\n    This step exports the model to a specified format (e.g., Keras format, SavedModel).\n\n    Args:\n        model: The Keras model to export.\n        export_path: The destination path for the exported model.\n        export_format: The format to export to (default: \"keras\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the exported model artifact.\n    \"\"\"\n    logger.info(f\"Exporting model to: {export_path}\")\n\n    exporter = ModelExporter()\n\n    exporter.export(\n        model=model,\n        path=export_path,\n        export_format=export_format,\n    )\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"export_path\": export_path})\n\n    return export_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.inspect_model","title":"inspect_model","text":"<pre><code>inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], ModelInspection]\n</code></pre> <p>Inspect a TensorFlow/Keras model using <code>ModelInspector</code>.</p> <p>This step extracts metadata about the model, such as layer configuration, input/output shapes, and parameter counts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to inspect.</p> required <code>include_layers</code> <code>bool</code> <p>Whether to include detailed layer information.</p> <code>True</code> <code>include_signatures</code> <code>bool</code> <p>Whether to include signature information.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, Any], ModelInspection]</code> <p>dict[str, Any]: A dictionary containing the inspection results.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], \"ModelInspection\"]:\n    \"\"\"Inspect a TensorFlow/Keras model using `ModelInspector`.\n\n    This step extracts metadata about the model, such as layer configuration,\n    input/output shapes, and parameter counts.\n\n    Args:\n        model: The Keras model to inspect.\n        include_layers: Whether to include detailed layer information.\n        include_signatures: Whether to include signature information.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the inspection results.\n    \"\"\"\n    logger.info(\"Inspecting model\")\n\n    inspector = ModelInspector(\n        include_layers=include_layers,\n        include_signatures=include_signatures,\n    )\n    inspection = inspector.inspect(model)\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"inspection\": inspection})\n\n    return inspection\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.load_data","title":"load_data","text":"<pre><code>load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str = \"target\",\n    column_names: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, TFDataset]\n</code></pre> <p>Load data from local CSV files using TensorFlow's efficient loading.</p> <p>This step uses <code>CSVDataLoader</code> to create a <code>tf.data.Dataset</code> from CSV files matching the specified pattern.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str</code> <p>Name of the column to use as the label.</p> <code>'target'</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[tf.data.Dataset, TFDataset]</code> <p>tf.data.Dataset: The loaded TensorFlow dataset.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str = \"target\",\n    column_names: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, \"TFDataset\"]:\n    \"\"\"Load data from local CSV files using TensorFlow's efficient loading.\n\n    This step uses `CSVDataLoader` to create a `tf.data.Dataset` from CSV files matching\n    the specified pattern.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        tf.data.Dataset: The loaded TensorFlow dataset.\n    \"\"\"\n    logger.info(f\"Loading data from: {file_path}\")\n\n    # defining configuration\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        label_name=label_name,\n        column_names=column_names,\n    )\n\n    # initializing data loader\n    loader = CSVDataLoader(**config.dict())\n    # loading data\n    dataset = loader.load()\n\n    # adding metadata\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, LoadedModel]\n</code></pre> <p>Load a TensorFlow/Keras model from disk using <code>ModelPersistence</code>.</p> <p>This step loads a previously saved model. It can optionally inspect the loaded model to log metadata about its structure.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> required <code>inspect</code> <code>bool</code> <p>Whether to inspect the model after loading.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[keras.Model, LoadedModel]</code> <p>keras.Model: The loaded Keras model.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, \"LoadedModel\"]:\n    \"\"\"Load a TensorFlow/Keras model from disk using `ModelPersistence`.\n\n    This step loads a previously saved model. It can optionally inspect the loaded model\n    to log metadata about its structure.\n\n    Args:\n        model_path: The path to the saved model.\n        inspect: Whether to inspect the model after loading.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        keras.Model: The loaded Keras model.\n    \"\"\"\n    logger.info(f\"Loading model from: {model_path}\")\n\n    persistence = ModelPersistence(path=model_path)\n    model, inspection = persistence.load(inspect=inspect)\n\n    if metadata:\n        meta = {**metadata}\n        if inspection:\n            meta[\"inspection\"] = inspection\n        log_step_metadata(metadata=meta)\n\n    return model\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.optimize_data","title":"optimize_data","text":"<pre><code>optimize_data(\n    dataset: tf.data.Dataset,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, TFDataset]\n</code></pre> <p>Optimize a TensorFlow dataset for training performance.</p> <p>This step applies optimizations like caching, shuffling, and prefetching to the dataset using <code>DatasetOptimizer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset</code> <p>The input <code>tf.data.Dataset</code>.</p> required <code>batch_size</code> <code>int</code> <p>Batch size (if re-batching is needed).</p> <code>32</code> <code>shuffle_buffer_size</code> <code>int | None</code> <p>Size of the shuffle buffer.</p> <code>None</code> <code>prefetch</code> <code>bool</code> <p>Whether to prefetch data.</p> <code>True</code> <code>cache</code> <code>bool</code> <p>Whether to cache data in memory.</p> <code>False</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[tf.data.Dataset, TFDataset]</code> <p>tf.data.Dataset: The optimized TensorFlow dataset.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef optimize_data(\n    dataset: tf.data.Dataset,\n    batch_size: int = 32,\n    shuffle_buffer_size: int | None = None,\n    prefetch: bool = True,\n    cache: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[tf.data.Dataset, \"TFDataset\"]:\n    \"\"\"Optimize a TensorFlow dataset for training performance.\n\n    This step applies optimizations like caching, shuffling, and prefetching to the dataset\n    using `DatasetOptimizer`.\n\n    Args:\n        dataset: The input `tf.data.Dataset`.\n        batch_size: Batch size (if re-batching is needed).\n        shuffle_buffer_size: Size of the shuffle buffer.\n        prefetch: Whether to prefetch data.\n        cache: Whether to cache data in memory.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        tf.data.Dataset: The optimized TensorFlow dataset.\n    \"\"\"\n    logger.info(\"Optimizing dataset for training performance\")\n\n    config = DataOptimizationConfig(\n        batch_size=batch_size,\n        shuffle_buffer_size=shuffle_buffer_size,\n        prefetch=prefetch,\n        cache=cache,\n    )\n\n    optimizer = DatasetOptimizer(**config.dict())\n    dataset = optimizer.optimize(dataset)\n\n    # adding metadata\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, SavePath]\n</code></pre> <p>Save a TensorFlow/Keras model to disk using <code>ModelPersistence</code>.</p> <p>This step saves the model for later reloading.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to save.</p> required <code>save_path</code> <code>str</code> <p>The destination path.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, SavePath]</code> <p>The path to the saved model.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"SavePath\"]:\n    \"\"\"Save a TensorFlow/Keras model to disk using `ModelPersistence`.\n\n    This step saves the model for later reloading.\n\n    Args:\n        model: The Keras model to save.\n        save_path: The destination path.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved model.\n    \"\"\"\n    logger.info(f\"Saving model to: {save_path}\")\n\n    persistence = ModelPersistence(path=save_path, model=model)\n    persistence.save()\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"save_path\": save_path})\n\n    return save_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    epochs: int = 10,\n    validation_dataset: tf.data.Dataset | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, TrainedModel],\n    Annotated[dict[str, list[float]], TrainingHistory],\n]\n</code></pre> <p>Train a TensorFlow/Keras model using <code>ModelTrainer</code>.</p> <p>This step configures and runs a training session. It supports validation data and logging of training metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to train.</p> required <code>dataset</code> <code>tf.data.Dataset</code> <p>The training <code>tf.data.Dataset</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>validation_dataset</code> <code>tf.data.Dataset | None</code> <p>Optional validation <code>tf.data.Dataset</code>.</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer.</p> <code>0.001</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0, 1, or 2).</p> <code>1</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Annotated[keras.Model, TrainedModel], Annotated[dict[str, list[float]], TrainingHistory]]</code> <p>Tuple[keras.Model, dict[str, list[float]]]: The trained model and a dictionary of history metrics.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef train_model(\n    model: keras.Model,\n    dataset: tf.data.Dataset,\n    epochs: int = 10,\n    validation_dataset: tf.data.Dataset | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, \"TrainedModel\"],\n    Annotated[dict[str, list[float]], \"TrainingHistory\"],\n]:\n    \"\"\"Train a TensorFlow/Keras model using `ModelTrainer`.\n\n    This step configures and runs a training session. It supports validation data\n    and logging of training metrics.\n\n    Args:\n        model: The Keras model to train.\n        dataset: The training `tf.data.Dataset`.\n        epochs: Number of epochs to train.\n        validation_dataset: Optional validation `tf.data.Dataset`.\n        learning_rate: Learning rate for the Adam optimizer.\n        verbose: Verbosity mode (0, 1, or 2).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        Tuple[keras.Model, dict[str, list[float]]]: The trained model and a dictionary of history metrics.\n    \"\"\"\n    logger.info(f\"Training model for {epochs} epochs\")\n\n    trainer = ModelTrainer()\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metrics=[\"mae\"],\n    )\n\n    result = trainer.train(\n        model=model,\n        dataset=dataset,\n        config=config,\n        validation_dataset=validation_dataset,\n    )\n\n    # Result is TrainingResult object\n    training_metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"history\": result.history})\n\n    return model, training_metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.steps.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    dataset: tf.data.Dataset,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, OutputPath]\n</code></pre> <p>Transform data using a TensorFlow model and save predictions to CSV.</p> <p>This step uses <code>DataToCSVTransformer</code> to run inference on a dataset using a provided model and saves the results to the specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>tf.data.Dataset</code> <p>The input <code>tf.data.Dataset</code>.</p> required <code>model</code> <code>keras.Model</code> <p>The Keras model to use for transformation.</p> required <code>data_output_path</code> <code>str</code> <p>Path to save the transformed data (CSV).</p> required <code>data_output_per_batch</code> <code>bool</code> <p>Whether to save a separate file per batch.</p> <code>False</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, OutputPath]</code> <p>The path to the saved output file(s).</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/steps.py</code> <pre><code>@step\ndef transform_data(\n    dataset: tf.data.Dataset,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"OutputPath\"]:\n    \"\"\"Transform data using a TensorFlow model and save predictions to CSV.\n\n    This step uses `DataToCSVTransformer` to run inference on a dataset using a provided model\n    and saves the results to the specified output path.\n\n    Args:\n        dataset: The input `tf.data.Dataset`.\n        model: The Keras model to use for transformation.\n        data_output_path: Path to save the transformed data (CSV).\n        data_output_per_batch: Whether to save a separate file per batch.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved output file(s).\n    \"\"\"\n    logger.info(f\"Transforming data and saving to: {data_output_path}\")\n\n    transformer = DataToCSVTransformer(\n        dataset=dataset,\n        model=model,\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n    )\n\n    # Create minimal config for transform method\n    config = DataTransformationConfig(\n        file_pattern=\"\",  # Not used since dataset is provided\n        model_path=\"\",  # Not used since model is provided\n        model_input_signature={},  # Empty dict as model is provided directly\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n    )\n\n    transformer.transform(dataset=None, model=None, config=config)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return data_output_path\n</code></pre>"},{"location":"api/integrations/zenml.html#pytorch-steps","title":"PyTorch Steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps","title":"mlpotion.integrations.zenml.pytorch.steps","text":"<p>ZenML steps for PyTorch framework.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    verbose: int = 1,\n    max_batches: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], EvaluationMetrics]\n</code></pre> <p>Evaluate a PyTorch model using <code>ModelEvaluator</code>.</p> <p>This step computes metrics on a given dataset using the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to evaluate.</p> required <code>dataloader</code> <code>DataLoader</code> <p>The evaluation <code>DataLoader</code>.</p> required <code>loss_fn</code> <code>str</code> <p>Name of the loss function (e.g., \"mse\", \"cross_entropy\").</p> <code>'mse'</code> <code>device</code> <code>str</code> <p>Device to evaluate on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>max_batches</code> <code>int | None</code> <p>Limit number of batches to evaluate (useful for debugging).</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, float], EvaluationMetrics]</code> <p>dict[str, float]: A dictionary of computed metrics.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef evaluate_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    verbose: int = 1,\n    max_batches: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], \"EvaluationMetrics\"]:\n    \"\"\"Evaluate a PyTorch model using `ModelEvaluator`.\n\n    This step computes metrics on a given dataset using the provided model.\n\n    Args:\n        model: The PyTorch model to evaluate.\n        dataloader: The evaluation `DataLoader`.\n        loss_fn: Name of the loss function (e.g., \"mse\", \"cross_entropy\").\n        device: Device to evaluate on (\"cpu\" or \"cuda\").\n        verbose: Verbosity mode (0 or 1).\n        max_batches: Limit number of batches to evaluate (useful for debugging).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, float]: A dictionary of computed metrics.\n    \"\"\"\n    logger.info(f\"Evaluating model on {device}\")\n\n    config = ModelEvaluationConfig(\n        batch_size=dataloader.batch_size or 32,\n        verbose=verbose,\n        device=device,\n        framework_options={\"loss_fn\": loss_fn, \"max_batches\": max_batches},\n    )\n\n    evaluator = ModelEvaluator()\n    result = evaluator.evaluate(\n        model=model,\n        dataloader=dataloader,\n        config=config,\n    )\n\n    # Extract metrics and evaluation time from result\n    metrics = {**result.metrics, \"evaluation_time\": result.evaluation_time}\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"metrics\": metrics})\n\n    return metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: nn.Module,\n    export_path: str,\n    export_format: str = \"state_dict\",\n    device: str = \"cpu\",\n    example_input: torch.Tensor | None = None,\n    jit_mode: str = \"script\",\n    input_names: list[str] | None = None,\n    output_names: list[str] | None = None,\n    dynamic_axes: dict[str, dict[int, str]] | None = None,\n    opset_version: int = 14,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, ExportPath]\n</code></pre> <p>Export a PyTorch model to disk using <code>ModelExporter</code>.</p> <p>This step exports the model to a specified format (TorchScript, ONNX, or state_dict).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to export.</p> required <code>export_path</code> <code>str</code> <p>The destination path for the exported model.</p> required <code>export_format</code> <code>str</code> <p>The format to export to (\"torchscript\", \"onnx\", \"state_dict\").</p> <code>'state_dict'</code> <code>device</code> <code>str</code> <p>Device to use for export (important for tracing).</p> <code>'cpu'</code> <code>example_input</code> <code>torch.Tensor | None</code> <p>Example input tensor (required for ONNX and TorchScript trace).</p> <code>None</code> <code>jit_mode</code> <code>str</code> <p>TorchScript mode (\"script\" or \"trace\").</p> <code>'script'</code> <code>input_names</code> <code>list[str] | None</code> <p>List of input names for ONNX export.</p> <code>None</code> <code>output_names</code> <code>list[str] | None</code> <p>List of output names for ONNX export.</p> <code>None</code> <code>dynamic_axes</code> <code>dict[str, dict[int, str]] | None</code> <p>Dictionary of dynamic axes for ONNX export.</p> <code>None</code> <code>opset_version</code> <code>int</code> <p>ONNX opset version.</p> <code>14</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, ExportPath]</code> <p>The path to the exported model artifact.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef export_model(\n    model: nn.Module,\n    export_path: str,\n    export_format: str = \"state_dict\",\n    device: str = \"cpu\",\n    example_input: torch.Tensor | None = None,\n    jit_mode: str = \"script\",\n    input_names: list[str] | None = None,\n    output_names: list[str] | None = None,\n    dynamic_axes: dict[str, dict[int, str]] | None = None,\n    opset_version: int = 14,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"ExportPath\"]:\n    \"\"\"Export a PyTorch model to disk using `ModelExporter`.\n\n    This step exports the model to a specified format (TorchScript, ONNX, or state_dict).\n\n    Args:\n        model: The PyTorch model to export.\n        export_path: The destination path for the exported model.\n        export_format: The format to export to (\"torchscript\", \"onnx\", \"state_dict\").\n        device: Device to use for export (important for tracing).\n        example_input: Example input tensor (required for ONNX and TorchScript trace).\n        jit_mode: TorchScript mode (\"script\" or \"trace\").\n        input_names: List of input names for ONNX export.\n        output_names: List of output names for ONNX export.\n        dynamic_axes: Dictionary of dynamic axes for ONNX export.\n        opset_version: ONNX opset version.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the exported model artifact.\n    \"\"\"\n    logger.info(f\"Exporting model to: {export_path} (format: {export_format})\")\n\n    config = ModelExportConfig(\n        export_path=export_path,\n        format=export_format,\n        device=device,\n        jit_mode=jit_mode,\n        example_input=example_input,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        opset_version=opset_version,\n    )\n\n    exporter = ModelExporter()\n    result = exporter.export(model=model, config=config)\n\n    if metadata:\n        log_step_metadata(\n            metadata={\n                **metadata,\n                \"export_path\": result.export_path,\n                \"format\": result.format,\n                \"metadata\": result.metadata,\n            }\n        )\n\n    return str(result.export_path)\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.load_csv_data","title":"load_csv_data","text":"<pre><code>load_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, PyTorchDataLoader]\n</code></pre> <p>Load data from CSV files into a PyTorch DataLoader.</p> <p>This step uses <code>CSVDataset</code> and <code>CSVDataLoader</code> to load data matching the specified file pattern. It returns a configured <code>DataLoader</code> ready for training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to copy tensors into CUDA pinned memory.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch.</p> <code>False</code> <code>dtype</code> <code>str</code> <p>Data type for the features (e.g., \"float32\").</p> <code>'float32'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>Annotated[DataLoader, PyTorchDataLoader]</code> <p>The configured PyTorch DataLoader.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef load_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    drop_last: bool = False,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, \"PyTorchDataLoader\"]:\n    \"\"\"Load data from CSV files into a PyTorch DataLoader.\n\n    This step uses `CSVDataset` and `CSVDataLoader` to load data matching the specified file pattern.\n    It returns a configured `DataLoader` ready for training or evaluation.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        shuffle: Whether to shuffle the data.\n        num_workers: Number of subprocesses to use for data loading.\n        pin_memory: Whether to copy tensors into CUDA pinned memory.\n        drop_last: Whether to drop the last incomplete batch.\n        dtype: Data type for the features (e.g., \"float32\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        DataLoader: The configured PyTorch DataLoader.\n    \"\"\"\n    logger.info(f\"Loading data from: {file_path}\")\n\n    # Convert dtype string to torch.dtype\n    torch_dtype = getattr(torch, dtype)\n\n    # Create dataset\n    dataset = CSVDataset(\n        file_pattern=file_path,\n        column_names=column_names,\n        label_name=label_name,\n        dtype=torch_dtype,\n    )\n\n    # Create DataLoader config\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        drop_last=drop_last,\n    )\n\n    # Create DataLoader using factory (exclude fields not accepted by CSVDataLoader)\n    loader_factory = CSVDataLoader(**config.dict(exclude={\"file_pattern\", \"config\"}))\n    dataloader = loader_factory.load(dataset)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataloader\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    model_class: type[nn.Module] | None = None,\n    map_location: str = \"cpu\",\n    strict: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[nn.Module, LoadedModel]\n</code></pre> <p>Load a PyTorch model from disk using <code>ModelPersistence</code>.</p> <p>This step loads a previously saved model. If loading a state dict, <code>model_class</code> must be provided.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> required <code>model_class</code> <code>type[nn.Module] | None</code> <p>The class of the model (required for state dict loading).</p> <code>None</code> <code>map_location</code> <code>str</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce state dict keys match.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[nn.Module, LoadedModel]</code> <p>nn.Module: The loaded PyTorch model.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef load_model(\n    model_path: str,\n    model_class: type[nn.Module] | None = None,\n    map_location: str = \"cpu\",\n    strict: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[nn.Module, \"LoadedModel\"]:\n    \"\"\"Load a PyTorch model from disk using `ModelPersistence`.\n\n    This step loads a previously saved model. If loading a state dict, `model_class`\n    must be provided.\n\n    Args:\n        model_path: The path to the saved model.\n        model_class: The class of the model (required for state dict loading).\n        map_location: Device to load the model onto.\n        strict: Whether to strictly enforce state dict keys match.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        nn.Module: The loaded PyTorch model.\n    \"\"\"\n    logger.info(f\"Loading model from: {model_path}\")\n\n    persistence = ModelPersistence(path=model_path)\n    model, _ = persistence.load(\n        model_class=model_class,\n        map_location=map_location,\n        strict=strict,\n    )\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"model_path\": model_path})\n\n    return model\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.load_streaming_csv_data","title":"load_streaming_csv_data","text":"<pre><code>load_streaming_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    chunksize: int = 10000,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, PyTorchDataLoader]\n</code></pre> <p>Load large CSV files as a streaming PyTorch DataLoader.</p> <p>This step uses <code>StreamingCSVDataset</code> to load data in chunks, making it suitable for datasets that do not fit in memory. It returns a <code>DataLoader</code> wrapping the iterable dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to copy tensors into CUDA pinned memory.</p> <code>False</code> <code>chunksize</code> <code>int</code> <p>Number of rows to read into memory at a time per file.</p> <code>10000</code> <code>dtype</code> <code>str</code> <p>Data type for the features (e.g., \"float32\").</p> <code>'float32'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>Annotated[DataLoader, PyTorchDataLoader]</code> <p>The configured streaming PyTorch DataLoader.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef load_streaming_csv_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    chunksize: int = 10000,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[DataLoader, \"PyTorchDataLoader\"]:\n    \"\"\"Load large CSV files as a streaming PyTorch DataLoader.\n\n    This step uses `StreamingCSVDataset` to load data in chunks, making it suitable for\n    datasets that do not fit in memory. It returns a `DataLoader` wrapping the iterable dataset.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        num_workers: Number of subprocesses to use for data loading.\n        pin_memory: Whether to copy tensors into CUDA pinned memory.\n        chunksize: Number of rows to read into memory at a time per file.\n        dtype: Data type for the features (e.g., \"float32\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        DataLoader: The configured streaming PyTorch DataLoader.\n    \"\"\"\n    logger.info(f\"Loading streaming data from: {file_path}\")\n\n    # Convert dtype string to torch.dtype\n    torch_dtype = getattr(torch, dtype)\n\n    # Create streaming dataset\n    dataset = StreamingCSVDataset(\n        file_pattern=file_path,\n        column_names=column_names,\n        label_name=label_name,\n        chunksize=chunksize,\n        dtype=torch_dtype,\n    )\n\n    # Create DataLoader config (no shuffle for streaming)\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        shuffle=False,  # Streaming datasets don't support shuffle\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        drop_last=False,\n    )\n\n    # Create DataLoader using factory (exclude fields not accepted by CSVDataLoader)\n    loader_factory = CSVDataLoader(**config.dict(exclude={\"file_pattern\", \"config\"}))\n    dataloader = loader_factory.load(dataset)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return dataloader\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: nn.Module,\n    save_path: str,\n    save_full_model: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, SavePath]\n</code></pre> <p>Save a PyTorch model to disk using <code>ModelPersistence</code>.</p> <p>This step saves the model for later reloading. It supports saving just the state dict (recommended) or the full model object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to save.</p> required <code>save_path</code> <code>str</code> <p>The destination path.</p> required <code>save_full_model</code> <code>bool</code> <p>Whether to save the full model object (pickle) instead of state dict.</p> <code>False</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, SavePath]</code> <p>The path to the saved model.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef save_model(\n    model: nn.Module,\n    save_path: str,\n    save_full_model: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"SavePath\"]:\n    \"\"\"Save a PyTorch model to disk using `ModelPersistence`.\n\n    This step saves the model for later reloading. It supports saving just the state dict\n    (recommended) or the full model object.\n\n    Args:\n        model: The PyTorch model to save.\n        save_path: The destination path.\n        save_full_model: Whether to save the full model object (pickle) instead of state dict.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved model.\n    \"\"\"\n    logger.info(f\"Saving model to: {save_path}\")\n\n    persistence = ModelPersistence(path=save_path, model=model)\n    persistence.save(save_full_model=save_full_model)\n\n    if metadata:\n        log_step_metadata(\n            metadata={\n                **metadata,\n                \"save_path\": save_path,\n                \"save_full_model\": save_full_model,\n            }\n        )\n\n    return save_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    validation_dataloader: DataLoader | None = None,\n    verbose: int = 1,\n    max_batches_per_epoch: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[nn.Module, TrainedModel],\n    Annotated[dict[str, float], TrainingMetrics],\n]\n</code></pre> <p>Train a PyTorch model using <code>ModelTrainer</code>.</p> <p>This step configures and runs a training session. It supports validation data, custom loss functions, and automatic device management.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>The PyTorch model to train.</p> required <code>dataloader</code> <code>DataLoader</code> <p>The training <code>DataLoader</code>.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>optimizer</code> <code>str</code> <p>Name of the optimizer (e.g., \"adam\", \"sgd\").</p> <code>'adam'</code> <code>loss_fn</code> <code>str</code> <p>Name of the loss function (e.g., \"mse\", \"cross_entropy\").</p> <code>'mse'</code> <code>device</code> <code>str</code> <p>Device to train on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>validation_dataloader</code> <code>DataLoader | None</code> <p>Optional validation <code>DataLoader</code>.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>max_batches_per_epoch</code> <code>int | None</code> <p>Limit number of batches per epoch (useful for debugging).</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Annotated[nn.Module, TrainedModel], Annotated[dict[str, float], TrainingMetrics]]</code> <p>Tuple[nn.Module, dict[str, float]]: The trained model and a dictionary of final metrics.</p> Source code in <code>mlpotion/integrations/zenml/pytorch/steps.py</code> <pre><code>@step\ndef train_model(\n    model: nn.Module,\n    dataloader: DataLoader,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n    optimizer: str = \"adam\",\n    loss_fn: str = \"mse\",\n    device: str = \"cpu\",\n    validation_dataloader: DataLoader | None = None,\n    verbose: int = 1,\n    max_batches_per_epoch: int | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[nn.Module, \"TrainedModel\"], Annotated[dict[str, float], \"TrainingMetrics\"]\n]:\n    \"\"\"Train a PyTorch model using `ModelTrainer`.\n\n    This step configures and runs a training session. It supports validation data,\n    custom loss functions, and automatic device management.\n\n    Args:\n        model: The PyTorch model to train.\n        dataloader: The training `DataLoader`.\n        epochs: Number of epochs to train.\n        learning_rate: Learning rate for the optimizer.\n        optimizer: Name of the optimizer (e.g., \"adam\", \"sgd\").\n        loss_fn: Name of the loss function (e.g., \"mse\", \"cross_entropy\").\n        device: Device to train on (\"cpu\" or \"cuda\").\n        validation_dataloader: Optional validation `DataLoader`.\n        verbose: Verbosity mode (0 or 1).\n        max_batches_per_epoch: Limit number of batches per epoch (useful for debugging).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        Tuple[nn.Module, dict[str, float]]: The trained model and a dictionary of final metrics.\n    \"\"\"\n    logger.info(f\"Training model for {epochs} epochs on {device}\")\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        device=device,\n        verbose=verbose,\n        max_batches_per_epoch=max_batches_per_epoch,\n    )\n\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model=model,\n        dataloader=dataloader,\n        config=config,\n        validation_dataloader=validation_dataloader,\n    )\n\n    if metadata:\n        log_step_metadata(\n            metadata={\n                **metadata,\n                \"history\": result.history,\n                \"best_epoch\": result.best_epoch,\n                \"final_metrics\": result.metrics,\n            }\n        )\n    logger.info(f\"{result=}\")\n    model = result.model\n    metrics = result.metrics\n\n    return model, metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#keras-steps","title":"Keras Steps","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps","title":"mlpotion.integrations.zenml.keras.steps","text":"<p>ZenML steps for Keras framework.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: keras.Model,\n    data: CSVSequence,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], EvaluationMetrics]\n</code></pre> <p>Evaluate a Keras model using <code>ModelEvaluator</code>.</p> <p>This step computes metrics on a given dataset using the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to evaluate.</p> required <code>data</code> <code>CSVSequence</code> <p>The evaluation dataset (<code>CSVSequence</code>).</p> required <code>verbose</code> <code>int</code> <p>Verbosity mode (0 or 1).</p> <code>1</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, float], EvaluationMetrics]</code> <p>dict[str, float]: A dictionary of computed metrics.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef evaluate_model(\n    model: keras.Model,\n    data: CSVSequence,\n    verbose: int = 1,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, float], \"EvaluationMetrics\"]:\n    \"\"\"Evaluate a Keras model using `ModelEvaluator`.\n\n    This step computes metrics on a given dataset using the provided model.\n\n    Args:\n        model: The Keras model to evaluate.\n        data: The evaluation dataset (`CSVSequence`).\n        verbose: Verbosity mode (0 or 1).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, float]: A dictionary of computed metrics.\n    \"\"\"\n    logger.info(\"Evaluating model\")\n\n    evaluator = ModelEvaluator()\n\n    config = ModelEvaluationConfig(\n        verbose=verbose,\n    )\n\n    result = evaluator.evaluate(\n        model=model,\n        dataset=data,\n        config=config,\n    )\n\n    metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"metrics\": metrics})\n\n    return metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.export_model","title":"export_model","text":"<pre><code>export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, ExportPath]\n</code></pre> <p>Export a Keras model to disk using <code>ModelExporter</code>.</p> <p>This step exports the model to a specified format (e.g., SavedModel, H5, TFLite).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to export.</p> required <code>export_path</code> <code>str</code> <p>The destination path for the exported model.</p> required <code>export_format</code> <code>str | None</code> <p>The format to export to (optional).</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, ExportPath]</code> <p>The path to the exported model artifact.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef export_model(\n    model: keras.Model,\n    export_path: str,\n    export_format: str | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"ExportPath\"]:\n    \"\"\"Export a Keras model to disk using `ModelExporter`.\n\n    This step exports the model to a specified format (e.g., SavedModel, H5, TFLite).\n\n    Args:\n        model: The Keras model to export.\n        export_path: The destination path for the exported model.\n        export_format: The format to export to (optional).\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the exported model artifact.\n    \"\"\"\n    logger.info(f\"Exporting model to: {export_path}\")\n\n    exporter = ModelExporter()\n\n    config = {}\n    if export_format:\n        config[\"export_format\"] = export_format\n\n    exporter.export(\n        model=model,\n        path=export_path,\n        **config,\n    )\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"export_path\": export_path})\n\n    return export_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.inspect_model","title":"inspect_model","text":"<pre><code>inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], ModelInspection]\n</code></pre> <p>Inspect a Keras model using <code>ModelInspector</code>.</p> <p>This step extracts metadata about the model, such as layer configuration, input/output shapes, and parameter counts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to inspect.</p> required <code>include_layers</code> <code>bool</code> <p>Whether to include detailed layer information.</p> <code>True</code> <code>include_signatures</code> <code>bool</code> <p>Whether to include signature information.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[dict[str, Any], ModelInspection]</code> <p>dict[str, Any]: A dictionary containing the inspection results.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef inspect_model(\n    model: keras.Model,\n    include_layers: bool = True,\n    include_signatures: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[dict[str, Any], \"ModelInspection\"]:\n    \"\"\"Inspect a Keras model using `ModelInspector`.\n\n    This step extracts metadata about the model, such as layer configuration,\n    input/output shapes, and parameter counts.\n\n    Args:\n        model: The Keras model to inspect.\n        include_layers: Whether to include detailed layer information.\n        include_signatures: Whether to include signature information.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the inspection results.\n    \"\"\"\n    logger.info(\"Inspecting model\")\n\n    inspector = ModelInspector(\n        include_layers=include_layers,\n        include_signatures=include_signatures,\n    )\n    inspection = inspector.inspect(model)\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"inspection\": inspection})\n\n    return inspection\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.load_data","title":"load_data","text":"<pre><code>load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[CSVSequence, CSVSequence]\n</code></pre> <p>Load data from CSV files into a Keras Sequence.</p> <p>This step uses <code>CSVDataLoader</code> to load data matching the specified file pattern. It returns a <code>CSVSequence</code> which can be used for training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Glob pattern for CSV files (e.g., \"data/*.csv\").</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch.</p> <code>32</code> <code>label_name</code> <code>str | None</code> <p>Name of the column to use as the label.</p> <code>None</code> <code>column_names</code> <code>list[str] | None</code> <p>List of specific columns to load.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>dtype</code> <code>str</code> <p>Data type for the features (e.g., \"float32\").</p> <code>'float32'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CSVSequence</code> <code>Annotated[CSVSequence, CSVSequence]</code> <p>The loaded Keras Sequence.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef load_data(\n    file_path: str,\n    batch_size: int = 32,\n    label_name: str | None = None,\n    column_names: list[str] | None = None,\n    shuffle: bool = True,\n    dtype: str = \"float32\",\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[CSVSequence, \"CSVSequence\"]:\n    \"\"\"Load data from CSV files into a Keras Sequence.\n\n    This step uses `CSVDataLoader` to load data matching the specified file pattern.\n    It returns a `CSVSequence` which can be used for training or evaluation.\n\n    Args:\n        file_path: Glob pattern for CSV files (e.g., \"data/*.csv\").\n        batch_size: Number of samples per batch.\n        label_name: Name of the column to use as the label.\n        column_names: List of specific columns to load.\n        shuffle: Whether to shuffle the data.\n        dtype: Data type for the features (e.g., \"float32\").\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        CSVSequence: The loaded Keras Sequence.\n    \"\"\"\n    logger.info(f\"Loading data from: {file_path}\")\n\n    config = DataLoadingConfig(\n        file_pattern=file_path,\n        batch_size=batch_size,\n        column_names=column_names,\n        label_name=label_name,\n        shuffle=shuffle,\n        dtype=dtype,\n    )\n\n    loader = CSVDataLoader(**config.dict())\n    sequence = loader.load()\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return sequence\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.load_model","title":"load_model","text":"<pre><code>load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, LoadedModel]\n</code></pre> <p>Load a Keras model from disk using <code>ModelPersistence</code>.</p> <p>This step loads a previously saved model. It can optionally inspect the loaded model to log metadata about its structure.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> required <code>inspect</code> <code>bool</code> <p>Whether to inspect the model after loading.</p> <code>True</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Annotated[keras.Model, LoadedModel]</code> <p>keras.Model: The loaded Keras model.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef load_model(\n    model_path: str,\n    inspect: bool = True,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[keras.Model, \"LoadedModel\"]:\n    \"\"\"Load a Keras model from disk using `ModelPersistence`.\n\n    This step loads a previously saved model. It can optionally inspect the loaded model\n    to log metadata about its structure.\n\n    Args:\n        model_path: The path to the saved model.\n        inspect: Whether to inspect the model after loading.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        keras.Model: The loaded Keras model.\n    \"\"\"\n    logger.info(f\"Loading model from: {model_path}\")\n\n    persistence = ModelPersistence(path=model_path)\n    model, inspection = persistence.load(inspect=inspect)\n\n    if metadata:\n        meta = {**metadata}\n        if inspection:\n            meta[\"inspection\"] = inspection\n        log_step_metadata(metadata=meta)\n\n    return model\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.save_model","title":"save_model","text":"<pre><code>save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, SavePath]\n</code></pre> <p>Save a Keras model to disk using <code>ModelPersistence</code>.</p> <p>This step saves the model for later reloading, typically preserving the optimizer state.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to save.</p> required <code>save_path</code> <code>str</code> <p>The destination path.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, SavePath]</code> <p>The path to the saved model.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef save_model(\n    model: keras.Model,\n    save_path: str,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"SavePath\"]:\n    \"\"\"Save a Keras model to disk using `ModelPersistence`.\n\n    This step saves the model for later reloading, typically preserving the optimizer state.\n\n    Args:\n        model: The Keras model to save.\n        save_path: The destination path.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved model.\n    \"\"\"\n    logger.info(f\"Saving model to: {save_path}\")\n\n    persistence = ModelPersistence(path=save_path, model=model)\n    persistence.save()\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"save_path\": save_path})\n\n    return save_path\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: keras.Model,\n    data: CSVSequence,\n    epochs: int = 10,\n    validation_data: CSVSequence | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    callbacks: list[Any] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, TrainedModel],\n    Annotated[dict[str, float], TrainingMetrics],\n]\n</code></pre> <p>Train a Keras model using <code>ModelTrainer</code>.</p> <p>This step configures and runs a training session. It supports validation data, callbacks, and logging of training metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>keras.Model</code> <p>The Keras model to train.</p> required <code>data</code> <code>CSVSequence</code> <p>The training dataset (<code>CSVSequence</code>).</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>validation_data</code> <code>CSVSequence | None</code> <p>Optional validation dataset (<code>CSVSequence</code>).</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer.</p> <code>0.001</code> <code>verbose</code> <code>int</code> <p>Verbosity mode (0, 1, or 2).</p> <code>1</code> <code>callbacks</code> <code>list[Any] | None</code> <p>List of Keras callbacks to apply during training.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Annotated[keras.Model, TrainedModel], Annotated[dict[str, float], TrainingMetrics]]</code> <p>Tuple[keras.Model, dict[str, float]]: The trained model and a dictionary of final metrics.</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef train_model(\n    model: keras.Model,\n    data: CSVSequence,\n    epochs: int = 10,\n    validation_data: CSVSequence | None = None,\n    learning_rate: float = 0.001,\n    verbose: int = 1,\n    callbacks: list[Any] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Tuple[\n    Annotated[keras.Model, \"TrainedModel\"],\n    Annotated[dict[str, float], \"TrainingMetrics\"],\n]:\n    \"\"\"Train a Keras model using `ModelTrainer`.\n\n    This step configures and runs a training session. It supports validation data,\n    callbacks, and logging of training metrics.\n\n    Args:\n        model: The Keras model to train.\n        data: The training dataset (`CSVSequence`).\n        epochs: Number of epochs to train.\n        validation_data: Optional validation dataset (`CSVSequence`).\n        learning_rate: Learning rate for the Adam optimizer.\n        verbose: Verbosity mode (0, 1, or 2).\n        callbacks: List of Keras callbacks to apply during training.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        Tuple[keras.Model, dict[str, float]]: The trained model and a dictionary of final metrics.\n    \"\"\"\n    logger.info(f\"Training model for {epochs} epochs\")\n\n    trainer = ModelTrainer()\n\n    config = ModelTrainingConfig(\n        epochs=epochs,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        optimizer=\"adam\",  # Defaulting to adam as per previous logic\n        loss=\"mse\",\n        metrics=[\"mae\"],\n        framework_options={\"callbacks\": callbacks} if callbacks else {},\n    )\n    # If user passed custom optimizer/loss/metrics via some other way, we might need to handle it,\n    # but here we are hardcoding them as per previous implementation.\n    # Actually, the previous implementation created an optimizer instance.\n    # ModelTrainingConfig supports passing instances via arbitrary types if allowed,\n    # or we can pass them via framework_options if the trainer supports it.\n    # But Keras ModelTrainer uses config fields.\n    # Let's stick to the config fields.\n\n    # Note: The previous implementation created a new optimizer instance: keras.optimizers.Adam(learning_rate=learning_rate)\n    # The new ModelTrainer handles optimizer creation from config.\n\n    result = trainer.train(\n        model=model,\n        dataset=data,\n        config=config,\n        validation_dataset=validation_data,\n    )\n\n    # Result is TrainingResult object\n    training_metrics = result.metrics\n\n    if metadata:\n        log_step_metadata(metadata={**metadata, \"history\": result.history})\n\n    return model, training_metrics\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.keras.steps.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    dataset: CSVSequence,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    batch_size: int | None = None,\n    feature_names: list[str] | None = None,\n    input_columns: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, OutputPath]\n</code></pre> <p>Transform data using a Keras model and save predictions to CSV.</p> <p>This step uses <code>CSVDataTransformer</code> to run inference on a dataset using a provided model and saves the results to the specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>CSVSequence</code> <p>The input dataset (<code>CSVSequence</code>).</p> required <code>model</code> <code>keras.Model</code> <p>The Keras model to use for transformation.</p> required <code>data_output_path</code> <code>str</code> <p>Path to save the transformed data (CSV).</p> required <code>data_output_per_batch</code> <code>bool</code> <p>Whether to save a separate file per batch.</p> <code>False</code> <code>batch_size</code> <code>int | None</code> <p>Batch size for inference (overrides dataset batch size if provided).</p> <code>None</code> <code>feature_names</code> <code>list[str] | None</code> <p>Optional list of feature names for the output CSV.</p> <code>None</code> <code>input_columns</code> <code>list[str] | None</code> <p>Optional list of input columns to pass to the model.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to log to ZenML.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Annotated[str, OutputPath]</code> <p>The path to the saved output file(s).</p> Source code in <code>mlpotion/integrations/zenml/keras/steps.py</code> <pre><code>@step\ndef transform_data(\n    dataset: CSVSequence,\n    model: keras.Model,\n    data_output_path: str,\n    data_output_per_batch: bool = False,\n    batch_size: int | None = None,\n    feature_names: list[str] | None = None,\n    input_columns: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; Annotated[str, \"OutputPath\"]:\n    \"\"\"Transform data using a Keras model and save predictions to CSV.\n\n    This step uses `CSVDataTransformer` to run inference on a dataset using a provided model\n    and saves the results to the specified output path.\n\n    Args:\n        dataset: The input dataset (`CSVSequence`).\n        model: The Keras model to use for transformation.\n        data_output_path: Path to save the transformed data (CSV).\n        data_output_per_batch: Whether to save a separate file per batch.\n        batch_size: Batch size for inference (overrides dataset batch size if provided).\n        feature_names: Optional list of feature names for the output CSV.\n        input_columns: Optional list of input columns to pass to the model.\n        metadata: Optional dictionary of metadata to log to ZenML.\n\n    Returns:\n        str: The path to the saved output file(s).\n    \"\"\"\n    logger.info(f\"Transforming data and saving to: {data_output_path}\")\n\n    config = DataTransformationConfig(\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        input_columns=input_columns,\n    )\n\n    transformer = CSVDataTransformer(\n        dataset=dataset,\n        model=model,\n        data_output_path=data_output_path,\n        data_output_per_batch=data_output_per_batch,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        input_columns=input_columns,\n    )\n    transformer.transform(dataset=dataset, model=model, config=config)\n\n    if metadata:\n        log_step_metadata(metadata=metadata)\n\n    return data_output_path\n</code></pre>"},{"location":"api/integrations/zenml.html#materializers","title":"Materializers","text":"<p> See the ZenML Integration Guide for usage examples </p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers","title":"mlpotion.integrations.zenml.tensorflow.materializers","text":"<p>Custom materializers for TensorFlow types.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers-classes","title":"Classes","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer","title":"TFConfigDatasetMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Materializer for tf.data.Dataset created from CSV files.</p> <p>Instead of serializing the entire dataset to TFRecords, this materializer stores only the configuration needed to recreate the dataset using <code>tf.data.experimental.make_csv_dataset</code>. This is much more efficient and avoids shape-related issues during serialization/deserialization.</p> <p>This materializer works specifically with datasets created via: - <code>tf.data.experimental.make_csv_dataset</code> - MLPotion's <code>TFCSVDataLoader</code></p> <p>Advantages: - Lightweight: Only stores config, not data - Fast: No TFRecord serialization overhead - Reliable: Recreates dataset with exact same parameters - Flexible: Works with any subsequent transformations (batching, shuffling, etc.)</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer.load","title":"load","text":"<pre><code>load(data_type: Type[Any]) -&gt; tf.data.Dataset\n</code></pre> <p>Load dataset by recreating it from stored configuration.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Type[Any]</code> <p>The type of the data to load.</p> required <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>Recreated tf.data.Dataset with the same configuration.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: Type[Any]) -&gt; tf.data.Dataset:\n    \"\"\"Load dataset by recreating it from stored configuration.\n\n    Args:\n        data_type: The type of the data to load.\n\n    Returns:\n        Recreated tf.data.Dataset with the same configuration.\n    \"\"\"\n    config_path = Path(self.uri) / \"config.json\"\n\n    logger.info(\"Loading CSV dataset config from: %s\", config_path)\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        config = json.load(f)\n\n    logger.info(\"Recreating dataset with config: %s\", config)\n\n    # Use CSVDataLoader to recreate the dataset\n    # This ensures we handle empty lines correctly (unlike make_csv_dataset)\n    from mlpotion.frameworks.tensorflow.data.loaders import CSVDataLoader\n\n    # Extract parameters for CSVDataLoader\n    loader_config = {\n        \"file_pattern\": config[\"file_pattern\"],\n        \"batch_size\": config[\"batch_size\"],\n        \"label_name\": config.get(\"label_name\"),\n        \"column_names\": config.get(\"column_names\"),\n    }\n\n    # Handle num_epochs and other config\n    extra_params = config.get(\"extra_params\", {})\n    if \"num_epochs\" in config:\n        extra_params[\"num_epochs\"] = config[\"num_epochs\"]\n    elif \"num_epochs\" not in extra_params:\n        extra_params[\"num_epochs\"] = 1\n\n    if extra_params:\n        loader_config[\"config\"] = extra_params\n\n    # Create loader and load dataset\n    loader = CSVDataLoader(**loader_config)\n    dataset = loader.load()\n\n    # Apply any transformations that were recorded\n    transformations = config.get(\"transformations\", [])\n    for transform in transformations:\n        transform_type = transform[\"type\"]\n        params = transform[\"params\"]\n\n        if transform_type == \"batch\":\n            dataset = dataset.batch(params[\"batch_size\"])\n        elif transform_type == \"shuffle\":\n            dataset = dataset.shuffle(params[\"buffer_size\"])\n        elif transform_type == \"prefetch\":\n            buffer_size = params[\"buffer_size\"]\n            if buffer_size == \"AUTOTUNE\":\n                buffer_size = tf.data.AUTOTUNE\n            dataset = dataset.prefetch(buffer_size)\n        elif transform_type == \"unbatch\":\n            dataset = dataset.unbatch()\n        elif transform_type == \"repeat\":\n            count = params.get(\"count\")\n            dataset = dataset.repeat(count)\n        # Add more transformation types as needed\n\n    logger.info(\"\u2705 Successfully recreated CSV dataset\")\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFConfigDatasetMaterializer.save","title":"save","text":"<pre><code>save(data: tf.data.Dataset) -&gt; None\n</code></pre> <p>Save dataset configuration instead of actual data.</p> <p>This method attempts to extract the original CSV loading configuration from the dataset. If the dataset doesn't have this metadata, it falls back to the TFRecord materializer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>tf.data.Dataset</code> <p>The dataset to save configuration for.</p> required Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.data.Dataset) -&gt; None:\n    \"\"\"Save dataset configuration instead of actual data.\n\n    This method attempts to extract the original CSV loading configuration\n    from the dataset. If the dataset doesn't have this metadata, it falls\n    back to the TFRecord materializer.\n\n    Args:\n        data: The dataset to save configuration for.\n    \"\"\"\n    config_path = Path(self.uri) / \"config.json\"\n    config_path.parent.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"\ud83d\udd35 TFConfigDatasetMaterializer.save() called\")\n    logger.info(\"Saving CSV dataset config to: %s\", config_path)\n    logger.debug(\"Dataset type: %s\", type(data))\n    logger.debug(\"URI: %s\", self.uri)\n\n    # Try to extract configuration from the dataset\n    # This requires the dataset to have been created with our loader\n    # or to have metadata attached\n    config = self._extract_config_from_dataset(data)\n\n    if config is None:\n        logger.warning(\n            \"\u274c Could not extract CSV config from dataset. \"\n            \"This materializer only works with datasets created from CSV files. \"\n            \"Falling back to TFRecord materializer.\"\n        )\n        logger.debug(\n            \"Dataset attributes: %s\",\n            [attr for attr in dir(data) if not attr.startswith(\"__\")],\n        )\n        # Fall back to TFRecord materializer\n        from mlpotion.integrations.zenml.tensorflow.materializers import (\n            TFRecordDatasetMaterializer,\n        )\n\n        logger.info(\"\ud83d\udd04 Falling back to TFRecordDatasetMaterializer\")\n        try:\n            tfrecord_materializer = TFRecordDatasetMaterializer(self.uri)\n            tfrecord_materializer.save(data)\n            logger.info(\"\u2705 Successfully saved dataset as TFRecord\")\n        except Exception as e:\n            logger.error(f\"Failed to save as TFRecord: {e}\")\n            raise\n        return\n\n    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=2)\n\n    logger.info(\"\u2705 Successfully saved CSV dataset config to: %s\", config_path)\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer","title":"TFRecordDatasetMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Generic TFRecord materializer for <code>tf.data.Dataset</code>.</p> <p>This materializer is designed to be robust and round-trip safe for datasets produced by <code>tf.data.experimental.make_csv_dataset</code>, and in general for any dataset whose <code>element_spec</code> is a nested structure of:</p> <pre><code>- dict / tuple / list containers\n- `tf.TensorSpec` leaves\n</code></pre> <p>It works as follows:</p> <ul> <li> <p>Save:</p> <ul> <li>Reads <code>dataset.element_spec</code> and serializes it to JSON.</li> <li>For each batch (dataset element), recursively flattens it to a   list of tensors in a deterministic order implied by the spec.</li> <li>Writes a single <code>tf.train.Example</code> per batch, with features   named \"f0\", \"f1\", ... corresponding to each leaf tensor.</li> </ul> </li> <li> <p>Load:</p> <ul> <li>Deserializes <code>element_spec</code> from JSON.</li> <li>Builds a <code>feature_description</code> for <code>tf.io.parse_single_example</code>   using the leaf specs.</li> <li>Parses each example into a list of tensors.</li> <li>Recursively unflattens the list back into the same nested   structure as <code>element_spec</code>.</li> </ul> </li> </ul> <p>This supports all typical <code>make_csv_dataset</code> shapes:</p> <pre><code>1. label_name=None:\n   element: dict[str, Tensor]\n\n2. label_name=\"target\":\n   element: (dict[str, Tensor], Tensor)\n\n3. label_name=[\"t1\", \"t2\"]:\n   element: (dict[str, Tensor], dict[str, Tensor])\n</code></pre> <p>and also more complex nesting as long as it's composed of dict / tuple / list and TensorSpec leaves.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer.load","title":"load","text":"<pre><code>load(data_type: Type[Any]) -&gt; tf.data.Dataset\n</code></pre> <p>Deserialize a <code>tf.data.Dataset</code> from TFRecord + metadata JSON.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: Type[Any]) -&gt; tf.data.Dataset:\n    \"\"\"Deserialize a `tf.data.Dataset` from TFRecord + metadata JSON.\"\"\"\n    dataset_dir = Path(self.uri)\n    tfrecord_path = str(dataset_dir / \"data.tfrecord\")\n    metadata_path = dataset_dir / \"metadata.json\"\n\n    logger.info(\"Loading dataset from TFRecord: %s\", tfrecord_path)\n\n    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n        metadata = json.load(f)\n\n    element_spec = self._deserialize_element_spec(metadata[\"element_spec\"])\n    num_leaves = metadata[\"num_leaves\"]\n    concrete_shapes = metadata.get(\n        \"concrete_shapes\", None\n    )  # May be None for older versions\n\n    logger.info(\"Loaded element_spec: %s\", element_spec)\n    logger.info(\"Expected number of leaves: %s\", num_leaves)\n    if concrete_shapes:\n        logger.info(\"Concrete shapes available: %s\", concrete_shapes)\n\n    flat_spec_leaves = self._flatten_element_spec(element_spec)\n    if len(flat_spec_leaves) != num_leaves:\n        raise ValueError(\n            f\"Metadata num_leaves={num_leaves} but element_spec \"\n            f\"has {len(flat_spec_leaves)} leaves.\"\n        )\n\n    # Build feature description for parsing\n    feature_description = self._build_feature_description(flat_spec_leaves)\n\n    def parse_fn(serialized_example: tf.Tensor) -&gt; Any:\n        parsed = tf.io.parse_single_example(serialized_example, feature_description)\n\n        flat_tensors: list[tf.Tensor] = []\n        for i, (_, leaf_spec) in enumerate(flat_spec_leaves):\n            key = f\"f{i}\"\n\n            if leaf_spec.dtype in (tf.float32, tf.float64, tf.int32, tf.int64):\n                # Numeric: stored as VarLenFeature, results in 1D tensor\n                dense = tf.sparse.to_dense(parsed[key])\n                tensor = tf.cast(dense, leaf_spec.dtype)\n\n                # Use concrete shape if available, otherwise fall back to spec-based logic\n                if concrete_shapes and i &lt; len(concrete_shapes):\n                    # We have the actual shape from when the data was saved\n                    concrete_shape = concrete_shapes[i]\n                    # Replace None with -1 for reshape\n                    target_shape = [\n                        d if d is not None else -1 for d in concrete_shape\n                    ]\n                    tensor = tf.reshape(tensor, target_shape)\n                    # Set the shape with proper None values\n                    tensor.set_shape(concrete_shape)\n                else:\n                    # Fallback to spec-based reshaping (legacy behavior)\n                    if leaf_spec.shape.rank is not None:\n                        if leaf_spec.shape.rank == 1:\n                            # Original was 1D, VarLen already gives us 1D - just set shape\n                            tensor.set_shape(leaf_spec.shape)\n                        elif leaf_spec.shape.rank &gt; 1:\n                            # Original was multi-dimensional - need to reshape from 1D\n                            shape_list = leaf_spec.shape.as_list()\n                            none_indices = [\n                                i for i, d in enumerate(shape_list) if d is None\n                            ]\n\n                            if len(none_indices) &lt;= 1:\n                                # Safe to reshape with at most one -1\n                                target_shape = [\n                                    d if d is not None else -1 for d in shape_list\n                                ]\n                                tensor = tf.reshape(tensor, target_shape)\n                                tensor.set_shape(leaf_spec.shape)\n                    else:\n                        # Unknown rank - just set shape\n                        tensor.set_shape(leaf_spec.shape)\n            else:\n                # Other dtypes: stored as serialized bytes\n                serialized = parsed[key]\n                tensor = tf.io.parse_tensor(serialized, out_type=leaf_spec.dtype)\n                tensor.set_shape(leaf_spec.shape)\n\n            flat_tensors.append(tensor)\n\n        # Rebuild nested structure\n        flat_iter = iter(flat_tensors)\n        return self._unflatten_data_with_spec(element_spec, flat_iter)\n\n    dataset = tf.data.TFRecordDataset(tfrecord_path)\n    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n    logger.info(\"Successfully loaded dataset from TFRecord.\")\n    logger.info(\"Dataset cardinality: %s\", dataset.cardinality().numpy())\n\n    return dataset\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TFRecordDatasetMaterializer.save","title":"save","text":"<pre><code>save(data: tf.data.Dataset) -&gt; None\n</code></pre> <p>Serialize a <code>tf.data.Dataset</code> to TFRecord + metadata JSON.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.data.Dataset) -&gt; None:\n    \"\"\"Serialize a `tf.data.Dataset` to TFRecord + metadata JSON.\"\"\"\n    dataset_dir = Path(self.uri)\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    tfrecord_path = str(dataset_dir / \"data.tfrecord\")\n    metadata_path = dataset_dir / \"metadata.json\"\n\n    element_spec = data.element_spec\n\n    logger.info(\"Saving dataset to TFRecord: %s\", tfrecord_path)\n    logger.info(\"Dataset element_spec: %s\", element_spec)\n\n    # Handle cardinality\n    cardinality = data.cardinality().numpy()\n    logger.info(\"Dataset cardinality: %s\", cardinality)\n\n    if cardinality == tf.data.INFINITE_CARDINALITY:\n        logger.warning(\"Infinite dataset detected. Taking first 100000 batches.\")\n        data = data.take(100_000)\n    elif cardinality == tf.data.UNKNOWN_CARDINALITY:\n        logger.warning(\"Unknown dataset cardinality. Taking first 100000 batches.\")\n        data = data.take(100_000)\n    else:\n        logger.info(\"Finite dataset with %s batches.\", cardinality)\n\n    # Serialize element_spec so we can restore structure and leaf specs\n    serialized_spec = self._serialize_element_spec(element_spec)\n    flat_spec_leaves = self._flatten_element_spec(element_spec)\n    num_leaves = len(flat_spec_leaves)\n\n    # Get concrete shapes from the first batch element (if available)\n    # We store shapes WITHOUT the batch dimension to handle variable batch sizes\n    concrete_shapes = None\n    try:\n        first_batch = next(iter(data.take(1)))\n        flat_tensors_sample: list[tf.Tensor] = []\n        self._flatten_data_with_spec(first_batch, element_spec, flat_tensors_sample)\n        # Store the shape WITHOUT the first (batch) dimension\n        # This allows the materializer to work with variable batch sizes\n        concrete_shapes = []\n        for t in flat_tensors_sample:\n            shape_list = list(t.shape.as_list())\n            # Remove the first (batch) dimension, keep the rest\n            if len(shape_list) &gt; 1:\n                shape_without_batch = [None] + shape_list[1:]  # None for batch dim\n            else:\n                shape_without_batch = [None]  # Just batch dimension\n            concrete_shapes.append(shape_without_batch)\n    except Exception:\n        # If we can't get a sample, proceed without concrete shapes\n        pass\n\n    metadata = {\n        \"format_version\": \"3.1\",  # Increment version for new feature\n        \"element_spec\": serialized_spec,\n        \"num_leaves\": num_leaves,\n        \"concrete_shapes\": concrete_shapes,  # Store actual shapes if available\n    }\n\n    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, indent=2)\n\n    # Write TFRecord\n    writer = tf.io.TFRecordWriter(tfrecord_path)\n    batch_count = 0\n\n    for batch in data:\n        flat_tensors: list[tf.Tensor] = []\n        self._flatten_data_with_spec(batch, element_spec, flat_tensors)\n\n        if len(flat_tensors) != num_leaves:\n            raise ValueError(\n                f\"Flattened batch has {len(flat_tensors)} leaves but \"\n                f\"element_spec indicates {num_leaves}.\"\n            )\n\n        example = self._flat_tensors_to_example(flat_tensors)\n        writer.write(example.SerializeToString())\n        batch_count += 1\n\n        if batch_count % 100 == 0:\n            logger.info(\"Written %d batches...\", batch_count)\n\n    writer.close()\n    logger.info(\"Successfully saved %d batches to TFRecord.\", batch_count)\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer","title":"TensorMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Materializer for TensorFlow Tensor objects.</p> <p>This materializer handles the serialization and deserialization of <code>tf.Tensor</code> objects. It saves tensors as binary protobuf files (<code>tensor.pb</code>) using <code>tf.io.serialize_tensor</code>.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer.load","title":"load","text":"<pre><code>load(data_type: type[Any]) -&gt; tf.Tensor\n</code></pre> <p>Load a TensorFlow Tensor from the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type[Any]</code> <p>The type of the data to load (should be <code>tf.Tensor</code>).</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>tf.Tensor: The loaded tensor.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: type[Any]) -&gt; tf.Tensor:  # noqa: ARG002\n    \"\"\"Load a TensorFlow Tensor from the artifact store.\n\n    Args:\n        data_type: The type of the data to load (should be `tf.Tensor`).\n\n    Returns:\n        tf.Tensor: The loaded tensor.\n    \"\"\"\n    logger.info(\"Loading TensorFlow tensor...\")\n    try:\n        tensor_path = Path(self.uri) / \"tensor.pb\"\n        return tf.io.parse_tensor(\n            tf.io.read_file(str(tensor_path)), out_type=tf.float32\n        )\n    except Exception as e:\n        logger.error(f\"Failed to load tensor: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorMaterializer.save","title":"save","text":"<pre><code>save(data: tf.Tensor) -&gt; None\n</code></pre> <p>Save a TensorFlow Tensor to the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>tf.Tensor</code> <p>The tensor to save.</p> required Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.Tensor) -&gt; None:\n    \"\"\"Save a TensorFlow Tensor to the artifact store.\n\n    Args:\n        data: The tensor to save.\n    \"\"\"\n    logger.info(\"Saving TensorFlow tensor...\")\n    try:\n        Path(self.uri).mkdir(parents=True, exist_ok=True)\n        tensor_path = Path(self.uri) / \"tensor.pb\"\n        tf.io.write_file(str(tensor_path), tf.io.serialize_tensor(data))\n        logger.info(\"\u2705 Successfully saved TensorFlow tensor\")\n    except Exception as e:\n        logger.error(f\"Failed to save tensor: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer","title":"TensorSpecMaterializer","text":"<p>         Bases: <code>BaseMaterializer</code></p> <p>Materializer for TensorFlow TensorSpec objects.</p> <p>This materializer handles the serialization and deserialization of <code>tf.TensorSpec</code> objects. It saves the spec as a JSON file (<code>spec.json</code>) containing shape, dtype, and other metadata.</p>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer-functions","title":"Functions","text":""},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer.load","title":"load","text":"<pre><code>load(data_type: type[Any]) -&gt; tf.TensorSpec\n</code></pre> <p>Load a TensorFlow TensorSpec from the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type[Any]</code> <p>The type of the data to load (should be <code>tf.TensorSpec</code>).</p> required <p>Returns:</p> Type Description <code>tf.TensorSpec</code> <p>tf.TensorSpec: The loaded tensor spec.</p> Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def load(self, data_type: type[Any]) -&gt; tf.TensorSpec:  # noqa: ARG002\n    \"\"\"Load a TensorFlow TensorSpec from the artifact store.\n\n    Args:\n        data_type: The type of the data to load (should be `tf.TensorSpec`).\n\n    Returns:\n        tf.TensorSpec: The loaded tensor spec.\n    \"\"\"\n    logger.info(\"Loading TensorFlow TensorSpec...\")\n    try:\n        spec_path = Path(self.uri) / \"spec.json\"\n        with open(spec_path) as f:\n            spec_dict = json.load(f)\n        # Reconstruct TensorSpec from dict representation\n        return tf.TensorSpec.from_spec(spec_dict)\n    except Exception as e:\n        logger.error(f\"Failed to load TensorSpec: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.tensorflow.materializers.TensorSpecMaterializer.save","title":"save","text":"<pre><code>save(data: tf.TensorSpec) -&gt; None\n</code></pre> <p>Save a TensorFlow TensorSpec to the artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>tf.TensorSpec</code> <p>The tensor spec to save.</p> required Source code in <code>mlpotion/integrations/zenml/tensorflow/materializers.py</code> <pre><code>def save(self, data: tf.TensorSpec) -&gt; None:\n    \"\"\"Save a TensorFlow TensorSpec to the artifact store.\n\n    Args:\n        data: The tensor spec to save.\n    \"\"\"\n    logger.info(\"Saving TensorFlow TensorSpec...\")\n    try:\n        Path(self.uri).mkdir(parents=True, exist_ok=True)\n        spec_path = Path(self.uri) / \"spec.json\"\n        # Convert TensorSpec to serializable dict format\n        spec_dict = {\n            \"shape\": list(data.shape),\n            \"dtype\": str(data.dtype),\n        }\n        with open(spec_path, \"w\") as f:\n            json.dump(spec_dict, f, indent=2)\n        logger.info(\"\u2705 Successfully saved TensorFlow TensorSpec\")\n    except Exception as e:\n        logger.error(f\"Failed to save TensorSpec: {e}\")\n        raise\n</code></pre>"},{"location":"api/integrations/zenml.html#mlpotion.integrations.zenml.pytorch.materializers","title":"mlpotion.integrations.zenml.pytorch.materializers","text":""},{"location":"contributing/overview.html","title":"Contributing to MLPotion \ud83e\udd1d","text":"<p>Thank you for your interest in contributing to MLPotion! We love community contributions and want to make it as easy as possible.</p>"},{"location":"contributing/overview.html#ways-to-contribute","title":"Ways to Contribute \ud83c\udf1f","text":""},{"location":"contributing/overview.html#1-report-bugs","title":"1. Report Bugs \ud83d\udc1b","text":"<p>Found a bug? Please open an issue with:</p> <ul> <li>Clear description of the bug</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (Python version, OS, framework versions)</li> <li>Minimal reproducible example</li> </ul>"},{"location":"contributing/overview.html#2-suggest-features","title":"2. Suggest Features \ud83d\udca1","text":"<p>Have an idea? Start a discussion or open an issue with:</p> <ul> <li>Clear description of the feature</li> <li>Use case and motivation</li> <li>Example API (if applicable)</li> </ul>"},{"location":"contributing/overview.html#3-submit-pull-requests","title":"3. Submit Pull Requests \ud83d\udd28","text":"<p>We welcome code contributions! See below for guidelines.</p>"},{"location":"contributing/overview.html#4-improve-documentation","title":"4. Improve Documentation \ud83d\udcd6","text":"<p>Documentation improvements are always appreciated:</p> <ul> <li>Fix typos or unclear explanations</li> <li>Add examples</li> <li>Improve API documentation</li> <li>Write tutorials</li> </ul>"},{"location":"contributing/overview.html#5-help-others","title":"5. Help Others \ud83d\ude4b","text":"<ul> <li>Answer questions in Discussions</li> <li>Help review pull requests</li> <li>Share your MLPotion projects</li> </ul>"},{"location":"contributing/overview.html#development-setup","title":"Development Setup \ud83d\udee0\ufe0f","text":""},{"location":"contributing/overview.html#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/MLPotion.git\ncd MLPotion\n</code></pre>"},{"location":"contributing/overview.html#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\n</code></pre>"},{"location":"contributing/overview.html#3-install-development-dependencies","title":"3. Install Development Dependencies","text":"<pre><code># Install dependencies with Poetry (includes dev dependencies and all extras)\npoetry install --with dev -E all\n\n# Activate the virtual environment\npoetry shell\n</code></pre>"},{"location":"contributing/overview.html#4-install-pre-commit-hooks","title":"4. Install Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/overview.html#code-style","title":"Code Style \ud83c\udfa8","text":"<p>We use:</p> <ul> <li>Black for code formatting</li> <li>isort for import sorting</li> <li>Ruff for linting</li> <li>mypy for type checking</li> </ul> <p>Run before committing:</p> <pre><code># Format code\nblack mlpotion tests\n\n# Sort imports\nisort mlpotion tests\n\n# Lint\nruff check mlpotion tests\n\n# Type check\nmypy mlpotion\n</code></pre> <p>Or just let pre-commit handle it:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"contributing/overview.html#testing","title":"Testing \ud83e\uddea","text":""},{"location":"contributing/overview.html#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/overview.html#run-specific-framework-tests","title":"Run Specific Framework Tests","text":"<pre><code># TensorFlow only\npytest -m tensorflow\n\n# PyTorch only\npytest -m pytorch\n\n# Keras only\npytest -m keras\n</code></pre>"},{"location":"contributing/overview.html#run-with-coverage","title":"Run with Coverage","text":"<pre><code>pytest --cov=mlpotion --cov-report=html\n</code></pre>"},{"location":"contributing/overview.html#write-tests","title":"Write Tests","text":"<p>All new features need tests! Place tests in <code>tests/</code> with the same structure as <code>mlpotion/</code>.</p> <p>Example:</p> <pre><code># tests/frameworks/tensorflow/test_loaders.py\nimport pytest\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader\n\ndef test_csv_loader_basic():\n    \"\"\"Test basic CSV loading.\"\"\"\n    loader = TFCSVDataLoader(\"test_data.csv\", label_name=\"target\")\n    dataset = loader.load()\n\n    assert dataset is not None\n    # More assertions...\n</code></pre>"},{"location":"contributing/overview.html#pull-request-process","title":"Pull Request Process \ud83d\udd04","text":""},{"location":"contributing/overview.html#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributing/overview.html#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write clean, documented code</li> <li>Follow existing patterns</li> <li>Add tests for new features</li> <li>Update documentation</li> </ul>"},{"location":"contributing/overview.html#3-commit-changes","title":"3. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add awesome feature\"\n</code></pre> <p>Commit message format:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation</li> <li><code>test:</code> - Tests</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>style:</code> - Code style changes</li> <li><code>chore:</code> - Maintenance</li> </ul>"},{"location":"contributing/overview.html#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with:</p> <ul> <li>Clear description of changes</li> <li>Link to related issue (if any)</li> <li>Screenshots (if UI changes)</li> <li>Checklist of completed items</li> </ul>"},{"location":"contributing/overview.html#project-structure","title":"Project Structure \ud83d\udcc1","text":"<pre><code>mlpotion/\n\u251c\u2500\u2500 core/                    # Framework-agnostic core\n\u2502   \u251c\u2500\u2500 protocols.py        # Protocol definitions\n\u2502   \u251c\u2500\u2500 results.py          # Result types\n\u2502   \u251c\u2500\u2500 config.py           # Configuration classes\n\u2502   \u2514\u2500\u2500 exceptions.py       # Custom exceptions\n\u251c\u2500\u2500 frameworks/              # Framework implementations\n\u2502   \u251c\u2500\u2500 tensorflow/         # TensorFlow components\n\u2502   \u251c\u2500\u2500 pytorch/            # PyTorch components\n\u2502   \u2514\u2500\u2500 keras/              # Keras components\n\u251c\u2500\u2500 integrations/            # Third-party integrations\n\u2502   \u2514\u2500\u2500 zenml/              # ZenML integration\n\u2514\u2500\u2500 utils/                   # Utility functions\n</code></pre>"},{"location":"contributing/overview.html#adding-a-new-component","title":"Adding a New Component \ud83c\udd95","text":""},{"location":"contributing/overview.html#1-define-protocol-if-new","title":"1. Define Protocol (if new)","text":"<pre><code># mlpotion/core/protocols.py\nfrom typing import Protocol, TypeVar\n\nDatasetT = TypeVar(\"DatasetT\")\n\nclass NewComponent(Protocol[DatasetT]):\n    \"\"\"Protocol for new component type.\"\"\"\n\n    def do_something(self, dataset: DatasetT) -&gt; DatasetT:\n        \"\"\"Do something with dataset.\"\"\"\n        ...\n</code></pre>"},{"location":"contributing/overview.html#2-implement-for-each-framework","title":"2. Implement for Each Framework","text":"<pre><code># mlpotion/frameworks/tensorflow/new_module.py\nimport tensorflow as tf\n\nclass TFNewComponent:\n    \"\"\"TensorFlow implementation.\"\"\"\n\n    def do_something(self, dataset: tf.data.Dataset) -&gt; tf.data.Dataset:\n        \"\"\"Implementation for TensorFlow.\"\"\"\n        # Your code here\n        return dataset\n</code></pre>"},{"location":"contributing/overview.html#3-add-tests","title":"3. Add Tests","text":"<pre><code># tests/frameworks/tensorflow/test_new_module.py\ndef test_new_component():\n    \"\"\"Test new component.\"\"\"\n    component = TFNewComponent()\n    result = component.do_something(dataset)\n    assert result is not None\n</code></pre>"},{"location":"contributing/overview.html#4-document","title":"4. Document","text":"<pre><code># Add docstrings\nclass TFNewComponent:\n    \"\"\"TensorFlow implementation of NewComponent.\n\n    This component does X, Y, and Z.\n\n    Args:\n        param1: Description\n        param2: Description\n\n    Example:\n        ```python\n        component = TFNewComponent(param1=\"value\")\n        result = component.do_something(dataset)\n        ```\n    \"\"\"\n</code></pre>"},{"location":"contributing/overview.html#code-review-process","title":"Code Review Process \ud83d\udc40","text":"<ol> <li>Automated Checks: CI must pass</li> <li>Code Review: At least one maintainer approval</li> <li>Documentation: Must be updated if needed</li> <li>Tests: Must pass and have good coverage</li> <li>Breaking Changes: Require discussion</li> </ol>"},{"location":"contributing/overview.html#release-process","title":"Release Process \ud83d\ude80","text":"<ol> <li>Version bump in <code>pyproject.toml</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create release tag</li> <li>GitHub Actions publishes to PyPI</li> </ol>"},{"location":"contributing/overview.html#questions","title":"Questions? \ud83d\udcac","text":"<ul> <li>GitHub Discussions</li> <li>Issues</li> <li>Email: piotr@unicolab.ai</li> </ul>"},{"location":"contributing/overview.html#code-of-conduct","title":"Code of Conduct \ud83d\udcdc","text":"<p>Be respectful, inclusive, and collaborative. We're all here to make ML better!</p> <p> Thank you for contributing to MLPotion! \ud83d\ude4f Built with \u2764\ufe0f by the community for the community </p>"},{"location":"examples/index.html","title":"MLPotion Examples","text":"<p>This directory contains practical examples demonstrating how to use MLPotion with different ML frameworks, both standalone and integrated with ZenML pipelines.</p>"},{"location":"examples/index.html#directory-structure","title":"Directory Structure","text":"<pre><code>examples/\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 sample.csv          # Sample dataset for all examples\n\u251c\u2500\u2500 keras/\n\u2502   \u251c\u2500\u2500 basic_usage.py      # Keras standalone example\n\u2502   \u2514\u2500\u2500 zenml_pipeline.py   # Keras with ZenML orchestration\n\u251c\u2500\u2500 pytorch/\n\u2502   \u251c\u2500\u2500 basic_usage.py      # PyTorch standalone example\n\u2502   \u2514\u2500\u2500 zenml_pipeline.py   # PyTorch with ZenML orchestration\n\u251c\u2500\u2500 tensorflow/\n\u2502   \u251c\u2500\u2500 basic_usage.py      # TensorFlow standalone example\n\u2502   \u2514\u2500\u2500 zenml_pipeline.py   # TensorFlow with ZenML orchestration\n\u2514\u2500\u2500 standalone/\n    \u2514\u2500\u2500 ...                 # Framework-agnostic examples\n</code></pre>"},{"location":"examples/index.html#quick-start","title":"Quick Start","text":""},{"location":"examples/index.html#1-standalone-examples-without-zenml","title":"1. Standalone Examples (Without ZenML)","text":"<p>These examples demonstrate the core MLPotion workflow without any orchestration framework:</p>"},{"location":"examples/index.html#keras","title":"Keras","text":"<pre><code>python examples/keras/basic_usage.py\n</code></pre> <p>Features demonstrated: - Load data from CSV - Create and compile a Keras model - Train the model - Evaluate performance - Save and load models</p>"},{"location":"examples/index.html#pytorch","title":"PyTorch","text":"<pre><code>python examples/pytorch/basic_usage.py\n</code></pre> <p>Features demonstrated: - Load data from CSV using PyTorchCSVDataset - Create DataLoaders with PyTorchDataLoaderFactory - Define and train a PyTorch model - Evaluate model performance - Save and load model state</p>"},{"location":"examples/index.html#tensorflow","title":"TensorFlow","text":"<pre><code>python examples/tensorflow/basic_usage.py\n</code></pre> <p>Features demonstrated: - Load data from CSV with TFCSVDataLoader - Optimize datasets for performance - Build and compile TensorFlow models - Train and evaluate models - Save and export for serving</p>"},{"location":"examples/index.html#2-zenml-pipeline-examples","title":"2. ZenML Pipeline Examples","text":"<p>These examples show how to orchestrate MLPotion components in reproducible ZenML pipelines:</p>"},{"location":"examples/index.html#prerequisites","title":"Prerequisites","text":"<pre><code># Install ZenML\npip install zenml\n\n# Initialize ZenML (first time only)\nzenml init\n\n# For testing without full stack setup\nexport ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true\n</code></pre>"},{"location":"examples/index.html#keras-pipeline","title":"Keras Pipeline","text":"<pre><code>python examples/keras/zenml_pipeline.py\n</code></pre> <p>Pipeline steps: 1. Load data 2. Create model 3. Create training config 4. Train model 5. Evaluate model 6. Save model 7. Export for serving</p>"},{"location":"examples/index.html#pytorch-pipeline","title":"PyTorch Pipeline","text":"<pre><code>python examples/pytorch/zenml_pipeline.py\n</code></pre> <p>Pipeline steps: 1. Load CSV data 2. Create PyTorch model 3. Create training config 4. Train model 5. Evaluate model 6. Save model (state_dict) 7. Export as TorchScript</p>"},{"location":"examples/index.html#tensorflow-pipeline","title":"TensorFlow Pipeline","text":"<pre><code>python examples/tensorflow/zenml_pipeline.py\n</code></pre> <p>Pipeline steps: 1. Load data from CSV 2. Optimize dataset 3. Create TensorFlow model 4. Create training config 5. Train model 6. Evaluate model 7. Save model 8. Export as SavedModel</p>"},{"location":"examples/index.html#common-patterns","title":"Common Patterns","text":""},{"location":"examples/index.html#data-loading","title":"Data Loading","text":"<p>Keras: <pre><code>from mlpotion.frameworks.keras import KerasCSVDataLoader\n\nloader = KerasCSVDataLoader(\n    file_pattern=\"examples/data/sample.csv\",\n    label_name=\"target\",\n    batch_size=8,\n    shuffle=True,\n)\ndataset = loader.load()\n</code></pre></p> <p>PyTorch: <pre><code>from mlpotion.frameworks.pytorch import PyTorchCSVDataset, PyTorchDataLoaderFactory\n\ndataset = PyTorchCSVDataset(\n    file_pattern=\"examples/data/sample.csv\",\n    label_name=\"target\",\n)\nfactory = PyTorchDataLoaderFactory(batch_size=8, shuffle=True)\ndataloader = factory.load(dataset)\n</code></pre></p> <p>TensorFlow: <pre><code>from mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFDatasetOptimizer\n\nloader = TFCSVDataLoader(\n    file_pattern=\"examples/data/sample.csv\",\n    label_name=\"target\",\n)\ndataset = loader.load()\n\noptimizer = TFDatasetOptimizer(batch_size=8, shuffle_buffer_size=100)\ndataset = optimizer.optimize(dataset)\n</code></pre></p>"},{"location":"examples/index.html#training","title":"Training","text":"<p>All frameworks use a similar training pattern:</p> <pre><code>from mlpotion.frameworks.[framework] import [Framework]ModelTrainer, [Framework]TrainingConfig\n\ntrainer = [Framework]ModelTrainer()\nconfig = [Framework]TrainingConfig(\n    epochs=10,\n    batch_size=8,\n    learning_rate=0.001,\n    verbose=1,\n)\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"examples/index.html#evaluation","title":"Evaluation","text":"<pre><code>from mlpotion.frameworks.[framework] import [Framework]ModelEvaluator\n\nevaluator = [Framework]ModelEvaluator()\neval_result = evaluator.evaluate(model, dataset, config)\nprint(eval_result.metrics)\n</code></pre>"},{"location":"examples/index.html#model-persistence","title":"Model Persistence","text":"<pre><code>from mlpotion.frameworks.[framework] import [Framework]ModelPersistence\n\npersistence = [Framework]ModelPersistence()\n\n# Save\npersistence.save(model, \"/path/to/model\", save_format=\"...\")\n\n# Load\nloaded_model = persistence.load(\"/path/to/model\")\n</code></pre>"},{"location":"examples/index.html#sample-data","title":"Sample Data","text":"<p>The <code>examples/data/sample.csv</code> file contains synthetic regression data with: - 10 features (feature_0 through feature_9) - 1 target variable - 50 samples</p> <p>This dataset is used across all examples for consistency.</p>"},{"location":"examples/index.html#zenml-integration-benefits","title":"ZenML Integration Benefits","text":"<p>Using ZenML pipelines provides:</p> <ol> <li>Reproducibility: Track all pipeline runs with versioned artifacts</li> <li>Experiment Tracking: Compare different configurations and results</li> <li>Collaboration: Share pipelines with team members</li> <li>Scalability: Run pipelines on different compute backends</li> <li>Artifact Caching: Skip unchanged steps in subsequent runs</li> </ol>"},{"location":"examples/index.html#customization","title":"Customization","text":"<p>Each example is designed to be easily customizable:</p> <ol> <li>Change model architecture: Modify the model creation code</li> <li>Adjust hyperparameters: Update the training configuration</li> <li>Use your own data: Replace the file path with your CSV file</li> <li>Add validation split: Set <code>validation_split</code> in training config</li> <li>Enable early stopping: Configure callbacks in training config</li> </ol>"},{"location":"examples/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/index.html#import-errors","title":"Import Errors","text":"<p>Make sure MLPotion is installed: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"examples/index.html#zenml-errors","title":"ZenML Errors","text":"<p>If you encounter ZenML initialization errors: <pre><code>export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true\n</code></pre></p>"},{"location":"examples/index.html#data-loading-issues","title":"Data Loading Issues","text":"<p>Ensure the CSV file exists and has the correct format: <pre><code>ls -la examples/data/sample.csv\nhead examples/data/sample.csv\n</code></pre></p>"},{"location":"examples/index.html#next-steps","title":"Next Steps","text":"<ul> <li>Explore the MLPotion documentation</li> <li>Check out the test suite for more usage examples</li> <li>Read about ZenML best practices</li> <li>Customize the examples for your own use cases</li> </ul>"},{"location":"examples/index.html#contributing","title":"Contributing","text":"<p>Found an issue or want to add a new example? Please open an issue or PR!</p>"},{"location":"frameworks/keras.html","title":"Keras Guide \ud83c\udfa8","text":"<p>Complete guide to using MLPotion with Keras 3 - the user-friendly, backend-agnostic ML framework!</p>"},{"location":"frameworks/keras.html#why-keras-mlpotion","title":"Why Keras + MLPotion? \ud83e\udd14","text":"<ul> <li>User-Friendly: Simple, consistent API</li> <li>Backend-Agnostic: Switch between TensorFlow, PyTorch, and JAX</li> <li>Fast Prototyping: Build models quickly</li> <li>Production-Ready: Keras 3 is production-grade</li> <li>MLPotion Benefits: Type-safe, modular components</li> </ul>"},{"location":"frameworks/keras.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code># Keras with TensorFlow backend (default)\npoetry add mlpotion -E tensorflow\n\n# Keras with PyTorch backend\npoetry add mlpotion -E keras-pytorch\n\n# Keras with JAX backend\npoetry add mlpotion -E keras-jax\n</code></pre>"},{"location":"frameworks/keras.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport keras\n\n# Load data\nloader = CSVDataLoader(\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()\n\n# Create model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\nresult = trainer.train(model, dataset, config)\n\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/keras.html#advanced-training","title":"Advanced Training \ud83c\udf93","text":""},{"location":"frameworks/keras.html#custom-optimizers-loss-and-metrics","title":"Custom Optimizers, Loss, and Metrics","text":"<pre><code>import keras\n\n# Custom optimizer instance\ncustom_optimizer = keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    clipnorm=1.0,\n)\n\n# Custom loss instance\ncustom_loss = keras.losses.Huber(delta=1.0)\n\n# Custom metrics\ncustom_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n\nconfig = ModelTrainingConfig(\n    epochs=50,\n    batch_size=32,\n    optimizer=custom_optimizer,  # Optimizer instance\n    loss=custom_loss,            # Loss instance\n    metrics=[custom_metric, \"mse\"],  # Mix of instances and strings\n)\n\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/keras.html#callbacks-and-tensorboard","title":"Callbacks and TensorBoard","text":"<pre><code># Method 1: Pass callback instances\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True,\n)\n\ncsv_logger = keras.callbacks.CSVLogger(\"training.log\")\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[early_stopping, csv_logger],\n    use_tensorboard=True,  # Enabled by default\n    tensorboard_log_dir=\"logs/keras_experiment\",\n)\n\n# Method 2: Pass callback configs as dicts\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[\n        {\n            \"name\": \"EarlyStopping\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"patience\": 5,\n                \"restore_best_weights\": True,\n            }\n        },\n        {\n            \"name\": \"ReduceLROnPlateau\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"factor\": 0.5,\n                \"patience\": 3,\n            }\n        },\n    ],\n)\n\nresult = trainer.train(model, dataset, config, validation_dataset=val_dataset)\n\n# View TensorBoard\n# tensorboard --logdir=logs/keras_experiment\n</code></pre>"},{"location":"frameworks/keras.html#complete-example","title":"Complete Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport keras\n\n# Load data\nloader = CSVDataLoader(\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()\n\n# Create model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\n# Configure training with all features\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    batch_size=32,\n\n    # Custom optimizer\n    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n\n    # Loss and metrics\n    loss=\"mse\",\n    metrics=[\"mae\", keras.metrics.RootMeanSquaredError()],\n\n    # Callbacks\n    callbacks=[\n        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n        {\"name\": \"ReduceLROnPlateau\", \"params\": {\"factor\": 0.5, \"patience\": 5}},\n    ],\n\n    # TensorBoard\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/my_experiment\",\n    tensorboard_params={\n        \"histogram_freq\": 1,\n        \"write_graph\": True,\n    },\n)\n\n# Train\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"Training completed in {result.training_time:.2f}s\")\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre> <p>For complete Keras documentation, see the TensorFlow Guide as Keras components use the same patterns!</p> <p> Keras + MLPotion = Simplicity + Power! \ud83c\udfa8 </p>"},{"location":"frameworks/pytorch.html","title":"PyTorch Guide \ud83d\udd25","text":"<p>Complete guide to using MLPotion with PyTorch - the researcher's favorite framework!</p>"},{"location":"frameworks/pytorch.html#why-pytorch-mlpotion","title":"Why PyTorch + MLPotion? \ud83e\udd14","text":"<ul> <li>Research-Friendly: Dynamic computation graphs, easy debugging</li> <li>Pythonic: Feels like native Python code</li> <li>Flexible: Full control over training loops</li> <li>Ecosystem: Huge community, extensive libraries</li> <li>MLPotion Benefits: Type-safe, modular components with consistent APIs</li> </ul>"},{"location":"frameworks/pytorch.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code>poetry add mlpotion -E pytorch\n</code></pre> <p>This installs: - <code>torch&gt;=2.0</code> - <code>torchvision&gt;=0.16</code> - All PyTorch-specific MLPotion components</p>"},{"location":"frameworks/pytorch.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.pytorch import (\n    CSVDataset,\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport torch.nn as nn\n\n# Load data\ndataset = CSVDataset(\"data.csv\", label_name=\"target\")\nfactory = CSVDataLoader(batch_size=32, shuffle=True)\ndataloader = factory.load(dataset)\n\n# Create model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(10, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nmodel = SimpleModel()\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\nresult = trainer.train(model, dataloader, config)\n\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#data-loading","title":"Data Loading \ud83d\udcca","text":""},{"location":"frameworks/pytorch.html#csv-dataset","title":"CSV Dataset","text":"<pre><code>from mlpotion.frameworks.pytorch import CSVDataset\n\ndataset = CSVDataset(\n    file_pattern=\"data.csv\",        # File path or pattern\n    label_name=\"target\",            # Label column name\n    column_names=None,              # Auto-detect or specify\n    dtype=torch.float32,            # Data type\n)\n\n# Use like any PyTorch dataset\nprint(f\"Dataset size: {len(dataset)}\")\nfeatures, label = dataset[0]\n</code></pre>"},{"location":"frameworks/pytorch.html#dataloader-factory","title":"DataLoader Factory","text":"<pre><code>from mlpotion.frameworks.pytorch import CSVDataLoader\n\nfactory = CSVDataLoader(\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,                  # Parallel data loading\n    pin_memory=True,                # Faster GPU transfer\n    drop_last=False,\n    persistent_workers=True,        # Keep workers alive\n)\n\n# Create dataloaders\ntrain_loader = factory.load(train_dataset)\nval_loader = factory.load(val_dataset)\ntest_loader = factory.load(test_dataset)\n</code></pre>"},{"location":"frameworks/pytorch.html#model-training","title":"Model Training \ud83c\udf93","text":""},{"location":"frameworks/pytorch.html#basic-training","title":"Basic Training","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelTrainer, ModelTrainingConfig\n\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    device=\"cuda\",\n    optimizer=\"adam\",\n    loss_fn=\"mse\",\n    verbose=True,\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, train_loader, config)\n\nprint(f\"Training time: {result.training_time:.2f}s\")\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#advanced-training-configuration","title":"Advanced Training Configuration","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Using string optimizer name\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    optimizer=\"adamw\",  # String name\n    loss_fn=\"mse\",\n    verbose=True,\n)\n\n# Using custom optimizer instance\ncustom_optimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    weight_decay=0.01,\n    amsgrad=True,\n)\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    device=\"cuda\",\n    optimizer=custom_optimizer,  # Pass optimizer instance\n    loss_fn=\"mse\",\n)\n\n# Using custom loss function\ncustom_loss = nn.SmoothL1Loss(beta=1.0)\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=custom_loss,  # Custom loss instance\n)\n\nresult = trainer.train(model, train_loader, config, validation_dataloader=val_loader)\n</code></pre>"},{"location":"frameworks/pytorch.html#callbacks-and-tensorboard","title":"Callbacks and TensorBoard","text":"<pre><code># Custom callback class\nclass TrainingCallback:\n    def on_train_begin(self):\n        print(\"\ud83d\ude80 Training started!\")\n\n    def on_epoch_end(self, epoch, metrics):\n        print(f\"Epoch {epoch + 1} completed: {metrics}\")\n        # Add custom logic (e.g., save checkpoint, adjust LR)\n\n    def on_train_end(self):\n        print(\"\u2705 Training completed!\")\n\n# Early stopping callback example\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.should_stop = False\n\n    def on_epoch_end(self, epoch, metrics):\n        val_loss = metrics.get('val_loss')\n        if val_loss is None:\n            return\n\n        if val_loss &lt; self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n                self.should_stop = True\n\n# Configure with callbacks and TensorBoard\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=\"mse\",\n\n    # Add callbacks\n    callbacks=[\n        TrainingCallback(),\n        EarlyStopping(patience=10),\n    ],\n\n    # Enable TensorBoard\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/pytorch_experiment\",\n    tensorboard_params={\n        \"comment\": \"My experiment\",\n        \"flush_secs\": 30,\n    },\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, train_loader, config, validation_dataloader=val_loader)\n\n# View TensorBoard logs\n# tensorboard --logdir=logs/pytorch_experiment\n</code></pre>"},{"location":"frameworks/pytorch.html#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code># Method 1: Use nn.Module\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=0.5):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, predictions, targets):\n        mse = nn.functional.mse_loss(predictions, targets)\n        mae = nn.functional.l1_loss(predictions, targets)\n        return self.alpha * mse + (1 - self.alpha) * mae\n\n# Method 2: Use callable function\ndef custom_loss_fn(predictions, targets):\n    return torch.mean((predictions - targets) ** 2) + 0.1 * torch.mean(torch.abs(predictions - targets))\n\n# Use in config\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    optimizer=\"adam\",\n    loss_fn=CustomLoss(alpha=0.7),  # or custom_loss_fn\n)\n</code></pre>"},{"location":"frameworks/pytorch.html#custom-training-loop","title":"Custom Training Loop","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelTrainer\nimport torch\n\nclass CustomTrainer(ModelTrainer):\n    def training_step(self, model, batch, device):\n        \"\"\"Custom training step logic.\"\"\"\n        features, labels = batch\n        features, labels = features.to(device), labels.to(device)\n\n        # Forward pass\n        predictions = model(features)\n        loss = self.criterion(predictions, labels)\n\n        # Add custom regularization\n        l2_reg = sum(p.pow(2).sum() for p in model.parameters())\n        loss = loss + 0.001 * l2_reg\n\n        return loss\n\n    def validation_step(self, model, batch, device):\n        \"\"\"Custom validation step logic.\"\"\"\n        features, labels = batch\n        features, labels = features.to(device), labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(features)\n            loss = self.criterion(predictions, labels)\n\n        return loss\n\n# Use custom trainer\ncustom_trainer = CustomTrainer()\nresult = custom_trainer.train(model, train_loader, config)\n</code></pre>"},{"location":"frameworks/pytorch.html#model-evaluation","title":"Model Evaluation \ud83d\udcca","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelEvaluator, ModelEvaluationConfig\n\nconfig = ModelEvaluationConfig(\n    device=\"cuda\",\n    batch_size=32,\n    metrics=[\"mse\", \"mae\"],\n    verbose=True,\n)\n\nevaluator = ModelEvaluator()\nresult = evaluator.evaluate(model, test_loader, config)\n\nprint(f\"Test loss: {result.metrics['loss']:.4f}\")\nprint(f\"Test MAE: {result.metrics['mae']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#model-persistence","title":"Model Persistence \ud83d\udcbe","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelPersistence\n\npersistence = ModelPersistence(path=\"models/my_model.pth\", model=model)\n\n# Save model (state_dict - recommended)\npersistence.save()\n\n# Save full model\npersistence.save(save_full_model=True)\n\n# Load model\nloader = ModelPersistence(path=\"models/my_model.pth\")\nloaded_model, metadata = loader.load(\n    model_class=SimpleModel,  # Need model class for state_dict\n)\n\n# Load full model (auto-detected)\nloader_full = ModelPersistence(path=\"models/my_model_full.pth\")\nloaded_model_full, _ = loader_full.load()\n</code></pre>"},{"location":"frameworks/pytorch.html#model-export","title":"Model Export \ud83d\udce4","text":""},{"location":"frameworks/pytorch.html#torchscript","title":"TorchScript","text":"<pre><code>from mlpotion.frameworks.pytorch import ModelExporter, ModelExportConfig\n\nexporter = ModelExporter()\n\nconfig = ModelExportConfig(\n    format=\"torchscript\",\n    method=\"trace\",  # or \"script\"\n    example_inputs=torch.randn(1, 10),  # For tracing\n)\n\nresult = exporter.export(model, \"exports/model.pt\", config)\nprint(f\"Exported to: {result.export_path}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#onnx","title":"ONNX","text":"<pre><code>config = ModelExportConfig(\n    format=\"onnx\",\n    input_names=[\"features\"],\n    output_names=[\"predictions\"],\n    dynamic_axes={\"features\": {0: \"batch_size\"}},\n    opset_version=14,\n)\n\nresult = exporter.export(model, \"exports/model.onnx\", config)\n</code></pre>"},{"location":"frameworks/pytorch.html#common-patterns","title":"Common Patterns \ud83c\udfaf","text":""},{"location":"frameworks/pytorch.html#pattern-train-val-test-pipeline","title":"Pattern: Train-Val-Test Pipeline","text":"<pre><code># Load data\ntrain_dataset = CSVDataset(\"train.csv\", label_name=\"target\")\nval_dataset = CSVDataset(\"val.csv\", label_name=\"target\")\ntest_dataset = CSVDataset(\"test.csv\", label_name=\"target\")\n\n# Create dataloaders\nfactory = CSVDataLoader(batch_size=32, shuffle=True, num_workers=4)\ntrain_loader = factory.load(train_dataset)\nval_loader = factory.load(val_dataset)\ntest_loader = factory.load(test_dataset)\n\n# Train with validation\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    early_stopping=True,\n    early_stopping_patience=10,\n)\n\nresult = trainer.train(model, train_loader, config, val_loader=val_loader)\n\n# Evaluate on test set\nevaluator = ModelEvaluator()\ntest_metrics = evaluator.evaluate(result.model, test_loader, config)\n\nprint(f\"Best epoch: {result.best_epoch}\")\nprint(f\"Test loss: {test_metrics.metrics['loss']:.4f}\")\n</code></pre>"},{"location":"frameworks/pytorch.html#pattern-multi-gpu-training","title":"Pattern: Multi-GPU Training","text":"<pre><code>import torch\nimport torch.nn as nn\n\n# Wrap model for multi-GPU\nif torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\n    device = \"cuda\"\nelse:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = model.to(device)\n\n# Train as usual\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    device=device,\n)\n\nresult = trainer.train(model, train_loader, config)\n</code></pre>"},{"location":"frameworks/pytorch.html#pattern-mixed-precision-training","title":"Pattern: Mixed Precision Training","text":"<pre><code>config = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    use_amp=True,  # Enable automatic mixed precision\n    device=\"cuda\",\n)\n\nresult = trainer.train(model, train_loader, config)\n</code></pre>"},{"location":"frameworks/pytorch.html#best-practices","title":"Best Practices \ud83d\udca1","text":"<ol> <li>Use DataLoaders: Always use <code>CSVDataLoader</code> for efficient loading</li> <li>Enable num_workers: Set <code>num_workers&gt;0</code> for parallel data loading</li> <li>Pin Memory: Use <code>pin_memory=True</code> for faster GPU transfer</li> <li>AMP Training: Enable mixed precision for faster training</li> <li>Gradient Clipping: Prevent exploding gradients with <code>clip_grad_norm</code></li> <li>State Dict: Save models as state_dict for better compatibility</li> </ol>"},{"location":"frameworks/pytorch.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>TensorFlow Guide \u2192 - Compare with TensorFlow</li> <li>ZenML Integration \u2192 - Add MLOps</li> <li>API Reference \u2192 - Detailed API docs</li> </ul> <p> PyTorch + MLPotion = Research + Production! \ud83d\udd25 </p>"},{"location":"frameworks/tensorflow.html","title":"TensorFlow Guide \ud83d\udd36","text":"<p>Complete guide to using MLPotion with TensorFlow - the production-ready ML framework.</p>"},{"location":"frameworks/tensorflow.html#why-tensorflow-mlpotion","title":"Why TensorFlow + MLPotion? \ud83e\udd14","text":"<ul> <li>Production Ready: Industry-standard deployment</li> <li>Ecosystem: Rich tooling (TensorBoard, TFX, TFLite)</li> <li>Performance: Optimized for TPUs and GPUs</li> <li>Scalability: Distributed training built-in</li> <li>MLPotion Benefits: Type-safe, modular components</li> </ul>"},{"location":"frameworks/tensorflow.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code>poetry add mlpotion -E tensorflow\n</code></pre> <p>This installs: - <code>tensorflow&gt;=2.15</code> - <code>keras&gt;=3.0</code> - All TensorFlow-specific MLPotion components</p>"},{"location":"frameworks/tensorflow.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\nimport tensorflow as tf\n\n# Load data\nloader = CSVDataLoader(\"data.csv\", label_name=\"target\")\ndataset = loader.load()\n\n# Create model\nmodel = tf.keras.Sequential([...])\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(epochs=10, learning_rate=0.001)\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#data-loading","title":"Data Loading \ud83d\udcca","text":""},{"location":"frameworks/tensorflow.html#csv-data-loader","title":"CSV Data Loader","text":"<pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\nloader = CSVDataLoader(\n    file_pattern=\"data/*.csv\",      # Supports glob patterns\n    label_name=\"target\",            # Column to use as label\n    column_names=None,              # Auto-detect or specify list[str]\n    batch_size=32,\n    # Extra options passed to tf.data.experimental.make_csv_dataset\n    config={\n        \"shuffle\": True,\n        \"shuffle_buffer_size\": 10000,\n        \"num_parallel_reads\": 4,\n        \"compression_type\": None,\n        \"header\": True,\n        \"field_delim\": \",\",\n        \"use_quote_delim\": True,\n        \"na_value\": \"\",\n        \"select_columns\": None,\n    }\n)\n\ndataset = loader.load()  # Returns tf.data.Dataset\n</code></pre> <p>Features: - Automatic type inference - Parallel reading for performance - Memory-efficient streaming - Glob pattern support for multiple files</p>"},{"location":"frameworks/tensorflow.html#dataset-optimization","title":"Dataset Optimization","text":"<pre><code>from mlpotion.frameworks.tensorflow import DatasetOptimizer\n\noptimizer = DatasetOptimizer(\n    batch_size=32,\n    shuffle_buffer_size=10000,      # Set to enable shuffling\n    cache=True,                     # Cache in memory\n    prefetch=True,                  # Prefetch for performance\n)\n\noptimized = optimizer.optimize(dataset)\n</code></pre> <p>Performance Tips: - Use <code>cache=True</code> for datasets that fit in memory - Enable <code>prefetch=True</code> to overlap data loading with training - Set <code>shuffle_buffer_size</code> to a large enough value for good randomization</p>"},{"location":"frameworks/tensorflow.html#model-training","title":"Model Training \ud83c\udf93","text":""},{"location":"frameworks/tensorflow.html#basic-training","title":"Basic Training","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelTrainer, ModelTrainingConfig\n\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    batch_size=32,\n    optimizer=\"adam\",  # Can be string or optimizer instance\n    loss=\"mse\",\n    metrics=[\"mae\"],\n    verbose=1,\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, train_dataset, config)\n\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"Training time: {result.training_time:.2f}s\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#advanced-training-configuration","title":"Advanced Training Configuration","text":"<pre><code>import keras\n\n# Using string optimizer name\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    batch_size=32,\n    optimizer=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\n# Using custom optimizer instance\ncustom_optimizer = keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    clipnorm=1.0,\n)\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    batch_size=32,\n    optimizer=custom_optimizer,  # Pass optimizer instance\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\n# Using custom loss and metrics\ncustom_loss = keras.losses.Huber(delta=1.0)\ncustom_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    batch_size=32,\n    optimizer=\"adam\",\n    loss=custom_loss,  # Custom loss instance\n    metrics=[custom_metric, \"mse\"],  # Mix of instances and strings\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#callbacks-and-tensorboard","title":"Callbacks and TensorBoard","text":"<pre><code>import keras\n\n# Method 1: Pass callback instances directly\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True,\n)\n\ncsv_logger = keras.callbacks.CSVLogger(\"training.log\")\n\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[early_stopping, csv_logger],  # Pass instances\n    use_tensorboard=True,  # TensorBoard enabled by default\n    tensorboard_log_dir=\"logs/my_experiment\",  # Optional custom path\n)\n\n# Method 2: Pass callback configs as dicts\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[\n        {\n            \"name\": \"EarlyStopping\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"patience\": 5,\n                \"restore_best_weights\": True,\n            }\n        },\n        {\n            \"name\": \"ReduceLROnPlateau\",\n            \"params\": {\n                \"monitor\": \"val_loss\",\n                \"factor\": 0.5,\n                \"patience\": 3,\n            }\n        },\n    ],\n    use_tensorboard=True,\n)\n\n# Method 3: Mix of instances and dicts\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    callbacks=[\n        early_stopping,  # Instance\n        {\"name\": \"CSVLogger\", \"params\": {\"filename\": \"training.log\"}},  # Dict\n    ],\n)\n\n# Disable TensorBoard if needed\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    use_tensorboard=False,  # Disable TensorBoard\n    callbacks=[early_stopping],\n)\n\n# Custom TensorBoard configuration\nconfig = ModelTrainingConfig(\n    epochs=100,\n    learning_rate=0.001,\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/experiment_1\",\n    tensorboard_params={\n        \"histogram_freq\": 1,\n        \"write_graph\": True,\n        \"write_images\": True,\n        \"update_freq\": \"epoch\",\n        \"profile_batch\": \"10,20\",\n    },\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#distributed-training","title":"Distributed Training","text":"<pre><code>import tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import ModelTrainer, ModelTrainingConfig\n\n# Create distribution strategy\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Create model within strategy scope\n    model = tf.keras.Sequential([...])\n\n    # Train as usual\n    trainer = ModelTrainer()\n    result = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-evaluation","title":"Model Evaluation \ud83d\udcca","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelEvaluator, ModelEvaluationConfig\n\nconfig = ModelEvaluationConfig(\n    batch_size=32,\n    verbose=1,\n    return_dict=True,\n)\n\nevaluator = ModelEvaluator()\nresult = evaluator.evaluate(model, test_dataset, config)\n\nprint(f\"Test loss: {result.metrics['loss']:.4f}\")\nprint(f\"Test MAE: {result.metrics['mae']:.4f}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-persistence","title":"Model Persistence \ud83d\udcbe","text":""},{"location":"frameworks/tensorflow.html#saving-models","title":"Saving Models","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelPersistence\n\n# Save in TensorFlow format (recommended)\npersistence = ModelPersistence(\n    path=\"models/my_model\",\n    model=model,\n)\npersistence.save(save_format=\".keras\")\n\n# Save in H5 format\npersistence_h5 = ModelPersistence(\n    path=\"models/my_model.h5\",\n    model=model,\n)\npersistence_h5.save(save_format=\"h5\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#loading-models","title":"Loading Models","text":"<pre><code># Load full model\npersistence = ModelPersistence(\n    path=\"models/my_model\",\n    model=None,  # Will be loaded\n)\nloaded_model, metadata = persistence.load()\n\n# Load with custom objects\npersistence_custom = ModelPersistence(\n    path=\"models/my_model\",\n    model=None,\n)\nloaded_model, metadata = persistence_custom.load(\n    custom_objects={\"CustomLayer\": CustomLayer}\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-export","title":"Model Export \ud83d\udce4","text":""},{"location":"frameworks/tensorflow.html#savedmodel-format-recommended-for-serving","title":"SavedModel Format (Recommended for Serving)","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelExporter, ModelExportConfig\n\nexporter = ModelExporter()\n\nconfig = ModelExportConfig(\n    format=\"saved_model\",\n    include_optimizer=False,  # Smaller size for inference\n    signatures=None,  # Auto-detect\n)\n\nresult = exporter.export(model, \"exports/saved_model\", config)\nprint(f\"Exported to: {result.export_path}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#tflite-mobileedge-devices","title":"TFLite (Mobile/Edge Devices)","text":"<pre><code>config = ModelExportConfig(\n    format=\"tflite\",\n    optimization=\"default\",  # or \"float16\", \"int8\"\n    representative_dataset=None,  # For int8 quantization\n)\n\nresult = exporter.export(model, \"exports/model.tflite\", config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#tensorflowjs-web-deployment","title":"TensorFlow.js (Web Deployment)","text":"<pre><code>config = ModelExportConfig(\n    format=\"tfjs\",\n    quantization_dtype=\"float16\",  # Smaller model size\n)\n\nresult = exporter.export(model, \"exports/tfjs_model\", config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#model-inspection","title":"Model Inspection \ud83d\udd0d","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelInspector\n\ninspector = ModelInspector()\ninfo = inspector.inspect(model)\n\nprint(f\"Model name: {info.name}\")\nprint(f\"Backend: {info.backend}\")\nprint(f\"Trainable: {info.trainable}\")\nprint(f\"Total params: {info.parameters['total']}\")\nprint(f\"Trainable params: {info.parameters['trainable']}\")\n\n# Inspect inputs\nfor inp in info.inputs:\n    print(f\"Input: {inp['name']}, shape: {inp['shape']}, dtype: {inp['dtype']}\")\n\n# Inspect outputs\nfor out in info.outputs:\n    print(f\"Output: {out['name']}, shape: {out['shape']}, dtype: {out['dtype']}\")\n\n# Inspect layers\nfor layer in info.layers:\n    print(f\"Layer: {layer['name']}, type: {layer['type']}, params: {layer['params']}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#common-patterns","title":"Common Patterns \ud83c\udfaf","text":""},{"location":"frameworks/tensorflow.html#pattern-train-val-test-pipeline","title":"Pattern: Train-Val-Test Pipeline","text":"<pre><code>from mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelTrainingConfig,\n)\n\n# Load splits\ntrain_loader = CSVDataLoader(\"train.csv\", label_name=\"target\")\nval_loader = CSVDataLoader(\"val.csv\", label_name=\"target\")\ntest_loader = CSVDataLoader(\"test.csv\", label_name=\"target\")\n\ntrain_data = train_loader.load()\nval_data = val_loader.load()\ntest_data = test_loader.load()\n\n# Optimize\noptimizer = DatasetOptimizer(batch_size=32, cache=True, prefetch=True)\ntrain_data = optimizer.optimize(train_data)\nval_data = optimizer.optimize(val_data)\ntest_data = optimizer.optimize(test_data)\n\n# Train\ntrainer = ModelTrainer()\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n)\n\nresult = trainer.train(model, train_data, config, validation_dataset=val_data)\n\n# Evaluate\nevaluator = ModelEvaluator()\ntest_metrics = evaluator.evaluate(result.model, test_data, config)\n\nprint(f\"Best epoch: {result.best_epoch}\")\nprint(f\"Test accuracy: {test_metrics.metrics['accuracy']:.2%}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#pattern-hyperparameter-tuning","title":"Pattern: Hyperparameter Tuning","text":"<pre><code>from itertools import product\n\n# Define hyperparameters to try\nlearning_rates = [0.0001, 0.001, 0.01]\nbatch_sizes = [16, 32, 64]\noptimizers = [\"adam\", \"rmsprop\"]\n\nbest_val_loss = float('inf')\nbest_params = {}\n\nfor lr, bs, opt in product(learning_rates, batch_sizes, optimizers):\n    print(f\"\\nTrying: lr={lr}, batch_size={bs}, optimizer={opt}\")\n\n    # Create fresh model\n    model = create_model()\n\n    # Configure training\n    config = ModelTrainingConfig(\n        epochs=30,\n        learning_rate=lr,\n        batch_size=bs,\n        optimizer=opt,\n        verbose=0,\n        callbacks=[\n            {\"name\": \"EarlyStopping\", \"params\": {\"patience\": 5}},\n        ],\n    )\n\n    # Optimize dataset with new batch size\n    optimizer = DatasetOptimizer(batch_size=bs)\n    train_opt = optimizer.optimize(train_data)\n    val_opt = optimizer.optimize(val_data)\n\n    # Train\n    result = trainer.train(model, train_opt, config, validation_dataset=val_opt)\n\n    # Check if best\n    val_loss = result.history['val_loss'][-1]\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        best_params = {'lr': lr, 'batch_size': bs, 'optimizer': opt}\n        # Save best model\n        persistence.save(result.model, \"models/best_model\")\n        print(f\"\u2728 New best! Val loss: {val_loss:.4f}\")\n\nprint(f\"\\n\ud83c\udfc6 Best params: {best_params}\")\nprint(f\"\ud83c\udfc6 Best val loss: {best_val_loss:.4f}\")\n</code></pre>"},{"location":"frameworks/tensorflow.html#pattern-mixed-precision-training","title":"Pattern: Mixed Precision Training","text":"<pre><code>import tensorflow as tf\n\n# Enable mixed precision\npolicy = tf.keras.mixed_precision.Policy('mixed_float16')\ntf.keras.mixed_precision.set_global_policy(policy)\n\n# Create model (automatically uses mixed precision)\nmodel = create_model()\n\n# Train as usual\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer=\"sgd\",\n)\n\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#pattern-custom-training-loop","title":"Pattern: Custom Training Loop","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelTrainer\n\nclass CustomTrainer(ModelTrainer):\n    def train_step(self, model, batch):\n        \"\"\"Custom training step logic.\"\"\"\n        features, labels = batch\n\n        with tf.GradientTape() as tape:\n            predictions = model(features, training=True)\n            loss = self.loss_fn(labels, predictions)\n\n            # Add custom regularization\n            regularization_loss = sum(model.losses)\n            total_loss = loss + regularization_loss\n\n        # Compute gradients\n        gradients = tape.gradient(total_loss, model.trainable_variables)\n\n        # Custom gradient clipping\n        gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n\n        # Apply gradients\n        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        return {'loss': loss, 'total_loss': total_loss}\n\n# Use custom trainer\ncustom_trainer = CustomTrainer()\nresult = custom_trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#integration-with-tensorflow-ecosystem","title":"Integration with TensorFlow Ecosystem \ud83d\udd27","text":""},{"location":"frameworks/tensorflow.html#tensorboard-integration","title":"TensorBoard Integration","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelTrainer, ModelTrainingConfig\n\n# TensorBoard is enabled by default\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    use_tensorboard=True,  # Default is True\n    tensorboard_log_dir=\"logs/my_experiment\",  # Optional\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\n# View results in terminal:\n# tensorboard --logdir=logs\n\n# Advanced TensorBoard configuration\nconfig = ModelTrainingConfig(\n    epochs=50,\n    learning_rate=0.001,\n    use_tensorboard=True,\n    tensorboard_log_dir=\"logs/advanced\",\n    tensorboard_params={\n        \"histogram_freq\": 1,\n        \"write_graph\": True,\n        \"write_images\": True,\n        \"update_freq\": \"epoch\",\n        \"profile_batch\": \"10,20\",\n        \"embeddings_freq\": 1,\n    },\n)\n\nresult = trainer.train(model, dataset, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#tensorflow-datasets-integration","title":"TensorFlow Datasets Integration","text":"<pre><code>import tensorflow_datasets as tfds\nfrom mlpotion.frameworks.tensorflow import DatasetOptimizer\n\n# Load from TFDS\n(train_ds, val_ds, test_ds), info = tfds.load(\n    'mnist',\n    split=['train[:80%]', 'train[80%:]', 'test'],\n    with_info=True,\n    as_supervised=True,\n)\n\n# Optimize with MLPotion\noptimizer = DatasetOptimizer(batch_size=32, cache=True, prefetch=True)\ntrain_ds = optimizer.optimize(train_ds)\n\n# Train as usual\nresult = trainer.train(model, train_ds, config)\n</code></pre>"},{"location":"frameworks/tensorflow.html#best-practices","title":"Best Practices \ud83d\udca1","text":"<ol> <li>Use Dataset Optimization: Always use <code>DatasetOptimizer</code> for better performance</li> <li>Enable Prefetching: Set <code>prefetch=True</code> to overlap data loading with training</li> <li>Cache Small Datasets: Use <code>cache=True</code> if dataset fits in memory</li> <li>Early Stopping: Enable to prevent overfitting</li> <li>SavedModel Format: Use for production deployment</li> <li>Mixed Precision: Enable for faster training on modern GPUs</li> </ol>"},{"location":"frameworks/tensorflow.html#troubleshooting","title":"Troubleshooting \ud83d\udd27","text":""},{"location":"frameworks/tensorflow.html#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Solution: Reduce batch size or use gradient accumulation</p> <pre><code>config = ModelTrainingConfig(\n    batch_size=16,  # Smaller batches\n    # Or use gradient accumulation\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#issue-slow-data-loading","title":"Issue: Slow Data Loading","text":"<p>Solution: Enable optimization and parallel reading</p> <pre><code>loader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    num_parallel_reads=8,  # More parallel readers\n)\n\noptimizer = DatasetOptimizer(\n    prefetch=True,\n    num_parallel_calls=tf.data.AUTOTUNE,\n)\n</code></pre>"},{"location":"frameworks/tensorflow.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>PyTorch Guide \u2192 - Compare with PyTorch</li> <li>ZenML Integration \u2192 - Add MLOps</li> <li>API Reference \u2192 - Detailed API docs</li> </ul> <p> TensorFlow + MLPotion = Production ML Made Easy! \ud83d\udd36 </p>"},{"location":"getting-started/concepts.html","title":"Core Concepts \ud83e\udde0","text":"<p>Understanding MLPotion's architecture will make you a more effective potion brewer. Let's dive into the key concepts!</p>"},{"location":"getting-started/concepts.html#design-philosophy","title":"Design Philosophy \ud83c\udfaf","text":"<p>MLPotion is built on a foundation of modularity, reusability, and extensibility. Our design philosophy centers on providing composable building blocks that work seamlessly across frameworks and MLOps platforms.</p>"},{"location":"getting-started/concepts.html#core-pillars","title":"Core Pillars","text":"<ol> <li>Modularity First: Small, composable, reusable building blocks</li> <li>Each component has a single, well-defined responsibility</li> <li>Components can be mixed and matched freely</li> <li>No monolithic frameworks or rigid hierarchies</li> <li> <p>Build complex workflows from simple pieces</p> </li> <li> <p>Framework Agnostic: Works with TensorFlow, PyTorch, Keras with identical APIs</p> </li> <li>Write code once, use it with any framework</li> <li>Same patterns, same interfaces, different implementations</li> <li>Framework-specific optimizations under the hood</li> <li> <p>Easy migration between frameworks</p> </li> <li> <p>Community Extensible: Easy to add new frameworks, new integrations</p> </li> <li>ZenML is just one integration - extend to Prefect, Airflow, Kubeflow, or any orchestrator</li> <li>Protocol-based design makes adding new components straightforward</li> <li>No vendor lock-in - use what works for your team</li> <li> <p>Community contributions welcome - see Contributing Guide</p> </li> <li> <p>Protocol-Based: Type-safe, flexible design using Python protocols</p> </li> <li>Interfaces over inheritance</li> <li>Duck typing with type safety</li> <li>Framework-specific types preserved</li> <li>IDE autocomplete and static type checking</li> </ol>"},{"location":"getting-started/concepts.html#the-big-picture","title":"The Big Picture \ud83d\uddbc\ufe0f","text":"<p>MLPotion provides a consistent interface across different ML frameworks:</p> <pre><code>graph TB\n    A[Your Code] --&gt; B[MLPotion Protocols]\n    B --&gt; C[TensorFlow Implementation]\n    B --&gt; D[PyTorch Implementation]\n    B --&gt; E[Keras Implementation]\n\n    C --&gt; F[tf.data.Dataset]\n    D --&gt; G[torch.DataLoader]\n    E --&gt; H[keras Dataset]\n\n    style B fill:#4a86e8,color:#fff\n    style C fill:#ff6f00,color:#fff\n    style D fill:#ee4c2c,color:#fff\n    style E fill:#d00000,color:#fff</code></pre>"},{"location":"getting-started/concepts.html#the-protocol-pattern","title":"The Protocol Pattern \ud83c\udfaf","text":"<p>At MLPotion's heart are Protocols - Python's way of saying \"I don't care what you are, just what you can do.\"</p>"},{"location":"getting-started/concepts.html#whats-a-protocol","title":"What's a Protocol?","text":"<pre><code>from typing import Protocol\n\nclass DataLoader(Protocol):\n    \"\"\"I don't care HOW you load data, just that you CAN.\"\"\"\n\n    def load(self) -&gt; Dataset:\n        \"\"\"Load and return a dataset.\"\"\"\n        ...\n</code></pre> <p>Any class with a <code>load()</code> method satisfies this protocol!</p>"},{"location":"getting-started/concepts.html#why-protocols","title":"Why Protocols?","text":"<p>Without protocols (the old way):</p> <pre><code># Rigid inheritance - eww!\nclass BaseDataLoader(ABC):\n    @abstractmethod\n    def load(self) -&gt; Dataset:\n        pass\n\nclass TFDataLoader(BaseDataLoader):  # Must inherit\n    def load(self) -&gt; tf.data.Dataset:\n        # Implementation\n        pass\n</code></pre> <p>With protocols (the MLPotion way):</p> <pre><code># Duck typing with type safety!\nclass TFDataLoader:  # No inheritance needed!\n    def load(self) -&gt; tf.data.Dataset:\n        # Implementation\n        pass\n\n# Type checker knows this works!\nloader: DataLoader = TFDataLoader()  # \u2705 Type safe!\n</code></pre> <p>Benefits:</p> <ul> <li>No forced inheritance</li> <li>Framework-specific types preserved</li> <li>IDE autocomplete works perfectly</li> <li>Mix and match components easily</li> </ul>"},{"location":"getting-started/concepts.html#cross-framework-example","title":"Cross-Framework Example \ud83d\udd04","text":"<p>See how the same pattern works across all frameworks:</p> <pre><code># TensorFlow\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\nloader = TFCSVDataLoader(file_pattern=\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()  # Returns tf.data.Dataset\ntrainer = TFModelTrainer()\nresult = trainer.train(model, dataset, config)\n\n# PyTorch\nfrom mlpotion.frameworks.pytorch import PyTorchCSVDataset, PyTorchModelTrainer\n\ndataset = PyTorchCSVDataset(file_pattern=\"data.csv\", label_name=\"target\")\nloader = DataLoader(dataset, batch_size=32)  # Returns torch.utils.data.DataLoader\ntrainer = PyTorchModelTrainer()\nresult = trainer.train(model, loader, config)\n\n# Keras\nfrom mlpotion.frameworks.keras import KerasCSVDataLoader, KerasModelTrainer\n\nloader = KerasCSVDataLoader(file_pattern=\"data.csv\", label_name=\"target\", batch_size=32)\ndataset = loader.load()  # Returns keras-compatible dataset\ntrainer = KerasModelTrainer()\nresult = trainer.train(model, dataset, config)\n</code></pre> <p>Same concepts, same API, different frameworks!</p>"},{"location":"getting-started/concepts.html#extending-mlpotion","title":"Extending MLPotion \ud83d\udd27","text":"<p>MLPotion is designed for community extensibility:</p>"},{"location":"getting-started/concepts.html#adding-new-integrations","title":"Adding New Integrations","text":"<p>Want to integrate with Prefect, Airflow, or Kubeflow? Just wrap MLPotion components:</p> <pre><code># Example: Creating Prefect tasks\nfrom prefect import task\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\n@task\ndef load_data_task(file_pattern: str):\n    loader = TFCSVDataLoader(file_pattern=file_pattern, ...)\n    return loader.load()\n\n@task\ndef train_model_task(model, dataset, config):\n    trainer = TFModelTrainer()\n    return trainer.train(model, dataset, config)\n\n# Use in Prefect flow\nfrom prefect import flow\n\n@flow\ndef ml_flow():\n    dataset = load_data_task(\"data.csv\")\n    result = train_model_task(model, dataset, config)\n    return result\n</code></pre>"},{"location":"getting-started/concepts.html#adding-new-frameworks","title":"Adding New Frameworks","text":"<p>Want to add support for JAX, MXNet, or another framework? Implement the protocols:</p> <pre><code>from mlpotion.core.protocols import DataLoader, ModelTrainer\n\nclass JAXDataLoader:\n    \"\"\"Implements DataLoader protocol for JAX.\"\"\"\n    def load(self) -&gt; jax.Array:\n        # Your JAX-specific implementation\n        return jax_array\n\nclass JAXModelTrainer:\n    \"\"\"Implements ModelTrainer protocol for JAX.\"\"\"\n    def train(self, model, dataset, config, validation_dataset=None):\n        # Your JAX-specific training logic\n        return TrainingResult(...)\n</code></pre> <p>See Contributing Guide to add your integration!</p>"},{"location":"getting-started/concepts.html#core-components","title":"Core Components \ud83e\udde9","text":"<p>MLPotion has 6 main component types:</p>"},{"location":"getting-started/concepts.html#1-data-loaders","title":"1. Data Loaders \ud83d\udce5","text":"<p>Purpose: Load data from various sources into framework-specific formats.</p> <p>Protocol:</p> <pre><code>class DataLoader(Protocol[DatasetT]):\n    def load(self) -&gt; DatasetT:\n        \"\"\"Load data and return a dataset.\"\"\"\n        ...\n</code></pre> <p>Implementations:</p> <pre><code># TensorFlow\nTFCSVDataLoader \u2192 tf.data.Dataset\nTFParquetLoader \u2192 tf.data.Dataset\n\n# PyTorch\nPyTorchCSVDataset \u2192 torch.utils.data.Dataset\nPyTorchDataLoaderFactory \u2192 torch.utils.data.DataLoader\n\n# Keras\nKerasCSVDataLoader \u2192 keras dataset\n</code></pre> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFCSVDataLoader\n\nloader = TFCSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n)\ndataset = loader.load()  # Returns tf.data.Dataset\n</code></pre>"},{"location":"getting-started/concepts.html#2-dataset-optimizers","title":"2. Dataset Optimizers \u26a1","text":"<p>Purpose: Optimize datasets for training/inference performance.</p> <p>Protocol:</p> <pre><code>class DatasetOptimizer(Protocol[DatasetT]):\n    def optimize(self, dataset: DatasetT) -&gt; DatasetT:\n        \"\"\"Optimize dataset for performance.\"\"\"\n        ...\n</code></pre> <p>What they do:</p> <ul> <li>Batching</li> <li>Caching</li> <li>Prefetching</li> <li>Shuffling</li> <li>Parallelization</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFDatasetOptimizer\n\noptimizer = TFDatasetOptimizer(\n    batch_size=32,\n    shuffle_buffer_size=1000,\n    cache=True,\n    prefetch=True,\n)\n\noptimized_dataset = optimizer.optimize(dataset)\n</code></pre>"},{"location":"getting-started/concepts.html#3-model-trainers","title":"3. Model Trainers \ud83c\udf93","text":"<p>Purpose: Train models with a consistent interface.</p> <p>Protocol:</p> <pre><code>class ModelTrainer(Protocol[ModelT, DatasetT]):\n    def train(\n        self,\n        model: ModelT,\n        dataset: DatasetT,\n        config: TrainingConfig,\n        validation_dataset: DatasetT | None = None,\n    ) -&gt; TrainingResult[ModelT]:\n        \"\"\"Train a model and return results.\"\"\"\n        ...\n</code></pre> <p>What they handle:</p> <ul> <li>Model compilation</li> <li>Training loop</li> <li>Validation</li> <li>Callbacks</li> <li>History tracking</li> <li>Early stopping</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelTrainer, TFTrainingConfig\n\ntrainer = TFModelTrainer()\nconfig = TFTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    early_stopping=True,\n)\n\nresult = trainer.train(model, dataset, config)\nprint(f\"Final loss: {result.metrics['loss']}\")\n</code></pre>"},{"location":"getting-started/concepts.html#4-model-evaluators","title":"4. Model Evaluators \ud83d\udcca","text":"<p>Purpose: Evaluate trained models consistently.</p> <p>Protocol:</p> <pre><code>class ModelEvaluator(Protocol[ModelT, DatasetT]):\n    def evaluate(\n        self,\n        model: ModelT,\n        dataset: DatasetT,\n        config: EvaluationConfig,\n    ) -&gt; EvaluationResult:\n        \"\"\"Evaluate model and return metrics.\"\"\"\n        ...\n</code></pre> <p>What they provide:</p> <ul> <li>Metric computation</li> <li>Performance analysis</li> <li>Result objects</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelEvaluator\n\nevaluator = TFModelEvaluator()\nresult = evaluator.evaluate(model, test_dataset, config)\nprint(f\"Test accuracy: {result.metrics['accuracy']:.2%}\")\n</code></pre>"},{"location":"getting-started/concepts.html#5-model-persistence","title":"5. Model Persistence \ud83d\udcbe","text":"<p>Purpose: Save and load models reliably.</p> <p>Protocol:</p> <pre><code>class ModelPersistence(Protocol[ModelT]):\n    def save(\n        self,\n        model: ModelT,\n        path: str,\n        **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Save model to disk.\"\"\"\n        ...\n\n    def load(self, path: str, **kwargs: Any) -&gt; tuple[ModelT, dict[str, Any]]:\n        \"\"\"Load model from disk.\n\n        Returns:\n            Tuple of (loaded model, inspection metadata)\n        \"\"\"\n        ...\n</code></pre> <p>What they handle:</p> <ul> <li>Model serialization</li> <li>Metadata preservation</li> <li>Model inspection on load</li> <li>Version compatibility</li> <li>Path management</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelPersistence\n\npersistence = TFModelPersistence(path=\"models/my_model\", model=model)\n# Save\npersistence.save()\n\n# Load - returns tuple of (model, metadata)\nloader = TFModelPersistence(path=\"models/my_model\")\nloaded_model, metadata = loader.load()\nprint(f\"Model inputs: {metadata['inputs']}\")\nprint(f\"Model outputs: {metadata['outputs']}\")\n</code></pre>"},{"location":"getting-started/concepts.html#6-model-exporters","title":"6. Model Exporters \ud83d\udce4","text":"<p>Purpose: Export models for production deployment.</p> <p>Protocol:</p> <pre><code>class ModelExporter(Protocol[ModelT]):\n    def export(\n        self,\n        model: ModelT,\n        export_path: str,\n        config: ExportConfig,\n    ) -&gt; ExportResult:\n        \"\"\"Export model for serving.\"\"\"\n        ...\n</code></pre> <p>Export formats:</p> <ul> <li>TensorFlow: SavedModel, TFLite, TF.js</li> <li>PyTorch: TorchScript, ONNX</li> <li>Keras: Keras format, ONNX</li> </ul> <p>Example:</p> <pre><code>from mlpotion.frameworks.tensorflow import TFModelExporter, TFExportConfig\n\nexporter = TFModelExporter()\nconfig = TFExportConfig(format=\"saved_model\")\n\nresult = exporter.export(model, \"exports/model\", config)\nprint(f\"Exported to: {result.export_path}\")\n</code></pre>"},{"location":"getting-started/concepts.html#configuration-objects","title":"Configuration Objects \u2699\ufe0f","text":"<p>MLPotion uses Pydantic-based config objects for type safety and validation.</p>"},{"location":"getting-started/concepts.html#training-configuration","title":"Training Configuration","text":"<pre><code>from mlpotion.frameworks.tensorflow import TFTrainingConfig\n\nconfig = TFTrainingConfig(\n    # Required\n    epochs=10,\n    learning_rate=0.001,\n\n    # Optimizer\n    optimizer_type=\"adam\",  # or \"sgd\", \"rmsprop\", etc.\n    optimizer_kwargs={\"beta_1\": 0.9, \"beta_2\": 0.999\"},\n\n    # Loss and metrics\n    loss=\"mse\",  # or custom loss\n    metrics=[\"mae\", \"mse\"],\n\n    # Training behavior\n    batch_size=32,\n    validation_split=0.2,\n    shuffle=True,\n    verbose=1,\n\n    # Callbacks\n    early_stopping=True,\n    early_stopping_patience=10,\n    early_stopping_monitor=\"val_loss\",\n\n    # Checkpointing\n    save_best_only=True,\n    checkpoint_monitor=\"val_loss\",\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Type checking at creation time</li> <li>Validation before training</li> <li>IDE autocomplete for all options</li> <li>Serializable for reproducibility</li> </ul>"},{"location":"getting-started/concepts.html#result-objects","title":"Result Objects \ud83d\udccb","text":"<p>All operations return rich result objects:</p>"},{"location":"getting-started/concepts.html#trainingresult","title":"TrainingResult","text":"<pre><code>@dataclass\nclass TrainingResult(Generic[ModelT]):\n    model: ModelT                           # Trained model\n    history: dict[str, list[float]]         # Training history\n    metrics: dict[str, float]               # Final metrics\n    config: TrainingConfig                  # Configuration used\n    training_time: float | None             # Training duration\n    best_epoch: int | None                  # Best epoch (if early stopping)\n\n# Usage\nresult = trainer.train(...)\nprint(f\"Loss: {result.metrics['loss']}\")\nprint(f\"Training time: {result.training_time}s\")\nprint(f\"Best epoch: {result.best_epoch}\")\n\n# Access history\nlosses = result.history['loss']\nval_losses = result.history['val_loss']\n</code></pre>"},{"location":"getting-started/concepts.html#evaluationresult","title":"EvaluationResult","text":"<pre><code>@dataclass\nclass EvaluationResult:\n    metrics: dict[str, float]              # Evaluation metrics\n    config: EvaluationConfig               # Configuration used\n    evaluation_time: float | None          # Evaluation duration\n\n# Usage\nresult = evaluator.evaluate(...)\naccuracy = result.get_metric('accuracy')\n</code></pre>"},{"location":"getting-started/concepts.html#exportresult","title":"ExportResult","text":"<pre><code>@dataclass\nclass ExportResult:\n    export_path: str                       # Where model was saved\n    format: str                            # Export format used\n    config: ExportConfig                   # Configuration used\n    metadata: dict[str, Any]               # Additional info\n\n# Usage\nresult = exporter.export(...)\nprint(f\"Model saved to: {result.export_path}\")\nprint(f\"Format: {result.format}\")\n</code></pre>"},{"location":"getting-started/concepts.html#type-safety","title":"Type Safety \ud83d\udee1\ufe0f","text":"<p>MLPotion uses Python 3.10+ type hints for maximum safety:</p>"},{"location":"getting-started/concepts.html#generic-types","title":"Generic Types","text":"<pre><code>from typing import TypeVar, Generic\n\nModelT = TypeVar(\"ModelT\")\nDatasetT = TypeVar(\"DatasetT\")\n\nclass Trainer(Generic[ModelT, DatasetT]):\n    def train(\n        self,\n        model: ModelT,\n        dataset: DatasetT,\n    ) -&gt; TrainingResult[ModelT]:\n        ...\n</code></pre> <p>What this means:</p> <pre><code># TensorFlow\ntf_trainer: Trainer[tf.keras.Model, tf.data.Dataset]\ntf_result: TrainingResult[tf.keras.Model]\n\n# PyTorch\ntorch_trainer: Trainer[nn.Module, DataLoader]\ntorch_result: TrainingResult[nn.Module]\n</code></pre> <p>Your IDE knows the exact types!</p>"},{"location":"getting-started/concepts.html#runtime-checking","title":"Runtime Checking","text":"<pre><code>from mlpotion.core.protocols import DataLoader\n\n# This is checked at runtime!\nif isinstance(my_loader, DataLoader):\n    dataset = my_loader.load()\n</code></pre>"},{"location":"getting-started/concepts.html#error-handling","title":"Error Handling \ud83d\udea8","text":"<p>MLPotion has a consistent exception hierarchy:</p> <pre><code>MLPotionError                    # Base exception\n\u251c\u2500\u2500 DataLoadingError            # Data loading issues\n\u251c\u2500\u2500 TrainingError               # Training failures\n\u251c\u2500\u2500 EvaluationError             # Evaluation problems\n\u251c\u2500\u2500 ExportError                 # Export issues\n\u2514\u2500\u2500 ConfigurationError          # Invalid configuration\n</code></pre> <p>Usage:</p> <pre><code>from mlpotion.core import DataLoadingError, TrainingError\n\ntry:\n    dataset = loader.load()\n    result = trainer.train(model, dataset, config)\nexcept DataLoadingError as e:\n    print(f\"Failed to load data: {e}\")\nexcept TrainingError as e:\n    print(f\"Training failed: {e}\")\n</code></pre>"},{"location":"getting-started/concepts.html#framework-detection","title":"Framework Detection \ud83d\udd0d","text":"<p>MLPotion auto-detects available frameworks:</p> <pre><code>from mlpotion.utils import is_framework_available, get_available_frameworks\n\n# Check if a framework is available\nif is_framework_available(\"tensorflow\"):\n    from mlpotion.frameworks.tensorflow import TFCSVDataLoader\n    # Use TensorFlow components\n\n# Get all available frameworks\nframeworks = get_available_frameworks()\nprint(f\"Available: {frameworks}\")  # ['tensorflow', 'torch']\n</code></pre>"},{"location":"getting-started/concepts.html#composition-pattern","title":"Composition Pattern \ud83d\udd17","text":"<p>MLPotion components compose naturally:</p> <pre><code># Load data\nloader = TFCSVDataLoader(...)\ndataset = loader.load()\n\n# Optimize\noptimizer = TFDatasetOptimizer(...)\ndataset = optimizer.optimize(dataset)\n\n# Train\ntrainer = TFModelTrainer()\nresult = trainer.train(model, dataset, config)\n\n# Evaluate\nevaluator = TFModelEvaluator()\nmetrics = evaluator.evaluate(result.model, dataset, config)\n\n# Export\nexporter = TFModelExporter()\nexport_result = exporter.export(result.model, \"path/\", export_config)\n</code></pre> <p>Each component does one thing well, and they work together seamlessly!</p>"},{"location":"getting-started/concepts.html#zenml-integration","title":"ZenML Integration \ud83d\udd04","text":"<p>MLPotion provides ready-to-use ZenML steps for seamless MLOps integration:</p> <pre><code>from zenml import pipeline\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,\n    optimize_data,\n    train_model,\n    evaluate_model,\n    export_model,\n)\n\n@pipeline\ndef ml_pipeline():\n    # Load data into tf.data.Dataset\n    dataset = load_data(\n        file_path=\"data.csv\",\n        batch_size=32,\n        label_name=\"target\",\n    )\n\n    # Optimize dataset for training\n    dataset = optimize_data(\n        dataset=dataset,\n        prefetch=True,\n        cache=True,\n    )\n\n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        dataset=dataset,\n        epochs=10,\n        learning_rate=0.001,\n    )\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=trained_model,\n        dataset=dataset,\n    )\n\n    # Export for serving\n    export_path = export_model(\n        model=trained_model,\n        export_path=\"models/my_model\",\n        export_format=\"keras\",\n    )\n\n    return metrics, export_path\n</code></pre> <p>What you get:</p> <ul> <li>Automatic artifact tracking</li> <li>Pipeline reproducibility</li> <li>Version control for models and datasets</li> <li>Caching of unchanged steps</li> <li>Full lineage tracking</li> </ul> <p>Important: ZenML is just one integration! You can extend MLPotion to work with Prefect, Airflow, Kubeflow, or any other orchestrator. See Extending MLPotion above.</p>"},{"location":"getting-started/concepts.html#design-patterns","title":"Design Patterns \ud83c\udfa8","text":""},{"location":"getting-started/concepts.html#strategy-pattern","title":"Strategy Pattern","text":"<p>Different implementations of the same protocol:</p> <pre><code># All implement DataLoader protocol\nloaders = [\n    TFCSVDataLoader(...),\n    TFParquetLoader(...),\n    TFBigQueryLoader(...),\n]\n\n# Use any of them interchangeably\nfor loader in loaders:\n    dataset = loader.load()  # Works with all!\n</code></pre>"},{"location":"getting-started/concepts.html#factory-pattern","title":"Factory Pattern","text":"<p>Create complex objects easily:</p> <pre><code>from mlpotion.frameworks.pytorch import PyTorchDataLoaderFactory\n\nfactory = PyTorchDataLoaderFactory(\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n)\n\n# Create dataloaders for different datasets\ntrain_loader = factory.load(train_dataset)\nval_loader = factory.load(val_dataset)\ntest_loader = factory.load(test_dataset)\n</code></pre>"},{"location":"getting-started/concepts.html#builder-pattern","title":"Builder Pattern","text":"<p>Complex configuration:</p> <pre><code>config = (TFTrainingConfig()\n    .with_epochs(10)\n    .with_learning_rate(0.001)\n    .with_early_stopping(patience=5)\n    .with_checkpointing(save_best_only=True)\n    .build())\n</code></pre> <p>(Note: This is conceptual; actual API uses direct construction)</p>"},{"location":"getting-started/concepts.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<p>Now that you understand the concepts:</p> <ol> <li>TensorFlow Guide \u2192 - TensorFlow-specific details</li> <li>PyTorch Guide \u2192 - PyTorch-specific details</li> <li>Keras Guide \u2192 - Keras-specific details</li> <li>ZenML Integration \u2192 - Add MLOps powers</li> </ol>"},{"location":"getting-started/concepts.html#key-takeaways","title":"Key Takeaways \ud83d\udcdd","text":"<ul> <li>Protocols &gt; Inheritance: Flexible, type-safe interfaces</li> <li>Composition &gt; Configuration: Build by combining components</li> <li>Type Safety: Catch errors early with mypy</li> <li>Consistency: Same patterns across all frameworks</li> <li>Simplicity: Each component does one thing well</li> </ul> <p> Concepts mastered! Time to dive deeper! \ud83e\uddd9\u200d\u2642\ufe0f </p>"},{"location":"getting-started/installation.html","title":"Installation Guide \ud83d\udce5","text":"<p>Getting MLPotion installed is easier than pronouncing \"scikit-learn\" correctly! Let's get you set up.</p>"},{"location":"getting-started/installation.html#tldr-just-tell-me-what-to-run","title":"TL;DR - Just Tell Me What to Run! \ud83c\udfc3\u200d\u2642\ufe0f","text":"<pre><code># Most common setup (using Poetry)\npoetry add mlpotion -E tensorflow\n\n# ... or using pip\npip install \"mlpotion[tensorflow]\"\n\n# Or for PyTorch lovers\npoetry add mlpotion -E pytorch\n\n# Or if you can't decide (we don't judge)\npoetry add mlpotion -E all\n</code></pre> <p>Done! Jump to Quick Start to start brewing.</p>"},{"location":"getting-started/installation.html#requirements","title":"Requirements \ud83d\udccb","text":"<p>Before you start, make sure you have:</p> <ul> <li>Python &gt;3.10 (we live on the edge, but not too close to it)</li> <li>pip (you probably have this already)</li> <li>A sense of adventure (optional, but recommended)</li> </ul> <p>Python Version</p> <p>MLPotion requires Python 3.10 or 3.11. Why? Because we use modern type hints that make your IDE actually helpful! Python 3.12 support is coming soon.</p>"},{"location":"getting-started/installation.html#installation-options","title":"Installation Options \ud83c\udfaf","text":"<p>MLPotion follows a \"bring your own framework\" philosophy. You only install what you need!</p>"},{"location":"getting-started/installation.html#option-1-core-only-framework-agnostic","title":"Option 1: Core Only (Framework Agnostic) \ud83c\udf1f","text":"<p>Install just the core without any ML frameworks:</p> <pre><code>poetry add mlpotion\n</code></pre> <p>Use this when:</p> <ul> <li>You want to explore the package structure</li> <li>You're installing on systems without ML frameworks</li> <li>You're writing framework-agnostic code</li> <li>You're a minimalist at heart</li> </ul> <p>What you get:</p> <ul> <li>Core protocols and interfaces</li> <li>Result types and configurations</li> <li>Utility functions</li> <li>Type stubs for IDE support</li> </ul> <p>What you don't get:</p> <ul> <li>Actual ML framework implementations (obviously!)</li> <li>Data loaders (they need frameworks)</li> <li>Training components (ditto)</li> </ul>"},{"location":"getting-started/installation.html#option-2-with-tensorflow","title":"Option 2: With TensorFlow \ud83d\udd36","text":"<p>The production workhorse setup:</p> <pre><code>poetry add mlpotion -E tensorflow\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>TensorFlow 2.15+</li> <li>Keras 3.0+ (automatically included with TensorFlow)</li> <li>All TensorFlow-specific components</li> </ul> <p>Perfect for:</p> <ul> <li>Production deployments</li> <li>TensorFlow ecosystem users</li> <li>Google Cloud Platform projects</li> <li>When you need tf.data.Dataset optimization</li> </ul>"},{"location":"getting-started/installation.html#option-3-with-pytorch","title":"Option 3: With PyTorch \ud83d\udd25","text":"<p>The researcher's choice:</p> <pre><code>poetry add mlpotion -E pytorch\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>PyTorch 2.0+</li> <li>TorchVision (for image processing)</li> <li>All PyTorch-specific components</li> </ul> <p>Perfect for:</p> <ul> <li>Research projects</li> <li>Academic work</li> <li>When you love <code>nn.Module</code></li> <li>Dynamic computation graphs</li> </ul>"},{"location":"getting-started/installation.html#option-4-with-keras","title":"Option 4: With Keras \ud83c\udfa8","text":"<p>The friendly, backend-agnostic option:</p> <pre><code>poetry add mlpotion -E keras\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>Keras 3.0+ (standalone)</li> <li>Keras-specific components</li> </ul> <p>Perfect for:</p> <ul> <li>Quick prototyping</li> <li>When you want to switch backends later</li> <li>Teaching and learning</li> <li>Keras fans (obviously!)</li> </ul>"},{"location":"getting-started/installation.html#option-5-everything","title":"Option 5: Everything! \ud83c\udf89","text":"<p>When you can't choose or need it all:</p> <pre><code>poetry add mlpotion -E all\n</code></pre> <p>What's included:</p> <ul> <li>MLPotion core</li> <li>TensorFlow 2.15+</li> <li>PyTorch 2.0+ with TorchVision</li> <li>Keras 3.0+</li> <li>All framework-specific components</li> </ul> <p>Warning:</p> <p>This will install a lot of dependencies (~3GB). Your disk space might need therapy afterward.</p>"},{"location":"getting-started/installation.html#option-6-with-zenml-integration","title":"Option 6: With ZenML Integration \ud83d\udd04","text":"<p>For the MLOps enthusiasts:</p> <pre><code># TensorFlow + ZenML\npoetry add mlpotion -E tensorflow -E zenml\n\n# PyTorch + ZenML\npoetry add mlpotion -E pytorch -E zenml\n\n# Everything + ZenML (bold choice!)\npoetry add mlpotion -E all -E zenml\n</code></pre> <p>What you get extra:</p> <ul> <li>ZenML integration components</li> <li>Pre-built pipeline steps (\u267b\ufe0f REUSABLE)</li> <li>Custom materializers</li> <li>ZenML-specific utilities</li> </ul>"},{"location":"getting-started/installation.html#installing-from-source","title":"Installing from Source \ud83d\udee0\ufe0f","text":"<p>Want the bleeding edge or contributing? Clone and install:</p> <pre><code># Clone the repository\ngit clone https://github.com/UnicoLab/MLPotion.git\ncd MLPotion\n\n# Install in development mode\npip install -e .\n\n# Or with extras\npip install -e \".[tensorflow]\"\npip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation.html#using-poetry","title":"Using Poetry \ud83d\udce6","text":"<p>We use Poetry for dependency management. If you prefer Poetry:</p> <pre><code># Clone the repo\ngit clone https://github.com/UnicoLab/MLPotion.git\ncd MLPotion\n\n# Install with Poetry\npoetry install\n\n# Or with extras\npoetry install -E tensorflow\npoetry install -E pytorch\npoetry install -E all\n</code></pre>"},{"location":"getting-started/installation.html#virtual-environments-highly-recommended","title":"Virtual Environments (Highly Recommended!) \ud83e\uddca","text":"<p>Don't pollute your global Python! Use virtual environments:</p>"},{"location":"getting-started/installation.html#using-venv","title":"Using venv","text":"<pre><code># Create virtual environment\npython -m venv mlpotion-env\n\n# Activate (macOS/Linux)\nsource mlpotion-env/bin/activate\n\n# Activate (Windows)\nmlpotion-env\\Scripts\\activate\n\n# Install MLPotion\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#using-conda","title":"Using conda","text":"<pre><code># Create conda environment\nconda create -n mlpotion python=3.10\n\n# Activate\nconda activate mlpotion\n\n# Install MLPotion\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#verifying-installation","title":"Verifying Installation \u2705","text":"<p>Let's make sure everything works:</p> <pre><code># Test core installation\nimport mlpotion\nprint(f\"MLPotion version: {mlpotion.__version__}\")\n\n# Check available frameworks\nfrom mlpotion.utils import get_available_frameworks\nprint(f\"Available frameworks: {get_available_frameworks()}\")\n</code></pre> <p>Expected output:</p> <pre><code>MLPotion version: 0.1.0\nAvailable frameworks: ['tensorflow', 'torch']  # Depends on what you installed\n</code></pre>"},{"location":"getting-started/installation.html#framework-specific-tests","title":"Framework-Specific Tests","text":""},{"location":"getting-started/installation.html#tensorflow","title":"TensorFlow","text":"<pre><code>from mlpotion.frameworks.tensorflow import TFCSVDataLoader\nprint(\"\u2705 TensorFlow support is working!\")\n</code></pre>"},{"location":"getting-started/installation.html#pytorch","title":"PyTorch","text":"<pre><code>from mlpotion.frameworks.pytorch import PyTorchCSVDataset\nprint(\"\u2705 PyTorch support is working!\")\n</code></pre>"},{"location":"getting-started/installation.html#keras","title":"Keras","text":"<pre><code>from mlpotion.frameworks.keras import KerasCSVDataLoader\nprint(\"\u2705 Keras support is working!\")\n</code></pre>"},{"location":"getting-started/installation.html#common-installation-issues","title":"Common Installation Issues \ud83d\udd27","text":""},{"location":"getting-started/installation.html#issue-tensorflow-not-installing-on-m1m2-macs","title":"Issue: TensorFlow not installing on M1/M2 Macs","text":"<p>Problem: Apple Silicon can be picky about TensorFlow.</p> <p>Solution:</p> <pre><code># Use conda for M1/M2 Macs\nconda create -n mlpotion python=3.10\nconda activate mlpotion\nconda install -c apple tensorflow-deps\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#issue-pytorch-cuda-version-mismatch","title":"Issue: PyTorch CUDA version mismatch","text":"<p>Problem: PyTorch CUDA version doesn't match your GPU.</p> <p>Solution: Install PyTorch first with the correct CUDA version:</p> <pre><code># Check your CUDA version first\nnvidia-smi\n\n# Install PyTorch with specific CUDA version\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# Then install MLPotion core only\npip install mlpotion\n</code></pre>"},{"location":"getting-started/installation.html#issue-conflicting-dependencies","title":"Issue: Conflicting dependencies","text":"<p>Problem: Package version conflicts.</p> <p>Solution: Use a fresh virtual environment:</p> <pre><code># Remove old environment\nrm -rf venv\n\n# Create fresh one\npython -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#issue-import-errors-after-installation","title":"Issue: Import errors after installation","text":"<p>Problem: Python can't find the package.</p> <p>Solution:</p> <pre><code># Check where Python looks for packages\nimport sys\nprint(sys.path)\n\n# Verify installation location\npip show mlpotion\n</code></pre> <p>If they don't match, you might have multiple Python installations. Use <code>python -m pip</code> instead of <code>pip</code>.</p>"},{"location":"getting-started/installation.html#upgrading-mlpotion","title":"Upgrading MLPotion \ud83d\udd04","text":"<p>Keep your potion fresh:</p> <pre><code># Upgrade to latest version\npip install --upgrade mlpotion[tensorflow]\n\n# Or force reinstall everything\npip install --force-reinstall mlpotion[tensorflow]\n</code></pre>"},{"location":"getting-started/installation.html#uninstalling","title":"Uninstalling \ud83d\uddd1\ufe0f","text":"<p>Sad to see you go, but here's how:</p> <pre><code># Uninstall MLPotion\npip uninstall mlpotion\n\n# Remove the frameworks too (if you want)\npip uninstall tensorflow torch keras\n</code></pre>"},{"location":"getting-started/installation.html#docker-setup","title":"Docker Setup \ud83d\udc33","text":"<p>Prefer containers? We got you:</p> <pre><code>FROM python:3.10-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install MLPotion with TensorFlow\nRUN pip install --no-cache-dir mlpotion[tensorflow]\n\n# Verify installation\nRUN python -c \"import mlpotion; print(mlpotion.__version__)\"\n\nWORKDIR /app\nCMD [\"python\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t mlpotion:latest .\ndocker run -it mlpotion:latest python\n</code></pre>"},{"location":"getting-started/installation.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<p>Installation complete! Now what?</p> <ol> <li>Quick Start \u2192 - Build your first pipeline in 5 minutes</li> <li>Core Concepts \u2192 - Understand the architecture</li> <li>Framework Guides \u2192 - Deep dive into your framework</li> </ol>"},{"location":"getting-started/installation.html#need-help","title":"Need Help? \ud83c\udd98","text":"<ul> <li>Check the FAQ for common questions</li> <li>Open an issue on GitHub</li> <li>Join our community discussions</li> </ul> <p> Installation successful? Time to brew some magic! \ud83e\uddea\u2728 </p>"},{"location":"getting-started/quickstart.html","title":"Quick Start \u26a1","text":"<p>Welcome to the fastest way to get productive with MLPotion! In 5 minutes, you'll have your first ML pipeline running.</p>"},{"location":"getting-started/quickstart.html#your-first-pipeline-in-5-minutes","title":"Your First Pipeline in 5 Minutes \u23f1\ufe0f","text":"<p>Let's build a simple regression pipeline with TensorFlow. Same principles apply to PyTorch and Keras!</p>"},{"location":"getting-started/quickstart.html#step-1-install-mlpotion-30-seconds","title":"Step 1: Install MLPotion (30 seconds)","text":"<pre><code>poetry add mlpotion -E tensorflow\n# OR\npip install \"mlpotion[tensorflow]\"\n</code></pre>"},{"location":"getting-started/quickstart.html#step-2-prepare-your-data-1-minute","title":"Step 2: Prepare Your Data (1 minute)","text":"<p>Create a simple CSV file or use your own:</p> <pre><code># create_data.py\nimport pandas as pd\nimport numpy as np\n\n# Generate synthetic data\nnp.random.seed(42)\nn_samples = 1000\n\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(n_samples),\n    'feature_2': np.random.randn(n_samples),\n    'feature_3': np.random.randn(n_samples),\n    'target': np.random.randn(n_samples)\n})\n\ndata.to_csv('data.csv', index=False)\nprint(\"\u2705 Data created!\")\n</code></pre> <p>Run it:</p> <pre><code>python create_data.py\n</code></pre>"},{"location":"getting-started/quickstart.html#step-3-load-and-explore-data-1-minute","title":"Step 3: Load and Explore Data (1 minute)","text":"<pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\n# Create a data loader\nloader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    shuffle=True,\n)\n\n# Load the dataset\ndataset = loader.load()\nprint(\"\u2705 Data loaded!\")\n\n# Peek at the data\nfor batch in dataset.take(1):\n    features, labels = batch\n    print(f\"Features shape: {features.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n</code></pre> <p>Output:</p> <pre><code>\u2705 Data loaded!\nFeatures shape: (32, 3)\nLabels shape: (32,)\n</code></pre>"},{"location":"getting-started/quickstart.html#step-4-create-and-train-a-model-2-minutes","title":"Step 4: Create and Train a Model (2 minutes)","text":"<pre><code>import tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import (\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n# Create a simple model\ndef create_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n    return model\n\nmodel = create_model()\n\n# Configure training\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n    verbose=1,\n)\n\n# Train the model\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"\u2705 Training complete!\")\nprint(f\"Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"Final MAE: {result.metrics['mae']:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart.html#step-5-evaluate-and-save-1-minute","title":"Step 5: Evaluate and Save (1 minute)","text":"<pre><code>from mlpotion.frameworks.tensorflow import (\n    ModelEvaluator,\n    ModelPersistence,\n)\n\n# Evaluate the model\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(result.model, dataset, config)\n\nprint(f\"\u2705 Evaluation complete!\")\nprint(f\"Test MAE: {eval_result.metrics['mae']:.4f}\")\n\n# Save the model\npersistence = ModelPersistence(\n    path=\"my_model\",\n    model=result.model,\n)\npersistence.save()\n\nprint(\"\u2705 Model saved to 'my_model' directory!\")\n</code></pre>"},{"location":"getting-started/quickstart.html#complete-script","title":"Complete Script","text":"<p>Here's everything together:</p> <pre><code>\"\"\"quickstart_example.py - Your first MLPotion pipeline!\"\"\"\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainingConfig,\n)\n\n# 1. Create sample data\nprint(\"\ud83d\udcca Creating sample data...\")\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'target': np.random.randn(1000)\n})\ndata.to_csv('data.csv', index=False)\n\n# 2. Load data\nprint(\"\ud83d\udce5 Loading data...\")\nloader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    shuffle=True,\n)\ndataset = loader.load()\n\n# 3. Create model\nprint(\"\ud83c\udfd7\ufe0f Creating model...\")\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\n# 4. Train model\nprint(\"\ud83c\udf93 Training model...\")\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"\\n\u2705 Training complete!\")\nprint(f\"   Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"   Final MAE: {result.metrics['mae']:.4f}\")\n\n# 5. Evaluate\nprint(\"\\n\ud83d\udcca Evaluating model...\")\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(result.model, dataset, config)\n\nprint(f\"\u2705 Evaluation complete!\")\nprint(f\"   Test MAE: {eval_result.metrics['mae']:.4f}\")\n\n# 6. Save model\nprint(\"\\n\ud83d\udcbe Saving model...\")\npersistence = ModelPersistence(\n    path=\"my_model\",\n    model=result.model,\n)\npersistence.save()\n\nprint(\"\\n\ud83c\udf89 All done! Your first MLPotion pipeline is complete!\")\n</code></pre> <p>Run it:</p> <pre><code>python quickstart_example.py\n</code></pre>"},{"location":"getting-started/quickstart.html#quick-start-with-pytorch","title":"Quick Start with PyTorch \ud83d\udd25","text":"<p>Prefer PyTorch? Here's the same pipeline:</p> <pre><code>\"\"\"quickstart_pytorch.py - PyTorch version\"\"\"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom mlpotion.frameworks.pytorch import (\n    CSVDataset,\n    CSVDataLoader,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelTrainingConfig,\n)\n\n# 1. Create sample data\nprint(\"\ud83d\udcca Creating sample data...\")\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'target': np.random.randn(1000)\n})\ndata.to_csv('data.csv', index=False)\n\n# 2. Load data\nprint(\"\ud83d\udce5 Loading data...\")\ndataset = CSVDataset(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n)\n\nfactory = CSVDataLoader(batch_size=32, shuffle=True)\ndataloader = factory.load(dataset)\n\n# 3. Create model\nprint(\"\ud83c\udfd7\ufe0f Creating model...\")\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(3, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nmodel = SimpleModel()\n\n# 4. Train model\nprint(\"\ud83c\udf93 Training model...\")\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataloader, config)\n\nprint(f\"\\n\u2705 Training complete!\")\nprint(f\"   Final loss: {result.metrics['loss']:.4f}\")\n\n# 5. Evaluate\nprint(\"\\n\ud83d\udcca Evaluating model...\")\nevaluator = ModelEvaluator()\neval_result = evaluator.evaluate(result.model, dataloader, config)\n\nprint(f\"\u2705 Evaluation complete!\")\nprint(f\"   Test loss: {eval_result.metrics['loss']:.4f}\")\n\nprint(\"\\n\ud83c\udf89 PyTorch pipeline complete!\")\n</code></pre>"},{"location":"getting-started/quickstart.html#quick-start-with-keras","title":"Quick Start with Keras \ud83c\udfa8","text":"<p>And the Keras version:</p> <pre><code>\"\"\"quickstart_keras.py - Keras version\"\"\"\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom mlpotion.frameworks.keras import (\n    CSVDataLoader,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelTrainingConfig,\n)\n\n# 1. Create sample data\nprint(\"\ud83d\udcca Creating sample data...\")\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'target': np.random.randn(1000)\n})\ndata.to_csv('data.csv', index=False)\n\n# 2. Load data\nprint(\"\ud83d\udce5 Loading data...\")\nloader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n    shuffle=True,\n)\ndataset = loader.load()\n\n# 3. Create model\nprint(\"\ud83c\udfd7\ufe0f Creating model...\")\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\n# 4. Train model\nprint(\"\ud83c\udf93 Training model...\")\nconfig = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n\ntrainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n\nprint(f\"\\n\u2705 Training complete!\")\nprint(f\"   Final loss: {result.metrics['loss']:.4f}\")\nprint(f\"   Final MAE: {result.metrics['mae']:.4f}\")\n\nprint(\"\\n\ud83c\udf89 Keras pipeline complete!\")\n</code></pre>"},{"location":"getting-started/quickstart.html#understanding-what-just-happened","title":"Understanding What Just Happened \ud83e\udd14","text":"<p>Let's break down what you just built:</p>"},{"location":"getting-started/quickstart.html#1-data-loading-the-easy-part","title":"1. Data Loading (The Easy Part)","text":"<pre><code>loader = CSVDataLoader(\n    file_pattern=\"data.csv\",\n    label_name=\"target\",\n    batch_size=32,\n)\ndataset = loader.load()\n</code></pre> <p>What it does:</p> <ul> <li>Reads your CSV file</li> <li>Separates features from labels</li> <li>Batches data for training</li> <li>Handles shuffling and caching</li> </ul> <p>Why it's cool:</p> <ul> <li>Same API across all frameworks</li> <li>Optimized for performance</li> <li>Handles edge cases automatically</li> </ul>"},{"location":"getting-started/quickstart.html#2-training-configuration-the-type-safe-part","title":"2. Training Configuration (The Type-Safe Part)","text":"<pre><code>config = ModelTrainingConfig(\n    epochs=10,\n    learning_rate=0.001,\n    optimizer_type=\"adam\",\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\n</code></pre> <p>What it does:</p> <ul> <li>Defines all training parameters</li> <li>Type-checked at runtime</li> <li>Validated before training</li> </ul> <p>Why it's cool:</p> <ul> <li>Your IDE autocompletes everything</li> <li>Catch errors before running</li> <li>Consistent across frameworks</li> </ul>"},{"location":"getting-started/quickstart.html#3-training-the-simple-part","title":"3. Training (The Simple Part)","text":"<pre><code>trainer = ModelTrainer()\nresult = trainer.train(model, dataset, config)\n</code></pre> <p>What it does:</p> <ul> <li>Compiles your model</li> <li>Runs training loop</li> <li>Tracks metrics and history</li> <li>Returns a rich result object</li> </ul> <p>Why it's cool:</p> <ul> <li>One line to train</li> <li>Consistent interface</li> <li>Detailed results</li> </ul>"},{"location":"getting-started/quickstart.html#common-patterns","title":"Common Patterns \ud83c\udfaf","text":""},{"location":"getting-started/quickstart.html#pattern-1-train-val-test-split","title":"Pattern 1: Train-Val-Test Split","text":"<pre><code>from mlpotion.frameworks.tensorflow import CSVDataLoader\n\n# Load different splits\ntrain_loader = CSVDataLoader(file_pattern=\"train.csv\", label_name=\"target\")\nval_loader = CSVDataLoader(file_pattern=\"val.csv\", label_name=\"target\")\ntest_loader = CSVDataLoader(file_pattern=\"test.csv\", label_name=\"target\")\n\ntrain_data = train_loader.load()\nval_data = val_loader.load()\ntest_data = test_loader.load()\n\n# Train with validation\nresult = trainer.train(\n    model,\n    train_data,\n    config,\n    validation_dataset=val_data\n)\n\n# Evaluate on test set\ntest_metrics = evaluator.evaluate(result.model, test_data, config)\n</code></pre>"},{"location":"getting-started/quickstart.html#pattern-2-early-stopping","title":"Pattern 2: Early Stopping","text":"<pre><code>config = ModelTrainingConfig(\n    epochs=100,  # Set high\n    learning_rate=0.001,\n    early_stopping=True,\n    early_stopping_patience=10,\n    early_stopping_monitor=\"val_loss\",\n)\n\nresult = trainer.train(model, train_data, config, validation_dataset=val_data)\nprint(f\"Best epoch: {result.best_epoch}\")\n</code></pre>"},{"location":"getting-started/quickstart.html#pattern-3-model-checkpointing","title":"Pattern 3: Model Checkpointing","text":"<pre><code>from mlpotion.frameworks.tensorflow import ModelPersistence\n\n# During training, save checkpoints\nfor epoch in range(10):\n    result = trainer.train(model, dataset, config)\n    if epoch % 2 == 0:  # Save every 2 epochs\n        persistence = ModelPersistence(\n            path=f\"checkpoint_epoch_{epoch}\",\n            model=result.model,\n        )\n        persistence.save()\n</code></pre>"},{"location":"getting-started/quickstart.html#pattern-4-hyperparameter-tuning","title":"Pattern 4: Hyperparameter Tuning","text":"<pre><code>learning_rates = [0.0001, 0.001, 0.01]\nbest_mae = float('inf')\nbest_model = None\n\nfor lr in learning_rates:\n    config = ModelTrainingConfig(\n        epochs=10,\n        learning_rate=lr,\n        verbose=0,  # Quiet mode\n    )\n\n    result = trainer.train(model, dataset, config)\n\n    if result.metrics['mae'] &lt; best_mae:\n        best_mae = result.metrics['mae']\n        best_model = result.model\n        print(f\"\u2728 New best! LR={lr}, MAE={best_mae:.4f}\")\n\npersistence = ModelPersistence(path=\"best_model\", model=best_model)\npersistence.save()\n</code></pre>"},{"location":"getting-started/quickstart.html#whats-next","title":"What's Next? \ud83d\uddfa\ufe0f","text":"<p>You've built your first pipeline! Here are your next steps:</p>"},{"location":"getting-started/quickstart.html#beginner-path","title":"Beginner Path \ud83c\udf31","text":"<ol> <li>Core Concepts \u2192 - Understand the architecture</li> <li>Framework Guide \u2192 - Deep dive into your framework</li> <li>Basic Tutorial \u2192 - Build a complete project</li> </ol>"},{"location":"getting-started/quickstart.html#intermediate-path","title":"Intermediate Path \ud83c\udf3f","text":"<ol> <li>ZenML Integration \u2192 - Add MLOps superpowers</li> <li>Advanced Training \u2192 - Custom callbacks, mixed precision</li> <li>Multi-Framework \u2192 - Switch between frameworks</li> </ol>"},{"location":"getting-started/quickstart.html#advanced-path","title":"Advanced Path \ud83c\udf33","text":"<ol> <li>Custom Components \u2192 - Build your own atoms</li> <li>Production Deployment \u2192 - Ship to production</li> <li>API Reference \u2192 - Deep technical docs</li> </ol>"},{"location":"getting-started/quickstart.html#need-help","title":"Need Help? \ud83c\udd98","text":"<ul> <li>Questions? Check the FAQ</li> <li>Issues? GitHub Issues</li> <li>Ideas? Discussions</li> </ul>"},{"location":"getting-started/quickstart.html#quick-reference-card","title":"Quick Reference Card \ud83d\udcc7","text":"<pre><code># Import patterns (UNIFIED API - same names across all frameworks!)\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,           # Load CSV data\n    DatasetOptimizer,        # Optimize datasets\n    ModelTrainer,            # Train models\n    ModelEvaluator,          # Evaluate models\n    ModelPersistence,        # Save/load models\n    ModelExporter,           # Export for serving\n    ModelTrainingConfig,     # Training configuration\n    ModelEvaluationConfig,   # Evaluation configuration\n    ModelExportConfig,       # Export configuration\n)\n\n# Basic workflow\nloader = CSVDataLoader(...)        # 1. Load data\ndataset = loader.load()\n\ntrainer = ModelTrainer()           # 2. Train\nresult = trainer.train(model, dataset, config)\n\nevaluator = ModelEvaluator()       # 3. Evaluate\nmetrics = evaluator.evaluate(result.model, dataset, config)\n\npersistence = ModelPersistence(    # 4. Save\n    path=\"path/\",\n    model=result.model,\n)\npersistence.save()\n</code></pre> <p> Congratulations! You're now brewing ML magic! \ud83e\uddea\u2728 Ready for more? Check out the tutorials! </p>"},{"location":"integrations/zenml.html","title":"ZenML Integration \ud83d\udd04","text":"<p>Transform your MLPotion pipelines into production-ready MLOps workflows with ZenML!</p> <p>Important: ZenML is just one integration example. MLPotion is designed to be framework and orchestrator agnostic. You can easily extend MLPotion to work with Prefect, Airflow, Kubeflow, or any other orchestration platform. Community contributions welcome - see Contributing Guide!</p>"},{"location":"integrations/zenml.html#why-zenml","title":"Why ZenML? \ud83e\udd14","text":"<ul> <li>Reproducibility: Track every pipeline run with full lineage</li> <li>Versioning: Automatic artifact and model versioning</li> <li>Collaboration: Share pipelines with your team</li> <li>Scalability: Run on different compute backends</li> <li>Observability: Monitor pipeline health and performance</li> </ul>"},{"location":"integrations/zenml.html#installation","title":"Installation \ud83d\udce5","text":"<pre><code># TensorFlow + ZenML\npoetry add mlpotion -E tensorflow -E zenml\n\n# PyTorch + ZenML\npoetry add mlpotion -E pytorch -E zenml\n\n# Initialize ZenML (first time only)\nzenml init\n</code></pre>"},{"location":"integrations/zenml.html#quick-example","title":"Quick Example \ud83d\ude80","text":"<pre><code>from zenml import pipeline, step\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,\n    train_model,\n    evaluate_model,\n)\n\n# custom model init step\n@step\ndef init_model() -&gt; keras.Model:\n    \"\"\"Initialize the model.\"\"\"\n    model = keras.Sequential([\n        keras.layers.Dense(10, input_shape=(3,)),\n        keras.layers.Dense(1),\n    ])\n    return model\n\n@pipeline\ndef ml_pipeline():\n    \"\"\"Simple ML pipeline with MLPotion + ZenML.\"\"\"\n    # Load data into tf.data.Dataset\n    dataset = load_data(\n        file_path=\"data.csv\",\n        batch_size=32,\n        label_name=\"target\",\n    )\n    # load model\n    model = init_model()\n\n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        dataset=dataset,\n        epochs=10,\n        learning_rate=0.001,\n    )\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=trained_model,\n        dataset=dataset,\n    )\n\n    return metrics\n\n# Run the pipeline\nimport keras\n\nmetrics = ml_pipeline()\nprint(f\"Loss: {metrics['loss']:.4f}\")\nprint(f\"MAE: {metrics['mae']:.4f}\")\n</code></pre>"},{"location":"integrations/zenml.html#available-zenml-steps","title":"Available ZenML Steps \ud83d\udce6","text":""},{"location":"integrations/zenml.html#tensorflow-steps","title":"TensorFlow Steps","text":"<pre><code>from mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,          # Load CSV data into tf.data.Dataset\n    optimize_data,      # Optimize dataset for training (caching, prefetch, etc.)\n    transform_data,     # Transform data using model and save predictions\n    train_model,        # Train TensorFlow/Keras model\n    evaluate_model,     # Evaluate model performance\n    save_model,         # Save model to disk\n    load_model,         # Load model from disk\n    export_model,       # Export model for serving (SavedModel, TFLite, etc.)\n    inspect_model,      # Inspect model architecture\n)\n</code></pre>"},{"location":"integrations/zenml.html#pytorch-steps","title":"PyTorch Steps","text":"<pre><code>from mlpotion.integrations.zenml.pytorch.steps import (\n    load_csv_data,              # Load CSV into PyTorch DataLoader\n    load_streaming_csv_data,    # Load large CSV as streaming DataLoader\n    train_model,                # Train PyTorch model\n    evaluate_model,             # Evaluate PyTorch model\n    save_model,                 # Save PyTorch model (state_dict or full)\n    load_model,                 # Load PyTorch model\n    export_model,               # Export model (TorchScript, ONNX, state_dict)\n)\n</code></pre>"},{"location":"integrations/zenml.html#keras-steps","title":"Keras Steps","text":"<pre><code>from mlpotion.integrations.zenml.keras.steps import (\n    load_data,          # Load CSV into CSVSequence\n    transform_data,     # Transform data with predictions\n    train_model,        # Train Keras model\n    evaluate_model,     # Evaluate Keras model\n    save_model,         # Save Keras model\n    load_model,         # Load Keras model\n    export_model,       # Export Keras model\n    inspect_model,      # Inspect Keras model\n)\n</code></pre>"},{"location":"integrations/zenml.html#complete-production-pipeline","title":"Complete Production Pipeline \ud83d\ude80","text":""},{"location":"integrations/zenml.html#tensorflow-example","title":"TensorFlow Example","text":"<pre><code>from zenml import pipeline\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    load_data,\n    optimize_data,\n    train_model,\n    evaluate_model,\n    save_model,\n    export_model,\n)\nimport keras\n\n# custom model init step\n@step\ndef init_model() -&gt; keras.Model:\n    \"\"\"Initialize the model.\"\"\"\n    model = keras.Sequential([\n        keras.layers.Dense(10, input_shape=(3,)),\n        keras.layers.Dense(1),\n    ])\n    return model\n\n@pipeline\ndef production_ml_pipeline(\n    train_data: str,\n    test_data: str,\n    model_name: str,\n    epochs: int = 50,\n):\n    \"\"\"Full production ML pipeline for TensorFlow.\"\"\"\n    # Load data\n    train_dataset = load_data(\n        file_path=train_data,\n        batch_size=32,\n        label_name=\"target\",\n    )\n    test_dataset = load_data(\n        file_path=test_data,\n        batch_size=32,\n        label_name=\"target\",\n    )\n\n    # Optimize datasets\n    train_dataset = optimize_data(\n        dataset=train_dataset,\n        batch_size=32,\n        cache=True,\n        prefetch=True,\n        shuffle_buffer_size=1000,\n    )\n    # init model\n    model = init_model()\n\n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        dataset=train_dataset,\n        epochs=epochs,\n        learning_rate=0.001,\n        validation_dataset=test_dataset,\n    )\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=trained_model,\n        dataset=test_dataset,\n    )\n\n    # Save model\n    save_path = save_model(\n        model=trained_model,\n        save_path=f\"models/{model_name}\",\n    )\n\n    # Export for serving\n    export_path = export_model(\n        model=trained_model,\n        export_path=f\"exports/{model_name}\",\n        export_format=\"keras\",\n    )\n\n    return metrics, export_path\n\n\n# running the pipeline\nresult = production_ml_pipeline(\n    model=model,\n    train_data=\"s3://bucket/train.csv\",\n    test_data=\"s3://bucket/test.csv\",\n    model_name=\"my-model-v1\",\n    epochs=50,\n)\n</code></pre>"},{"location":"integrations/zenml.html#pytorch-example","title":"PyTorch Example","text":"<pre><code>from zenml import pipeline\nfrom mlpotion.integrations.zenml.pytorch.steps import (\n    load_csv_data,\n    train_model,\n    evaluate_model,\n    export_model,\n)\nimport torch.nn as nn\n\n@step\ndef init_model() -&gt; nn.Module:\n    \"\"\"Initialize the model.\"\"\"\n    model = nn.Sequential(\n        nn.Linear(10, 64),\n        nn.ReLU(),\n        nn.Linear(64, 1),\n    )\n    return model\n\n@pipeline\ndef pytorch_ml_pipeline(\n    train_data: str,\n    test_data: str,\n    model_name: str,\n    epochs: int = 50,\n):\n    \"\"\"Full production ML pipeline for PyTorch.\"\"\"\n    # Load data\n    train_loader = load_csv_data(\n        file_path=train_data,\n        batch_size=32,\n        label_name=\"target\",\n        shuffle=True,\n        num_workers=4,\n    )\n    test_loader = load_csv_data(\n        file_path=test_data,\n        batch_size=32,\n        label_name=\"target\",\n        shuffle=False,\n    )\n    # init model\n    model = init_model()\n\n    # Train model\n    trained_model, train_metrics = train_model(\n        model=model,\n        dataloader=train_loader,\n        epochs=epochs,\n        learning_rate=0.001,\n        device=\"cuda\",\n        validation_dataloader=test_loader,\n    )\n\n    # Evaluate\n    eval_metrics = evaluate_model(\n        model=trained_model,\n        dataloader=test_loader,\n        device=\"cuda\",\n    )\n\n    # Export for serving\n    export_path = export_model(\n        model=trained_model,\n        export_path=f\"exports/{model_name}\",\n        export_format=\"torchscript\",\n        jit_mode=\"script\",\n    )\n\n    return eval_metrics, export_path\n\n# Run the pipeline\nresult = pytorch_ml_pipeline(\n    train_data=\"data/train.csv\",\n    test_data=\"data/test.csv\",\n    model_name=\"pytorch-model-v1\",\n    epochs=50,\n)\n</code></pre>"},{"location":"integrations/zenml.html#benefits-of-zenml-integration","title":"Benefits of ZenML Integration \ud83c\udf1f","text":""},{"location":"integrations/zenml.html#1-automatic-tracking","title":"1. Automatic Tracking","text":"<p>Every pipeline run is tracked automatically: - Input data versions - Model versions - Hyperparameters - Training metrics - Output artifacts</p>"},{"location":"integrations/zenml.html#2-artifact-caching","title":"2. Artifact Caching","text":"<p>ZenML caches artifacts, so unchanged steps are skipped:</p> <pre><code># First run: All steps execute\nresult1 = ml_pipeline()\n\n# Second run: Only changed steps execute\nresult2 = ml_pipeline()  # Much faster!\n</code></pre>"},{"location":"integrations/zenml.html#3-experiment-comparison","title":"3. Experiment Comparison","text":"<p>Compare different runs:</p> <pre><code># View all pipeline runs\nzenml pipeline runs list\n\n# Compare specific runs\nzenml pipeline runs compare RUN_ID_1 RUN_ID_2\n</code></pre>"},{"location":"integrations/zenml.html#4-model-registry","title":"4. Model Registry","text":"<p>Automatically version and register models:</p> <pre><code>from zenml.integrations.mlflow.model_deployers import MLFlowModelDeployer\n\n# Models are automatically registered\n# Access them via ZenML's model registry\n</code></pre>"},{"location":"integrations/zenml.html#custom-zenml-steps","title":"Custom ZenML Steps \ud83d\udd27","text":"<p>Create your own MLPotion-based ZenML steps:</p> <pre><code>from zenml import step\nfrom mlpotion.frameworks.tensorflow import TFModelTrainer\nimport keras\n\n@step\ndef custom_train_step(\n    model: keras.Model,\n    dataset,\n    epochs: int = 10,\n    learning_rate: float = 0.001,\n):\n    \"\"\"Custom training step with MLPotion.\"\"\"\n    trainer = TFModelTrainer()\n\n    compile_params = {\n        \"optimizer\": keras.optimizers.Adam(learning_rate=learning_rate),\n        \"loss\": \"mse\",\n        \"metrics\": [\"mae\"],\n    }\n\n    fit_params = {\n        \"epochs\": epochs,\n        \"verbose\": 1,\n    }\n\n    history = trainer.train(\n        model=model,\n        data=dataset,\n        compile_params=compile_params,\n        fit_params=fit_params,\n    )\n    return model, history\n</code></pre>"},{"location":"integrations/zenml.html#extending-beyond-zenml","title":"Extending Beyond ZenML \ud83d\ude80","text":"<p>MLPotion is not limited to ZenML! The same modular components work with any orchestrator:</p>"},{"location":"integrations/zenml.html#prefect-integration-example","title":"Prefect Integration Example","text":"<pre><code>from prefect import task, flow\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\n@task\ndef load_data_task(file_path: str):\n    loader = TFCSVDataLoader(file_pattern=file_path, label_name=\"target\", batch_size=32)\n    return loader.load()\n\n@task\ndef train_model_task(model, dataset, epochs: int = 10):\n    trainer = TFModelTrainer()\n    compile_params = {\"optimizer\": \"adam\", \"loss\": \"mse\", \"metrics\": [\"mae\"]}\n    fit_params = {\"epochs\": epochs}\n    return trainer.train(model, dataset, compile_params, fit_params)\n\n@flow\ndef ml_flow():\n    dataset = load_data_task(\"data.csv\")\n    result = train_model_task(model, dataset, epochs=10)\n    return result\n</code></pre>"},{"location":"integrations/zenml.html#airflow-integration-example","title":"Airflow Integration Example","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader, TFModelTrainer\n\ndef load_data(**context):\n    loader = TFCSVDataLoader(file_pattern=\"data.csv\", label_name=\"target\", batch_size=32)\n    dataset = loader.load()\n    # Store dataset reference or materialize\n    context['ti'].xcom_push(key='dataset', value=dataset)\n\ndef train_model(**context):\n    dataset = context['ti'].xcom_pull(key='dataset')\n    trainer = TFModelTrainer()\n    # Training logic...\n\nwith DAG('ml_pipeline', ...) as dag:\n    load_task = PythonOperator(task_id='load_data', python_callable=load_data)\n    train_task = PythonOperator(task_id='train_model', python_callable=train_model)\n    load_task &gt;&gt; train_task\n</code></pre> <p>Community contributions welcome! Want to add official support for your favorite orchestrator? See Contributing Guide!</p>"},{"location":"integrations/zenml.html#next-steps","title":"Next Steps \ud83d\uddfa\ufe0f","text":"<ul> <li>ZenML Documentation - Learn more about ZenML</li> <li>API Reference \u2192 - Detailed API docs</li> <li>Contributing Guide \u2192 - Add new integrations</li> </ul> <p> MLPotion: Built for extensibility, works with any orchestrator! \ud83d\udd04 </p>"},{"location":"tutorials/basic-pipeline.html","title":"Your First Pipeline Tutorial \ud83c\udf93","text":"<p>Let's build a complete end-to-end ML pipeline from scratch! This tutorial covers data loading, training, evaluation, and deployment.</p>"},{"location":"tutorials/basic-pipeline.html#what-well-build","title":"What We'll Build \ud83c\udfaf","text":"<p>A regression pipeline that: 1. Loads data from CSV 2. Trains a neural network 3. Evaluates performance 4. Saves the model 5. Exports for serving</p> <p>Time: ~15 minutes Level: Beginner</p>"},{"location":"tutorials/basic-pipeline.html#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<pre><code>poetry add mlpotion -E tensorflow\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-1-prepare-your-data","title":"Step 1: Prepare Your Data \ud83d\udcca","text":"<pre><code># create_data.py\nimport pandas as pd\nimport numpy as np\n\n# Generate synthetic house price data\nnp.random.seed(42)\nn_samples = 10000\n\ndata = pd.DataFrame({\n    'square_feet': np.random.randint(500, 5000, n_samples),\n    'bedrooms': np.random.randint(1, 6, n_samples),\n    'bathrooms': np.random.randint(1, 4, n_samples),\n    'age_years': np.random.randint(0, 100, n_samples),\n    'price': np.random.randint(100000, 1000000, n_samples)\n})\n\n# Add some correlation\ndata['price'] = (\n    data['square_feet'] * 200 +\n    data['bedrooms'] * 10000 +\n    data['bathrooms'] * 15000 -\n    data['age_years'] * 500 +\n    np.random.randn(n_samples) * 50000\n)\n\n# Split into train/test\ntrain_size = int(0.8 * len(data))\ntrain_data = data[:train_size]\ntest_data = data[train_size:]\n\ntrain_data.to_csv('train.csv', index=False)\ntest_data.to_csv('test.csv', index=False)\n\nprint(f\"\u2705 Created {len(train_data)} training samples\")\nprint(f\"\u2705 Created {len(test_data)} test samples\")\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-2-build-the-pipeline","title":"Step 2: Build the Pipeline \ud83c\udfd7\ufe0f","text":"<pre><code># pipeline.py\nimport tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelTrainer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelExporter,\n    ModelTrainingConfig,\n    ModelExportConfig,\n)\n\ndef create_model(input_dim: int) -&gt; tf.keras.Model:\n    \"\"\"Create a simple neural network.\"\"\"\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n\ndef main():\n    print(\"\ud83d\ude80 Starting ML Pipeline...\")\n\n    # 1. Load data\n    print(\"\\n\ud83d\udce5 Loading data...\")\n    train_loader = CSVDataLoader(\n        file_pattern=\"train.csv\",\n        label_name=\"price\",\n        batch_size=32,\n    )\n    test_loader = CSVDataLoader(\n        file_pattern=\"test.csv\",\n        label_name=\"price\",\n        batch_size=32,\n    )\n\n    train_dataset = train_loader.load()\n    test_dataset = test_loader.load()\n\n    # 2. Optimize datasets\n    print(\"\u26a1 Optimizing datasets...\")\n    optimizer = DatasetOptimizer(\n        batch_size=32,\n        cache=True,\n        prefetch=True,\n    )\n\n    train_dataset = optimizer.optimize(train_dataset)\n    test_dataset = optimizer.optimize(test_dataset)\n\n    # 3. Create model\n    print(\"\\n\ud83c\udfd7\ufe0f Creating model...\")\n    model = create_model(input_dim=4)  # 4 features\n\n    # 4. Configure training\n    print(\"\u2699\ufe0f Configuring training...\")\n    config = ModelTrainingConfig(\n        epochs=50,\n        learning_rate=0.001,\n        optimizer_type=\"adam\",\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n        early_stopping=True,\n        early_stopping_patience=10,\n        verbose=1,\n    )\n\n    # 5. Train model\n    print(\"\\n\ud83c\udf93 Training model...\")\n    trainer = ModelTrainer()\n    result = trainer.train(\n        model,\n        train_dataset,\n        config,\n        validation_dataset=test_dataset,\n    )\n\n    print(f\"\\n\u2705 Training complete!\")\n    print(f\"   Final loss: {result.metrics['loss']:.2f}\")\n    print(f\"   Final MAE: {result.metrics['mae']:.2f}\")\n    print(f\"   Best epoch: {result.best_epoch}\")\n    print(f\"   Training time: {result.training_time:.2f}s\")\n\n    # 6. Evaluate\n    print(\"\\n\ud83d\udcca Evaluating model...\")\n    evaluator = ModelEvaluator()\n    eval_result = evaluator.evaluate(result.model, test_dataset, config)\n\n    print(f\"\u2705 Evaluation complete!\")\n    print(f\"   Test MAE: ${eval_result.metrics['mae']:,.2f}\")\n    print(f\"   Test MSE: {eval_result.metrics['mse']:,.2f}\")\n\n    # 7. Save model\n    print(\"\\n\ud83d\udcbe Saving model...\")\n    persistence = ModelPersistence(\n        path=\"models/house_price_model\",\n        model=result.model,\n    )\n    persistence.save(save_format=\".keras\")\n\n    print(\"\u2705 Model saved!\")\n\n    # 8. Export for serving\n    print(\"\\n\ud83d\udce4 Exporting model...\")\n    exporter = ModelExporter()\n    export_config = ModelExportConfig(format=\"saved_model\")\n\n    export_result = exporter.export(\n        result.model,\n        \"exports/house_price_model\",\n        export_config,\n    )\n\n    print(f\"\u2705 Model exported to: {export_result.export_path}\")\n\n    print(\"\\n\ud83c\udf89 Pipeline complete!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline \ud83c\udfc3","text":"<pre><code># Create data\npython create_data.py\n\n# Run pipeline\npython pipeline.py\n</code></pre> <p>You'll see output like:</p> <pre><code>\ud83d\ude80 Starting ML Pipeline...\n\n\ud83d\udce5 Loading data...\n\u26a1 Optimizing datasets...\n\n\ud83c\udfd7\ufe0f Creating model...\n\u2699\ufe0f Configuring training...\n\n\ud83c\udf93 Training model...\nEpoch 1/50\n250/250 [==============================] - 2s 8ms/step - loss: 52345.67 - mae: 178.32 - val_loss: 48234.56 - val_mae: 165.43\n...\nEpoch 25/50\n250/250 [==============================] - 1s 4ms/step - loss: 15234.56 - mae: 98.21 - val_loss: 16543.21 - val_mae: 102.34\n\n\u2705 Training complete!\n   Final loss: 15234.56\n   Final MAE: 98.21\n   Best epoch: 25\n   Training time: 45.23s\n\n\ud83d\udcca Evaluating model...\n\u2705 Evaluation complete!\n   Test MAE: $102.34\n   Test MSE: 16543.21\n\n\ud83d\udcbe Saving model...\n\u2705 Model saved!\n\n\ud83d\udce4 Exporting model...\n\u2705 Model exported to: exports/house_price_model\n\n\ud83c\udf89 Pipeline complete!\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#step-4-use-the-model","title":"Step 4: Use the Model \ud83d\udd2e","text":"<pre><code># predict.py\nimport tensorflow as tf\nfrom mlpotion.frameworks.tensorflow import ModelPersistence\n\n# Load model\npersistence = ModelPersistence(\n    path=\"models/house_price_model\",\n    model=None,  # Will be loaded\n)\nmodel, metadata = persistence.load()\n\n# Make predictions\nnew_house = [[2500, 3, 2, 10]]  # sq_ft, beds, baths, age\nprediction = model.predict(new_house)\n\nprint(f\"Predicted price: ${prediction[0][0]:,.2f}\")\n</code></pre>"},{"location":"tutorials/basic-pipeline.html#what-you-learned","title":"What You Learned \ud83c\udf93","text":"<ol> <li>\u2705 How to load data from CSV</li> <li>\u2705 How to optimize datasets for performance</li> <li>\u2705 How to configure and train models</li> <li>\u2705 How to evaluate model performance</li> <li>\u2705 How to save and export models</li> </ol>"},{"location":"tutorials/basic-pipeline.html#next-steps","title":"Next Steps \ud83d\ude80","text":"<ul> <li>ZenML Pipeline Tutorial \u2192 - Add MLOps superpowers</li> <li>Advanced Training \u2192 - Custom callbacks and more</li> <li>Production Deployment \u2192 - Ship to production</li> </ul> <p> Congratulations! You've built your first complete ML pipeline! \ud83c\udf89 </p>"},{"location":"tutorials/multi-framework.html","title":"Multi-Framework Tutorial \ud83d\udd00","text":"<p>Learn how to use the same MLPotion components across TensorFlow, PyTorch, and Keras!</p>"},{"location":"tutorials/multi-framework.html#the-power-of-protocols","title":"The Power of Protocols \ud83d\udcaa","text":"<p>MLPotion's protocol-based design means you can switch frameworks with minimal code changes:</p> <pre><code># Same pattern, different framework!\n\n# TensorFlow\nfrom mlpotion.frameworks.tensorflow import TFCSVDataLoader\nloader = TFCSVDataLoader(\"data.csv\", label_name=\"target\")\ndataset = loader.load()  # Returns tf.data.Dataset\n\n# PyTorch\nfrom mlpotion.frameworks.pytorch import PyTorchCSVDataset\ndataset = PyTorchCSVDataset(\"data.csv\", label_name=\"target\")\n\n# Keras\nfrom mlpotion.frameworks.keras import KerasCSVDataLoader\nloader = KerasCSVDataLoader(\"data.csv\", label_name=\"target\")\ndataset = loader.load()  # Returns keras dataset\n</code></pre>"},{"location":"tutorials/multi-framework.html#framework-agnostic-code","title":"Framework-Agnostic Code \ud83c\udfaf","text":"<p>Write code that works with any framework:</p> <pre><code>from typing import Protocol\nfrom mlpotion.core.protocols import DataLoader, ModelTrainer\n\ndef train_model(\n    loader: DataLoader,\n    trainer: ModelTrainer,\n    config,\n):\n    \"\"\"Works with TensorFlow, PyTorch, or Keras!\"\"\"\n    dataset = loader.load()\n    result = trainer.train(model, dataset, config)\n    return result\n</code></pre>"},{"location":"tutorials/multi-framework.html#switching-frameworks-mid-project","title":"Switching Frameworks Mid-Project \ud83d\udd04","text":"<pre><code># Start with TensorFlow for prototyping\nfrom mlpotion.frameworks.tensorflow import *\n\n# Later: Switch to PyTorch for custom research\nfrom mlpotion.frameworks.pytorch import *\n\n# Finally: Deploy with Keras for production\nfrom mlpotion.frameworks.keras import *\n</code></pre> <p>The same MLPotion patterns (to some possible extents) work everywhere!</p> <p> One API, Multiple Frameworks! \ud83c\udfa8 </p>"},{"location":"tutorials/zenml-integration.html","title":"ZenML Pipeline Tutorial \ud83d\udd04","text":"<p>Transform your MLPotion pipeline into a production-ready MLOps workflow with ZenML tracking, versioning, and reproducibility!</p> <p>Note: ZenML is just one integration example. MLPotion is designed to be orchestrator-agnostic and works with Prefect, Airflow, Kubeflow, and any other orchestration platform. See ZenML Integration Guide for extending to other orchestrators.</p>"},{"location":"tutorials/zenml-integration.html#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<pre><code>poetry add mlpotion -E tensorflow -E zenml\nzenml init\n</code></pre>"},{"location":"tutorials/zenml-integration.html#converting-your-pipeline","title":"Converting Your Pipeline \ud83d\udd04","text":""},{"location":"tutorials/zenml-integration.html#before-standalone-pipeline","title":"Before: Standalone Pipeline","text":"<pre><code>\"\"\"Basic TensorFlow usage WITHOUT ZenML.\n\nThis example demonstrates the core MLPotion TensorFlow workflow:\n1. Load data from CSV\n2. Optimize dataset for performance\n3. Create a TensorFlow model\n4. Train the model\n5. Evaluate the model\n6. Save and export the model\n\"\"\"\n\nimport tensorflow as tf\n\nfrom mlpotion.frameworks.tensorflow import (\n    CSVDataLoader,\n    DatasetOptimizer,\n    ModelEvaluator,\n    ModelPersistence,\n    ModelTrainer,\n    ModelTrainingConfig,\n)\n\n\ndef main() -&gt; None:\n    \"\"\"Run basic TensorFlow training pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow Basic Usage\")\n    print(\"=\" * 60)\n\n    # 1. Load data\n    print(\"\\n1. Loading data...\")\n    loader = CSVDataLoader(\n        file_pattern=\"examples/data/sample.csv\",\n        label_name=\"target\",\n        batch_size=1,  # Load unbatched, let DatasetOptimizer handle batching\n    )\n    dataset = loader.load()\n    print(f\"Dataset: {dataset}\")\n\n    # Unbatch the dataset first (since CSVDataLoader batches by default)\n    dataset = dataset.unbatch()\n\n    # Transform OrderedDict to single tensor\n    def prepare_features(features, label):\n        \"\"\"Convert OrderedDict of features to single tensor.\"\"\"\n        feature_list = [features[key] for key in sorted(features.keys())]\n        stacked_features = tf.stack(feature_list, axis=-1)\n        return stacked_features, label\n\n    dataset = dataset.map(prepare_features)\n\n    # 2. Optimize dataset\n    print(\"\\n2. Optimizing dataset...\")\n    optimizer = DatasetOptimizer(batch_size=8, shuffle_buffer_size=100)\n    dataset = optimizer.optimize(dataset)\n\n    # 3. Create model\n    print(\"\\n3. Creating model...\")\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(10,)),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n        ]\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n    print(model.summary())\n\n    # 4. Train model\n    print(\"\\n4. Training model...\")\n    trainer = ModelTrainer()\n    config = ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n    result = trainer.train(\n        model=model,\n        data=dataset,\n        config=config,\n    )\n\n    print(\"\\nTraining completed!\")\n    print(f\"{result=}\")\n\n    # 5. Evaluate model\n    print(\"\\n5. Evaluating model...\")\n    evaluator = ModelEvaluator()\n    from mlpotion.frameworks.tensorflow import ModelEvaluationConfig\n\n    eval_config = ModelEvaluationConfig(batch_size=8, verbose=1)\n    eval_result = evaluator.evaluate(\n        model=model,\n        data=dataset,\n        config=eval_config,\n    )\n    print(f\"{eval_result=}\")\n\n    # 6. Save model\n    print(\"\\n6. Saving model...\")\n    model_path = \"/tmp/tensorflow_model.keras\"\n    persistence = ModelPersistence(\n        path=model_path,\n        model=model,\n    )\n    persistence.save(\n        save_format=\".keras\",\n    )\n    print(f\"Model saved to: {model_path}\")\n\n    # 7. Load model\n    print(\"\\n7. Loading model...\")\n    loaded_model, metadata = persistence.load()\n    print(f\"Model loaded successfully: {type(loaded_model)}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Complete!\")\n    print(\"=\" * 60)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/zenml-integration.html#after-zenml-pipeline","title":"After: ZenML Pipeline","text":"<pre><code>\"\"\"TensorFlow training pipeline WITH ZenML orchestration.\n\nThis example demonstrates how to use MLPotion's TensorFlow components\nwithin a ZenML pipeline for reproducible and tracked ML workflows.\n\nRequirements:\n    pip install zenml\n\nSetup:\n    zenml init  # Initialize ZenML repository\n    export ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK=true  # For testing without full stack\n\"\"\"\n\nimport tensorflow as tf\nfrom zenml import pipeline, step\n\nfrom mlpotion.frameworks.tensorflow import ModelTrainingConfig\nfrom mlpotion.integrations.zenml.tensorflow.steps import (\n    evaluate_model,\n    export_model,\n    load_data,\n    optimize_data,\n    save_model,\n    train_model,\n)\n\n\n@step(enable_cache=False)  # Disable caching to ensure fresh model\ndef create_model() -&gt; tf.keras.Model:\n    \"\"\"Create and compile a TensorFlow model that accepts dict inputs.\n\n    Returns:\n        Compiled TensorFlow/Keras model ready for training.\n    \"\"\"\n    # Create inputs for each feature (10 features: feature_0 to feature_9)\n    # After batching, make_csv_dataset produces tensors with shape (batch_size,) for each scalar feature\n    # The materializer now correctly preserves this shape as (None,) where None is the batch dimension\n    inputs = {}\n    feature_list = []\n\n    for i in range(10):\n        # Each input has shape (1,) per sample after batching and materializer roundtrip\n        # The materializer preserves the concrete shape (batch_size, 1)\n        inp = tf.keras.Input(shape=(1,), name=f\"feature_{i}\", dtype=tf.float32)\n        inputs[f\"feature_{i}\"] = inp\n        # Already shape (batch_size, 1), no need to reshape\n        feature_list.append(inp)\n\n    # Concatenate all features along the last axis\n    # This will create shape (batch_size, 10)\n    concatenated = tf.keras.layers.Concatenate(axis=-1)(feature_list)\n\n    # Build the model architecture\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(concatenated)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(1)(x)\n\n    # Create the functional model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\", \"mse\"],\n    )\n\n    return model\n\n\n@step\ndef create_training_config() -&gt; ModelTrainingConfig:\n    \"\"\"Create training configuration.\n\n    Returns:\n        Training configuration with hyperparameters.\n    \"\"\"\n    return ModelTrainingConfig(\n        epochs=10,\n        batch_size=8,\n        learning_rate=0.001,\n        verbose=1,\n    )\n\n\n@pipeline(enable_cache=False)\ndef tensorflow_training_pipeline(\n    file_path: str = \"examples/data/sample.csv\",\n    label_name: str = \"target\",\n    model_save_path: str = \"/tmp/tensorflow_model.keras\",\n    export_path: str = \"/tmp/tensorflow_model_export\",\n):\n    \"\"\"Complete TensorFlow training pipeline with ZenML.\n\n    This pipeline orchestrates the entire ML workflow:\n    1. Load data from CSV\n    2. Optimize dataset for performance\n    3. Create and configure model\n    4. Train model\n    5. Evaluate model\n    6. Save model\n    7. Export model for deployment\n\n    Args:\n        file_path: Path to CSV data file.\n        label_name: Name of the target column.\n        model_save_path: Path to save the trained model.\n        export_path: Path to export the model for serving.\n    \"\"\"\n    # Step 1: Load data\n    dataset = load_data(\n        file_path=file_path,\n        batch_size=1,\n        label_name=label_name,\n    )\n\n    # Step 2: Optimize dataset\n    optimized_dataset = optimize_data(\n        dataset=dataset,\n        batch_size=8,\n        shuffle_buffer_size=100,\n    )\n\n    # Step 3: Create model and config\n    model = create_model()\n\n    # Step 4: Train model\n    _config_train = {\n        \"epochs\": 10,\n        \"learning_rate\": 0.001,\n        \"verbose\": 1,\n    }\n    trained_model, training_metrics = train_model(\n        model=model,\n        dataset=optimized_dataset,\n        **_config_train,\n    )\n\n    # Step 5: Evaluate model\n    evaluation_metrics = evaluate_model(\n        model=trained_model,\n        dataset=optimized_dataset,\n    )\n\n    # Step 6: Save model\n    save_model(\n        model=trained_model,\n        save_path=model_save_path,\n    )\n\n    # Step 7: Export model for serving\n    export_model(\n        model=trained_model,\n        export_path=export_path,\n        export_format=\"keras\",\n    )\n\n    return trained_model, training_metrics, evaluation_metrics\n\n\nif __name__ == \"__main__\":\n    \"\"\"Run the TensorFlow ZenML pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"MLPotion - TensorFlow ZenML Pipeline\")\n    print(\"=\" * 60)\n\n    # Run the pipeline\n    print(\"\\nRunning ZenML pipeline...\")\n    result = tensorflow_training_pipeline()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Pipeline completed successfully!\")\n</code></pre>"},{"location":"tutorials/zenml-integration.html#benefits-you-get","title":"Benefits You Get \ud83c\udf1f","text":"<ul> <li>\u2705 Automatic artifact versioning</li> <li>\u2705 Full pipeline lineage tracking</li> <li>\u2705 Experiment comparison</li> <li>\u2705 Model registry integration</li> <li>\u2705 Reproducible runs</li> <li>\u2705 Caching of unchanged steps</li> <li>\u2705 Step output tracking with metadata</li> </ul>"},{"location":"tutorials/zenml-integration.html#viewing-pipeline-runs","title":"Viewing Pipeline Runs \ud83d\udd0d","text":"<pre><code># List all pipeline runs\nzenml pipeline runs list\n\n# View details of a specific run\nzenml pipeline runs describe &lt;RUN_ID&gt;\n\n# Compare different runs\nzenml pipeline runs compare &lt;RUN_ID_1&gt; &lt;RUN_ID_2&gt;\n</code></pre> <p>See the ZenML Integration Guide for complete documentation and examples with PyTorch and Keras!</p> <p> Ready for production MLOps! \ud83d\ude80 </p>"}]}